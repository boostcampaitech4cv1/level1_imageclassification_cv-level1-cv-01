/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Currently logged in as: qwer55252. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /opt/ml/project-T4193/wandb/run-20221028_120305-91ykk1ty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-paper-37
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1
wandb: üöÄ View run at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1/runs/91ykk1ty
Loading checkpoint: /opt/ml/project-T4193/saved/models/efficientnet-b7-pretrained/1028_095732/checkpoint-epoch10.pth ...
EfficientNet(
  (_conv_stem): Conv2dStaticSamePadding(
    3, 64, kernel_size=(3, 3), stride=(2, 2), bias=False
    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
  )
  (_bn0): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_blocks): ModuleList(
    (0): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        64, 64, kernel_size=(3, 3), stride=[1, 1], groups=64, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        64, 16, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        16, 64, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (3): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (4): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        192, 192, kernel_size=(3, 3), stride=[2, 2], groups=192, bias=False
        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        192, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 192, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (5): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (6): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (7): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (8): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (9): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (10): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (11): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(5, 5), stride=[2, 2], groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (12): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (13): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (14): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (15): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (16): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (17): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (18): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(3, 3), stride=[2, 2], groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (19): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (20): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (21): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (22): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (23): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (24): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (25): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (26): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (27): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (28): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=[1, 1], groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (29): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (30): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (31): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (32): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (33): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (34): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (35): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (36): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (37): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (38): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=[2, 2], groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (39): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (40): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (41): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (42): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (43): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (44): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (45): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (46): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (47): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (48): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (49): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (50): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (51): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(3, 3), stride=[1, 1], groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (52): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (53): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (54): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (_conv_head): Conv2dStaticSamePadding(
    640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False
    (static_padding): Identity()
  )
  (_bn1): BatchNorm2d(2560, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)
  (_dropout): Dropout(p=0.5, inplace=False)
  (_fc): Linear(in_features=2560, out_features=18, bias=True)
  (_swish): MemoryEfficientSwish()
)
Loading checkpoint: /opt/ml/project-T4193/saved/models/efficientnet-b7-pretrained/1028_095732/checkpoint-epoch10.pth ...
Checkpoint loaded. Resume training from epoch 11
Train Epoch: 11 [0/19430 (0%)] Loss: 0.064281
Train Epoch: 11 [160/19430 (1%)] Loss: 0.000857
Train Epoch: 11 [320/19430 (2%)] Loss: 0.000264
Train Epoch: 11 [480/19430 (2%)] Loss: 0.004321
Train Epoch: 11 [640/19430 (3%)] Loss: 0.000526
Train Epoch: 11 [800/19430 (4%)] Loss: 0.044799
Train Epoch: 11 [960/19430 (5%)] Loss: 0.001231
Train Epoch: 11 [1120/19430 (6%)] Loss: 0.002698
Train Epoch: 11 [1280/19430 (7%)] Loss: 0.001527
Train Epoch: 11 [1440/19430 (7%)] Loss: 0.007536
Train Epoch: 11 [1600/19430 (8%)] Loss: 0.057688
Train Epoch: 11 [1760/19430 (9%)] Loss: 0.001383
Train Epoch: 11 [1920/19430 (10%)] Loss: 0.001134
Train Epoch: 11 [2080/19430 (11%)] Loss: 0.000741
Train Epoch: 11 [2240/19430 (12%)] Loss: 0.003789
Train Epoch: 11 [2400/19430 (12%)] Loss: 0.000990
Train Epoch: 11 [2560/19430 (13%)] Loss: 0.008308
Train Epoch: 11 [2720/19430 (14%)] Loss: 0.000320
Train Epoch: 11 [2880/19430 (15%)] Loss: 0.000898
Train Epoch: 11 [3040/19430 (16%)] Loss: 0.029266
Train Epoch: 11 [3200/19430 (16%)] Loss: 0.000498
Train Epoch: 11 [3360/19430 (17%)] Loss: 0.000447
Train Epoch: 11 [3520/19430 (18%)] Loss: 0.001239
Train Epoch: 11 [3680/19430 (19%)] Loss: 0.001258
Train Epoch: 11 [3840/19430 (20%)] Loss: 0.032517
Train Epoch: 11 [4000/19430 (21%)] Loss: 0.019277
Train Epoch: 11 [4160/19430 (21%)] Loss: 0.008677
Train Epoch: 11 [4320/19430 (22%)] Loss: 0.000355
Train Epoch: 11 [4480/19430 (23%)] Loss: 0.002573
Train Epoch: 11 [4640/19430 (24%)] Loss: 0.082086
Train Epoch: 11 [4800/19430 (25%)] Loss: 0.006193
Train Epoch: 11 [4960/19430 (26%)] Loss: 0.000868
Train Epoch: 11 [5120/19430 (26%)] Loss: 0.012714
Train Epoch: 11 [5280/19430 (27%)] Loss: 0.020316
Train Epoch: 11 [5440/19430 (28%)] Loss: 0.006363
Train Epoch: 11 [5600/19430 (29%)] Loss: 0.084105
Train Epoch: 11 [5760/19430 (30%)] Loss: 0.001257
Train Epoch: 11 [5920/19430 (30%)] Loss: 0.000466
Train Epoch: 11 [6080/19430 (31%)] Loss: 0.000853
Train Epoch: 11 [6240/19430 (32%)] Loss: 0.000384
Train Epoch: 11 [6400/19430 (33%)] Loss: 0.013660
Train Epoch: 11 [6560/19430 (34%)] Loss: 0.000253
Train Epoch: 11 [6720/19430 (35%)] Loss: 0.001141
Train Epoch: 11 [6880/19430 (35%)] Loss: 0.003050
Train Epoch: 11 [7040/19430 (36%)] Loss: 0.001539
Train Epoch: 11 [7200/19430 (37%)] Loss: 0.000468
Train Epoch: 11 [7360/19430 (38%)] Loss: 0.031017
Train Epoch: 11 [7520/19430 (39%)] Loss: 0.000529
Train Epoch: 11 [7680/19430 (40%)] Loss: 0.001047
Train Epoch: 11 [7840/19430 (40%)] Loss: 0.001509
Train Epoch: 11 [8000/19430 (41%)] Loss: 0.000505
Train Epoch: 11 [8160/19430 (42%)] Loss: 0.000522
Train Epoch: 11 [8320/19430 (43%)] Loss: 0.000545
Train Epoch: 11 [8480/19430 (44%)] Loss: 0.000721
Train Epoch: 11 [8640/19430 (44%)] Loss: 0.000247
Train Epoch: 11 [8800/19430 (45%)] Loss: 0.000300
Train Epoch: 11 [8960/19430 (46%)] Loss: 0.001998
Train Epoch: 11 [9120/19430 (47%)] Loss: 0.002605
Train Epoch: 11 [9280/19430 (48%)] Loss: 0.000223
Train Epoch: 11 [9440/19430 (49%)] Loss: 0.003525
Train Epoch: 11 [9600/19430 (49%)] Loss: 0.000565
Train Epoch: 11 [9760/19430 (50%)] Loss: 0.000152
Train Epoch: 11 [9920/19430 (51%)] Loss: 0.000172
Train Epoch: 11 [10080/19430 (52%)] Loss: 0.000500
Train Epoch: 11 [10240/19430 (53%)] Loss: 0.001981
Train Epoch: 11 [10400/19430 (54%)] Loss: 0.003282
Train Epoch: 11 [10560/19430 (54%)] Loss: 0.000264
Train Epoch: 11 [10720/19430 (55%)] Loss: 0.217887
Train Epoch: 11 [10880/19430 (56%)] Loss: 0.024336
Train Epoch: 11 [11040/19430 (57%)] Loss: 0.000494
Train Epoch: 11 [11200/19430 (58%)] Loss: 0.009388
Train Epoch: 11 [11360/19430 (58%)] Loss: 0.013320
Train Epoch: 11 [11520/19430 (59%)] Loss: 0.001192
Train Epoch: 11 [11680/19430 (60%)] Loss: 0.014153
Train Epoch: 11 [11840/19430 (61%)] Loss: 0.000773
Train Epoch: 11 [12000/19430 (62%)] Loss: 0.000871
Train Epoch: 11 [12160/19430 (63%)] Loss: 0.001962
Train Epoch: 11 [12320/19430 (63%)] Loss: 0.003929
Train Epoch: 11 [12480/19430 (64%)] Loss: 0.000504
Train Epoch: 11 [12640/19430 (65%)] Loss: 0.000293
Train Epoch: 11 [12800/19430 (66%)] Loss: 0.000159
Train Epoch: 11 [12960/19430 (67%)] Loss: 0.000108
Train Epoch: 11 [13120/19430 (68%)] Loss: 0.001631
Train Epoch: 11 [13280/19430 (68%)] Loss: 0.000988
Train Epoch: 11 [13440/19430 (69%)] Loss: 0.000299
Train Epoch: 11 [13600/19430 (70%)] Loss: 0.001266
Train Epoch: 11 [13760/19430 (71%)] Loss: 0.000890
Train Epoch: 11 [13920/19430 (72%)] Loss: 0.001113
Train Epoch: 11 [14080/19430 (72%)] Loss: 0.003240
Train Epoch: 11 [14240/19430 (73%)] Loss: 0.019884
Train Epoch: 11 [14400/19430 (74%)] Loss: 0.000290
Train Epoch: 11 [14560/19430 (75%)] Loss: 0.000551
Train Epoch: 11 [14720/19430 (76%)] Loss: 0.003915
Train Epoch: 11 [14880/19430 (77%)] Loss: 0.000229
Train Epoch: 11 [15040/19430 (77%)] Loss: 0.000464
Train Epoch: 11 [15200/19430 (78%)] Loss: 0.010403
Train Epoch: 11 [15360/19430 (79%)] Loss: 0.004660
Train Epoch: 11 [15520/19430 (80%)] Loss: 0.004197
Train Epoch: 11 [15680/19430 (81%)] Loss: 0.071916
Train Epoch: 11 [15840/19430 (82%)] Loss: 0.001523
Train Epoch: 11 [16000/19430 (82%)] Loss: 0.000607
Train Epoch: 11 [16160/19430 (83%)] Loss: 0.005399
Train Epoch: 11 [16320/19430 (84%)] Loss: 0.000990
Train Epoch: 11 [16480/19430 (85%)] Loss: 0.005210
Train Epoch: 11 [16640/19430 (86%)] Loss: 0.000308
Train Epoch: 11 [16800/19430 (86%)] Loss: 0.000192
Train Epoch: 11 [16960/19430 (87%)] Loss: 0.098202
Train Epoch: 11 [17120/19430 (88%)] Loss: 0.000165
Train Epoch: 11 [17280/19430 (89%)] Loss: 0.019785
Train Epoch: 11 [17440/19430 (90%)] Loss: 0.000953
Train Epoch: 11 [17600/19430 (91%)] Loss: 0.000092
Train Epoch: 11 [17760/19430 (91%)] Loss: 0.000760
Train Epoch: 11 [17920/19430 (92%)] Loss: 0.000680
Train Epoch: 11 [18080/19430 (93%)] Loss: 0.000310
Train Epoch: 11 [18240/19430 (94%)] Loss: 0.000972
Train Epoch: 11 [18400/19430 (95%)] Loss: 0.001901
Train Epoch: 11 [18560/19430 (96%)] Loss: 0.000315
Train Epoch: 11 [18720/19430 (96%)] Loss: 0.000709
Train Epoch: 11 [18880/19430 (97%)] Loss: 0.001624
Train Epoch: 11 [19040/19430 (98%)] Loss: 0.001212
Train Epoch: 11 [19200/19430 (99%)] Loss: 0.001829
Train Epoch: 11 [19360/19430 (100%)] Loss: 0.000500
    epoch          : 11
    Train_loss     : 0.009025723441589818
    Train_accuracy : 0.9972245065789473
    Train_f1_score : 0.9972245097160339
    Val_loss       : 0.03229941565972266
    Val_accuracy   : 0.9911370798319328
    Val_f1_score   : 0.9911370873451233
Warning: Metric 'val_loss' is not found. Model performance monitoring is disabled.
Train Epoch: 12 [0/19430 (0%)] Loss: 0.000627
Train Epoch: 12 [160/19430 (1%)] Loss: 0.002935
Train Epoch: 12 [320/19430 (2%)] Loss: 0.001039
Train Epoch: 12 [480/19430 (2%)] Loss: 0.001534
Train Epoch: 12 [640/19430 (3%)] Loss: 0.000523
Train Epoch: 12 [800/19430 (4%)] Loss: 0.000228
Train Epoch: 12 [960/19430 (5%)] Loss: 0.000151
Train Epoch: 12 [1120/19430 (6%)] Loss: 0.000272
Train Epoch: 12 [1280/19430 (7%)] Loss: 0.000726
Train Epoch: 12 [1440/19430 (7%)] Loss: 0.000723
Train Epoch: 12 [1600/19430 (8%)] Loss: 0.001331
Train Epoch: 12 [1760/19430 (9%)] Loss: 0.000899
Train Epoch: 12 [1920/19430 (10%)] Loss: 0.001020
Train Epoch: 12 [2080/19430 (11%)] Loss: 0.000088
Train Epoch: 12 [2240/19430 (12%)] Loss: 0.000326
Train Epoch: 12 [2400/19430 (12%)] Loss: 0.001257
Train Epoch: 12 [2560/19430 (13%)] Loss: 0.046640
Train Epoch: 12 [2720/19430 (14%)] Loss: 0.000107
Train Epoch: 12 [2880/19430 (15%)] Loss: 0.000204
Train Epoch: 12 [3040/19430 (16%)] Loss: 0.000258
Train Epoch: 12 [3200/19430 (16%)] Loss: 0.000566
Train Epoch: 12 [3360/19430 (17%)] Loss: 0.000607
Train Epoch: 12 [3520/19430 (18%)] Loss: 0.000337
Train Epoch: 12 [3680/19430 (19%)] Loss: 0.000216
Train Epoch: 12 [3840/19430 (20%)] Loss: 0.000653
Train Epoch: 12 [4000/19430 (21%)] Loss: 0.003492
Train Epoch: 12 [4160/19430 (21%)] Loss: 0.001229
Train Epoch: 12 [4320/19430 (22%)] Loss: 0.001355
Train Epoch: 12 [4480/19430 (23%)] Loss: 0.000559
Train Epoch: 12 [4640/19430 (24%)] Loss: 0.000153
Train Epoch: 12 [4800/19430 (25%)] Loss: 0.000465
Train Epoch: 12 [4960/19430 (26%)] Loss: 0.000650
Train Epoch: 12 [5120/19430 (26%)] Loss: 0.001014
Train Epoch: 12 [5280/19430 (27%)] Loss: 0.000074
Train Epoch: 12 [5440/19430 (28%)] Loss: 0.000220
Train Epoch: 12 [5600/19430 (29%)] Loss: 0.000075
Train Epoch: 12 [5760/19430 (30%)] Loss: 0.002649
Train Epoch: 12 [5920/19430 (30%)] Loss: 0.000166
Train Epoch: 12 [6080/19430 (31%)] Loss: 0.000154
Train Epoch: 12 [6240/19430 (32%)] Loss: 0.005842
Train Epoch: 12 [6400/19430 (33%)] Loss: 0.000539
Train Epoch: 12 [6560/19430 (34%)] Loss: 0.000238
Train Epoch: 12 [6720/19430 (35%)] Loss: 0.000345
Train Epoch: 12 [6880/19430 (35%)] Loss: 0.000203
Train Epoch: 12 [7040/19430 (36%)] Loss: 0.000516
Train Epoch: 12 [7200/19430 (37%)] Loss: 0.000060
Train Epoch: 12 [7360/19430 (38%)] Loss: 0.000110
Train Epoch: 12 [7520/19430 (39%)] Loss: 0.000104
Train Epoch: 12 [7680/19430 (40%)] Loss: 0.001123
Train Epoch: 12 [7840/19430 (40%)] Loss: 0.001162
Train Epoch: 12 [8000/19430 (41%)] Loss: 0.005293
Train Epoch: 12 [8160/19430 (42%)] Loss: 0.007335
Train Epoch: 12 [8320/19430 (43%)] Loss: 0.000122
Train Epoch: 12 [8480/19430 (44%)] Loss: 0.000184
Train Epoch: 12 [8640/19430 (44%)] Loss: 0.000366
Train Epoch: 12 [8800/19430 (45%)] Loss: 0.000121
Train Epoch: 12 [8960/19430 (46%)] Loss: 0.000188
Train Epoch: 12 [9120/19430 (47%)] Loss: 0.000677
Train Epoch: 12 [9280/19430 (48%)] Loss: 0.001711
Train Epoch: 12 [9440/19430 (49%)] Loss: 0.000091
Train Epoch: 12 [9600/19430 (49%)] Loss: 0.000794
Train Epoch: 12 [9760/19430 (50%)] Loss: 0.000521
Train Epoch: 12 [9920/19430 (51%)] Loss: 0.000283
Train Epoch: 12 [10080/19430 (52%)] Loss: 0.000113
Train Epoch: 12 [10240/19430 (53%)] Loss: 0.000121
Train Epoch: 12 [10400/19430 (54%)] Loss: 0.000497
Train Epoch: 12 [10560/19430 (54%)] Loss: 0.000228
Train Epoch: 12 [10720/19430 (55%)] Loss: 0.000273
Train Epoch: 12 [10880/19430 (56%)] Loss: 0.001305
Train Epoch: 12 [11040/19430 (57%)] Loss: 0.000107
Train Epoch: 12 [11200/19430 (58%)] Loss: 0.000132
Train Epoch: 12 [11360/19430 (58%)] Loss: 0.000250
Train Epoch: 12 [11520/19430 (59%)] Loss: 0.006365
Train Epoch: 12 [11680/19430 (60%)] Loss: 0.000306
Train Epoch: 12 [11840/19430 (61%)] Loss: 0.000134
Train Epoch: 12 [12000/19430 (62%)] Loss: 0.000128
Train Epoch: 12 [12160/19430 (63%)] Loss: 0.000240
Train Epoch: 12 [12320/19430 (63%)] Loss: 0.000439
Train Epoch: 12 [12480/19430 (64%)] Loss: 0.000738
Train Epoch: 12 [12640/19430 (65%)] Loss: 0.001565
Train Epoch: 12 [12800/19430 (66%)] Loss: 0.000119
Train Epoch: 12 [12960/19430 (67%)] Loss: 0.000163
Train Epoch: 12 [13120/19430 (68%)] Loss: 0.000071
Train Epoch: 12 [13280/19430 (68%)] Loss: 0.000783
Train Epoch: 12 [13440/19430 (69%)] Loss: 0.000443
Train Epoch: 12 [13600/19430 (70%)] Loss: 0.004027
Train Epoch: 12 [13760/19430 (71%)] Loss: 0.000536
Train Epoch: 12 [13920/19430 (72%)] Loss: 0.000192
Train Epoch: 12 [14080/19430 (72%)] Loss: 0.000222
Train Epoch: 12 [14240/19430 (73%)] Loss: 0.000753
Train Epoch: 12 [14400/19430 (74%)] Loss: 0.000507
Train Epoch: 12 [14560/19430 (75%)] Loss: 0.003011
Train Epoch: 12 [14720/19430 (76%)] Loss: 0.005853
Train Epoch: 12 [14880/19430 (77%)] Loss: 0.000168
Train Epoch: 12 [15040/19430 (77%)] Loss: 0.000370
Train Epoch: 12 [15200/19430 (78%)] Loss: 0.000693
Train Epoch: 12 [15360/19430 (79%)] Loss: 0.001636
Train Epoch: 12 [15520/19430 (80%)] Loss: 0.000071
Train Epoch: 12 [15680/19430 (81%)] Loss: 0.000154
Train Epoch: 12 [15840/19430 (82%)] Loss: 0.000068
Train Epoch: 12 [16000/19430 (82%)] Loss: 0.000872
Train Epoch: 12 [16160/19430 (83%)] Loss: 0.000042
Train Epoch: 12 [16320/19430 (84%)] Loss: 0.000043
Train Epoch: 12 [16480/19430 (85%)] Loss: 0.000186
Train Epoch: 12 [16640/19430 (86%)] Loss: 0.000360
Train Epoch: 12 [16800/19430 (86%)] Loss: 0.000688
Train Epoch: 12 [16960/19430 (87%)] Loss: 0.000114
Train Epoch: 12 [17120/19430 (88%)] Loss: 0.000061
Train Epoch: 12 [17280/19430 (89%)] Loss: 0.000125
Train Epoch: 12 [17440/19430 (90%)] Loss: 0.000262
Train Epoch: 12 [17600/19430 (91%)] Loss: 0.000210
Train Epoch: 12 [17760/19430 (91%)] Loss: 0.000531
Train Epoch: 12 [17920/19430 (92%)] Loss: 0.001373
Train Epoch: 12 [18080/19430 (93%)] Loss: 0.000075
Train Epoch: 12 [18240/19430 (94%)] Loss: 0.001469
Train Epoch: 12 [18400/19430 (95%)] Loss: 0.003213
Train Epoch: 12 [18560/19430 (96%)] Loss: 0.000275
Train Epoch: 12 [18720/19430 (96%)] Loss: 0.013810
Train Epoch: 12 [18880/19430 (97%)] Loss: 0.000509
Train Epoch: 12 [19040/19430 (98%)] Loss: 0.006585
Train Epoch: 12 [19200/19430 (99%)] Loss: 0.000858
Train Epoch: 12 [19360/19430 (100%)] Loss: 0.000340
    epoch          : 12
    Train_loss     : 0.0029541499197098106
    Train_accuracy : 0.9994346217105263
    Train_f1_score : 0.9994346499443054
    Val_loss       : 0.02144047168116338
    Val_accuracy   : 0.9954044117647058
    Val_f1_score   : 0.9954044222831726
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1028_120308/checkpoint-epoch12.pth ...
Train Epoch: 13 [0/19430 (0%)] Loss: 0.000155
Train Epoch: 13 [160/19430 (1%)] Loss: 0.001255
Train Epoch: 13 [320/19430 (2%)] Loss: 0.010477
Train Epoch: 13 [480/19430 (2%)] Loss: 0.000258
Train Epoch: 13 [640/19430 (3%)] Loss: 0.001925
Train Epoch: 13 [800/19430 (4%)] Loss: 0.000109
Train Epoch: 13 [960/19430 (5%)] Loss: 0.000061
Train Epoch: 13 [1120/19430 (6%)] Loss: 0.000210
Train Epoch: 13 [1280/19430 (7%)] Loss: 0.000129
Train Epoch: 13 [1440/19430 (7%)] Loss: 0.000279
Train Epoch: 13 [1600/19430 (8%)] Loss: 0.000105
Train Epoch: 13 [1760/19430 (9%)] Loss: 0.000288
Train Epoch: 13 [1920/19430 (10%)] Loss: 0.000355
Train Epoch: 13 [2080/19430 (11%)] Loss: 0.000769
Train Epoch: 13 [2240/19430 (12%)] Loss: 0.003813
Train Epoch: 13 [2400/19430 (12%)] Loss: 0.000179
Train Epoch: 13 [2560/19430 (13%)] Loss: 0.000221
Train Epoch: 13 [2720/19430 (14%)] Loss: 0.001658
Train Epoch: 13 [2880/19430 (15%)] Loss: 0.000890
Train Epoch: 13 [3040/19430 (16%)] Loss: 0.000100
Train Epoch: 13 [3200/19430 (16%)] Loss: 0.000281
Train Epoch: 13 [3360/19430 (17%)] Loss: 0.000342
Train Epoch: 13 [3520/19430 (18%)] Loss: 0.000296
Train Epoch: 13 [3680/19430 (19%)] Loss: 0.002748
Train Epoch: 13 [3840/19430 (20%)] Loss: 0.056428
Train Epoch: 13 [4000/19430 (21%)] Loss: 0.000076
Train Epoch: 13 [4160/19430 (21%)] Loss: 0.000419
Train Epoch: 13 [4320/19430 (22%)] Loss: 0.000799
Train Epoch: 13 [4480/19430 (23%)] Loss: 0.006425
Train Epoch: 13 [4640/19430 (24%)] Loss: 0.000339
Train Epoch: 13 [4800/19430 (25%)] Loss: 0.000069
Train Epoch: 13 [4960/19430 (26%)] Loss: 0.000151
Train Epoch: 13 [5120/19430 (26%)] Loss: 0.000093
Train Epoch: 13 [5280/19430 (27%)] Loss: 0.000446
Train Epoch: 13 [5440/19430 (28%)] Loss: 0.000164
Train Epoch: 13 [5600/19430 (29%)] Loss: 0.001810
Train Epoch: 13 [5760/19430 (30%)] Loss: 0.002262
Train Epoch: 13 [5920/19430 (30%)] Loss: 0.000097
Train Epoch: 13 [6080/19430 (31%)] Loss: 0.000045
Train Epoch: 13 [6240/19430 (32%)] Loss: 0.001848
Train Epoch: 13 [6400/19430 (33%)] Loss: 0.000116
Train Epoch: 13 [6560/19430 (34%)] Loss: 0.000089
Train Epoch: 13 [6720/19430 (35%)] Loss: 0.000469
Train Epoch: 13 [6880/19430 (35%)] Loss: 0.001544
Train Epoch: 13 [7040/19430 (36%)] Loss: 0.000287
Train Epoch: 13 [7200/19430 (37%)] Loss: 0.002956
Train Epoch: 13 [7360/19430 (38%)] Loss: 0.000051
Train Epoch: 13 [7520/19430 (39%)] Loss: 0.000247
Train Epoch: 13 [7680/19430 (40%)] Loss: 0.000794
Train Epoch: 13 [7840/19430 (40%)] Loss: 0.000248
Train Epoch: 13 [8000/19430 (41%)] Loss: 0.000152
Train Epoch: 13 [8160/19430 (42%)] Loss: 0.000124
Train Epoch: 13 [8320/19430 (43%)] Loss: 0.000382
Train Epoch: 13 [8480/19430 (44%)] Loss: 0.000055
Train Epoch: 13 [8640/19430 (44%)] Loss: 0.000240
Train Epoch: 13 [8800/19430 (45%)] Loss: 0.000074
Train Epoch: 13 [8960/19430 (46%)] Loss: 0.000095
Train Epoch: 13 [9120/19430 (47%)] Loss: 0.000688
Train Epoch: 13 [9280/19430 (48%)] Loss: 0.000114
Train Epoch: 13 [9440/19430 (49%)] Loss: 0.000071
Train Epoch: 13 [9600/19430 (49%)] Loss: 0.037498
Train Epoch: 13 [9760/19430 (50%)] Loss: 0.173509
Train Epoch: 13 [9920/19430 (51%)] Loss: 0.000136
Train Epoch: 13 [10080/19430 (52%)] Loss: 0.000038
Train Epoch: 13 [10240/19430 (53%)] Loss: 0.001042
Train Epoch: 13 [10400/19430 (54%)] Loss: 0.176604
Train Epoch: 13 [10560/19430 (54%)] Loss: 0.000142
Train Epoch: 13 [10720/19430 (55%)] Loss: 0.000749
Train Epoch: 13 [10880/19430 (56%)] Loss: 0.000259
Train Epoch: 13 [11040/19430 (57%)] Loss: 0.000376
Train Epoch: 13 [11200/19430 (58%)] Loss: 0.000607
Train Epoch: 13 [11360/19430 (58%)] Loss: 0.000065
Train Epoch: 13 [11520/19430 (59%)] Loss: 0.003271
Train Epoch: 13 [11680/19430 (60%)] Loss: 0.001330
Train Epoch: 13 [11840/19430 (61%)] Loss: 0.014692
Train Epoch: 13 [12000/19430 (62%)] Loss: 0.000100
Train Epoch: 13 [12160/19430 (63%)] Loss: 0.000028
Train Epoch: 13 [12320/19430 (63%)] Loss: 0.000051
Train Epoch: 13 [12480/19430 (64%)] Loss: 0.000147
Train Epoch: 13 [12640/19430 (65%)] Loss: 0.000806
Train Epoch: 13 [12800/19430 (66%)] Loss: 0.004621
Train Epoch: 13 [12960/19430 (67%)] Loss: 0.000442
Train Epoch: 13 [13120/19430 (68%)] Loss: 0.000052
Train Epoch: 13 [13280/19430 (68%)] Loss: 0.000888
Train Epoch: 13 [13440/19430 (69%)] Loss: 0.000123
Train Epoch: 13 [13600/19430 (70%)] Loss: 0.000162
Train Epoch: 13 [13760/19430 (71%)] Loss: 0.000121
Train Epoch: 13 [13920/19430 (72%)] Loss: 0.000428
Train Epoch: 13 [14080/19430 (72%)] Loss: 0.000516
Train Epoch: 13 [14240/19430 (73%)] Loss: 0.000472
Train Epoch: 13 [14400/19430 (74%)] Loss: 0.000296
Train Epoch: 13 [14560/19430 (75%)] Loss: 0.000130
Train Epoch: 13 [14720/19430 (76%)] Loss: 0.003294
Train Epoch: 13 [14880/19430 (77%)] Loss: 0.001709
Train Epoch: 13 [15040/19430 (77%)] Loss: 0.000083
Train Epoch: 13 [15200/19430 (78%)] Loss: 0.006478
Train Epoch: 13 [15360/19430 (79%)] Loss: 0.003759
Train Epoch: 13 [15520/19430 (80%)] Loss: 0.000121
Train Epoch: 13 [15680/19430 (81%)] Loss: 0.000198
Train Epoch: 13 [15840/19430 (82%)] Loss: 0.002194
Train Epoch: 13 [16000/19430 (82%)] Loss: 0.000232
Train Epoch: 13 [16160/19430 (83%)] Loss: 0.000111
Train Epoch: 13 [16320/19430 (84%)] Loss: 0.000753
Train Epoch: 13 [16480/19430 (85%)] Loss: 0.000141
Train Epoch: 13 [16640/19430 (86%)] Loss: 0.000520
Train Epoch: 13 [16800/19430 (86%)] Loss: 0.000102
Train Epoch: 13 [16960/19430 (87%)] Loss: 0.000069
Train Epoch: 13 [17120/19430 (88%)] Loss: 0.000308
Train Epoch: 13 [17280/19430 (89%)] Loss: 0.001025
Train Epoch: 13 [17440/19430 (90%)] Loss: 0.010103
Train Epoch: 13 [17600/19430 (91%)] Loss: 0.000945
Train Epoch: 13 [17760/19430 (91%)] Loss: 0.001547
Train Epoch: 13 [17920/19430 (92%)] Loss: 0.000806
Train Epoch: 13 [18080/19430 (93%)] Loss: 0.000123
Train Epoch: 13 [18240/19430 (94%)] Loss: 0.000734
Train Epoch: 13 [18400/19430 (95%)] Loss: 0.001120
Train Epoch: 13 [18560/19430 (96%)] Loss: 0.000702
Train Epoch: 13 [18720/19430 (96%)] Loss: 0.000953
Train Epoch: 13 [18880/19430 (97%)] Loss: 0.000792
Train Epoch: 13 [19040/19430 (98%)] Loss: 0.000076
Train Epoch: 13 [19200/19430 (99%)] Loss: 0.000358
Train Epoch: 13 [19360/19430 (100%)] Loss: 0.000498
    epoch          : 13
    Train_loss     : 0.004211425016921423
    Train_accuracy : 0.9989549067982457
    Train_f1_score : 0.9989548921585083
    Val_loss       : 0.036162553571758134
    Val_accuracy   : 0.9908088235294118
    Val_f1_score   : 0.9908088445663452
Train Epoch: 14 [0/19430 (0%)] Loss: 0.000029
Train Epoch: 14 [160/19430 (1%)] Loss: 0.436093
Train Epoch: 14 [320/19430 (2%)] Loss: 0.016303
Train Epoch: 14 [480/19430 (2%)] Loss: 0.377586
Train Epoch: 14 [640/19430 (3%)] Loss: 0.185780
Train Epoch: 14 [800/19430 (4%)] Loss: 0.048114
Train Epoch: 14 [960/19430 (5%)] Loss: 0.002980
Train Epoch: 14 [1120/19430 (6%)] Loss: 0.128884
Train Epoch: 14 [1280/19430 (7%)] Loss: 0.046269
Train Epoch: 14 [1440/19430 (7%)] Loss: 0.299038
Train Epoch: 14 [1600/19430 (8%)] Loss: 0.149260
Train Epoch: 14 [1760/19430 (9%)] Loss: 0.009609
Train Epoch: 14 [1920/19430 (10%)] Loss: 0.033092
Train Epoch: 14 [2080/19430 (11%)] Loss: 0.151228
Train Epoch: 14 [2240/19430 (12%)] Loss: 0.091448
Train Epoch: 14 [2400/19430 (12%)] Loss: 0.064830
Train Epoch: 14 [2560/19430 (13%)] Loss: 0.003771
Train Epoch: 14 [2720/19430 (14%)] Loss: 0.005383
Train Epoch: 14 [2880/19430 (15%)] Loss: 0.052407
Train Epoch: 14 [3040/19430 (16%)] Loss: 0.068803
Train Epoch: 14 [3200/19430 (16%)] Loss: 0.108568
Train Epoch: 14 [3360/19430 (17%)] Loss: 0.204571
Train Epoch: 14 [3520/19430 (18%)] Loss: 0.015194
Train Epoch: 14 [3680/19430 (19%)] Loss: 0.008534
Train Epoch: 14 [3840/19430 (20%)] Loss: 0.010232
Train Epoch: 14 [4000/19430 (21%)] Loss: 0.000492
Train Epoch: 14 [4160/19430 (21%)] Loss: 0.006193
Train Epoch: 14 [4320/19430 (22%)] Loss: 0.006948
Train Epoch: 14 [4480/19430 (23%)] Loss: 0.005078
Train Epoch: 14 [4640/19430 (24%)] Loss: 0.035196
Train Epoch: 14 [4800/19430 (25%)] Loss: 0.006311
Train Epoch: 14 [4960/19430 (26%)] Loss: 0.003777
Train Epoch: 14 [5120/19430 (26%)] Loss: 0.011701
Train Epoch: 14 [5280/19430 (27%)] Loss: 0.172406
Train Epoch: 14 [5440/19430 (28%)] Loss: 0.089851
Train Epoch: 14 [5600/19430 (29%)] Loss: 0.004134
Train Epoch: 14 [5760/19430 (30%)] Loss: 0.010348
Train Epoch: 14 [5920/19430 (30%)] Loss: 0.162236
Train Epoch: 14 [6080/19430 (31%)] Loss: 0.007998
Train Epoch: 14 [6240/19430 (32%)] Loss: 0.021700
Train Epoch: 14 [6400/19430 (33%)] Loss: 0.153514
Train Epoch: 14 [6560/19430 (34%)] Loss: 0.090034
Train Epoch: 14 [6720/19430 (35%)] Loss: 0.005587
Train Epoch: 14 [6880/19430 (35%)] Loss: 0.009676
Train Epoch: 14 [7040/19430 (36%)] Loss: 0.059234
Train Epoch: 14 [7200/19430 (37%)] Loss: 0.011135
Train Epoch: 14 [7360/19430 (38%)] Loss: 0.067396
Train Epoch: 14 [7520/19430 (39%)] Loss: 0.006569
Train Epoch: 14 [7680/19430 (40%)] Loss: 0.112264
Train Epoch: 14 [7840/19430 (40%)] Loss: 0.003826
Train Epoch: 14 [8000/19430 (41%)] Loss: 0.002293
Train Epoch: 14 [8160/19430 (42%)] Loss: 0.004130
Train Epoch: 14 [8320/19430 (43%)] Loss: 0.011773
Train Epoch: 14 [8480/19430 (44%)] Loss: 0.010332
Train Epoch: 14 [8640/19430 (44%)] Loss: 0.167684
Train Epoch: 14 [8800/19430 (45%)] Loss: 0.001333
Train Epoch: 14 [8960/19430 (46%)] Loss: 0.007132
Train Epoch: 14 [9120/19430 (47%)] Loss: 0.005120
Train Epoch: 14 [9280/19430 (48%)] Loss: 0.005766
Train Epoch: 14 [9440/19430 (49%)] Loss: 0.002135
Train Epoch: 14 [9600/19430 (49%)] Loss: 0.010048
Train Epoch: 14 [9760/19430 (50%)] Loss: 0.042895
Train Epoch: 14 [9920/19430 (51%)] Loss: 0.002387
Train Epoch: 14 [10080/19430 (52%)] Loss: 0.002313
Train Epoch: 14 [10240/19430 (53%)] Loss: 0.002235
Train Epoch: 14 [10400/19430 (54%)] Loss: 0.022815
Train Epoch: 14 [10560/19430 (54%)] Loss: 0.002247
Train Epoch: 14 [10720/19430 (55%)] Loss: 0.000485
Train Epoch: 14 [10880/19430 (56%)] Loss: 0.007164
Train Epoch: 14 [11040/19430 (57%)] Loss: 0.052377
Train Epoch: 14 [11200/19430 (58%)] Loss: 0.000391
Train Epoch: 14 [11360/19430 (58%)] Loss: 0.042169
Train Epoch: 14 [11520/19430 (59%)] Loss: 0.003098
Train Epoch: 14 [11680/19430 (60%)] Loss: 0.000849
Train Epoch: 14 [11840/19430 (61%)] Loss: 0.026734
Train Epoch: 14 [12000/19430 (62%)] Loss: 0.001465
Train Epoch: 14 [12160/19430 (63%)] Loss: 0.042266
Train Epoch: 14 [12320/19430 (63%)] Loss: 0.003054
Train Epoch: 14 [12480/19430 (64%)] Loss: 0.001974
Train Epoch: 14 [12640/19430 (65%)] Loss: 0.000762
Train Epoch: 14 [12800/19430 (66%)] Loss: 0.108072
Train Epoch: 14 [12960/19430 (67%)] Loss: 0.000331
Train Epoch: 14 [13120/19430 (68%)] Loss: 0.001434
Train Epoch: 14 [13280/19430 (68%)] Loss: 0.005499
Train Epoch: 14 [13440/19430 (69%)] Loss: 0.112111
Train Epoch: 14 [13600/19430 (70%)] Loss: 0.006142
Train Epoch: 14 [13760/19430 (71%)] Loss: 0.001076
Train Epoch: 14 [13920/19430 (72%)] Loss: 0.040530
Train Epoch: 14 [14080/19430 (72%)] Loss: 0.117226
Train Epoch: 14 [14240/19430 (73%)] Loss: 0.001043
Train Epoch: 14 [14400/19430 (74%)] Loss: 0.017848
Train Epoch: 14 [14560/19430 (75%)] Loss: 0.017193
Train Epoch: 14 [14720/19430 (76%)] Loss: 0.005421
Train Epoch: 14 [14880/19430 (77%)] Loss: 0.012015
Train Epoch: 14 [15040/19430 (77%)] Loss: 0.010599
Train Epoch: 14 [15200/19430 (78%)] Loss: 0.006499
Train Epoch: 14 [15360/19430 (79%)] Loss: 0.001700
Train Epoch: 14 [15520/19430 (80%)] Loss: 0.000762
Train Epoch: 14 [15680/19430 (81%)] Loss: 0.007228
Train Epoch: 14 [15840/19430 (82%)] Loss: 0.004962
Train Epoch: 14 [16000/19430 (82%)] Loss: 0.000173
Train Epoch: 14 [16160/19430 (83%)] Loss: 0.001086
Train Epoch: 14 [16320/19430 (84%)] Loss: 0.000966
Train Epoch: 14 [16480/19430 (85%)] Loss: 0.005282
Train Epoch: 14 [16640/19430 (86%)] Loss: 0.000346
Train Epoch: 14 [16800/19430 (86%)] Loss: 0.061326
Train Epoch: 14 [16960/19430 (87%)] Loss: 0.000609
Train Epoch: 14 [17120/19430 (88%)] Loss: 0.002998
Train Epoch: 14 [17280/19430 (89%)] Loss: 0.000408
Train Epoch: 14 [17440/19430 (90%)] Loss: 0.003556
Train Epoch: 14 [17600/19430 (91%)] Loss: 0.003176
Train Epoch: 14 [17760/19430 (91%)] Loss: 0.020853
Train Epoch: 14 [17920/19430 (92%)] Loss: 0.000896
Train Epoch: 14 [18080/19430 (93%)] Loss: 0.000427
Train Epoch: 14 [18240/19430 (94%)] Loss: 0.010752
Train Epoch: 14 [18400/19430 (95%)] Loss: 0.000141
Train Epoch: 14 [18560/19430 (96%)] Loss: 0.000912
Train Epoch: 14 [18720/19430 (96%)] Loss: 0.005715
Train Epoch: 14 [18880/19430 (97%)] Loss: 0.001451
Train Epoch: 14 [19040/19430 (98%)] Loss: 0.000501
Train Epoch: 14 [19200/19430 (99%)] Loss: 0.005754
Train Epoch: 14 [19360/19430 (100%)] Loss: 0.006285
    epoch          : 14
    Train_loss     : 0.03881772372784852
    Train_accuracy : 0.9882298519736842
    Train_f1_score : 0.9882298707962036
    Val_loss       : 0.05103849588179882
    Val_accuracy   : 0.9894301470588235
    Val_f1_score   : 0.9894301295280457
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1028_120308/checkpoint-epoch14.pth ...
Train Epoch: 15 [0/19430 (0%)] Loss: 0.000657
Train Epoch: 15 [160/19430 (1%)] Loss: 0.002238
Train Epoch: 15 [320/19430 (2%)] Loss: 0.000966
Train Epoch: 15 [480/19430 (2%)] Loss: 0.000273
Train Epoch: 15 [640/19430 (3%)] Loss: 0.000551
Train Epoch: 15 [800/19430 (4%)] Loss: 0.001012
Train Epoch: 15 [960/19430 (5%)] Loss: 0.002938
Train Epoch: 15 [1120/19430 (6%)] Loss: 0.000732
Train Epoch: 15 [1280/19430 (7%)] Loss: 0.000329
Train Epoch: 15 [1440/19430 (7%)] Loss: 0.001518
Train Epoch: 15 [1600/19430 (8%)] Loss: 0.000560
Train Epoch: 15 [1760/19430 (9%)] Loss: 0.000379
Train Epoch: 15 [1920/19430 (10%)] Loss: 0.004626
Train Epoch: 15 [2080/19430 (11%)] Loss: 0.000344
Train Epoch: 15 [2240/19430 (12%)] Loss: 0.004066
Train Epoch: 15 [2400/19430 (12%)] Loss: 0.000736
Train Epoch: 15 [2560/19430 (13%)] Loss: 0.001381
Train Epoch: 15 [2720/19430 (14%)] Loss: 0.001634
Train Epoch: 15 [2880/19430 (15%)] Loss: 0.001843
Train Epoch: 15 [3040/19430 (16%)] Loss: 0.001559
Train Epoch: 15 [3200/19430 (16%)] Loss: 0.000326
Train Epoch: 15 [3360/19430 (17%)] Loss: 0.001292
Train Epoch: 15 [3520/19430 (18%)] Loss: 0.001268
Train Epoch: 15 [3680/19430 (19%)] Loss: 0.010854
Train Epoch: 15 [3840/19430 (20%)] Loss: 0.000197
Train Epoch: 15 [4000/19430 (21%)] Loss: 0.001068
Train Epoch: 15 [4160/19430 (21%)] Loss: 0.000566
Train Epoch: 15 [4320/19430 (22%)] Loss: 0.021508
Train Epoch: 15 [4480/19430 (23%)] Loss: 0.000167
Train Epoch: 15 [4640/19430 (24%)] Loss: 0.000701
Train Epoch: 15 [4800/19430 (25%)] Loss: 0.000485
Train Epoch: 15 [4960/19430 (26%)] Loss: 0.000305
Train Epoch: 15 [5120/19430 (26%)] Loss: 0.000276
Train Epoch: 15 [5280/19430 (27%)] Loss: 0.000282
Train Epoch: 15 [5440/19430 (28%)] Loss: 0.000092
Train Epoch: 15 [5600/19430 (29%)] Loss: 0.000365
Train Epoch: 15 [5760/19430 (30%)] Loss: 0.000135
Train Epoch: 15 [5920/19430 (30%)] Loss: 0.000504
Train Epoch: 15 [6080/19430 (31%)] Loss: 0.001600
Train Epoch: 15 [6240/19430 (32%)] Loss: 0.000351
Train Epoch: 15 [6400/19430 (33%)] Loss: 0.000341
Train Epoch: 15 [6560/19430 (34%)] Loss: 0.001061
Train Epoch: 15 [6720/19430 (35%)] Loss: 0.000570
Train Epoch: 15 [6880/19430 (35%)] Loss: 0.002439
Train Epoch: 15 [7040/19430 (36%)] Loss: 0.000960
Train Epoch: 15 [7200/19430 (37%)] Loss: 0.063101
Train Epoch: 15 [7360/19430 (38%)] Loss: 0.000267
Train Epoch: 15 [7520/19430 (39%)] Loss: 0.000728
Train Epoch: 15 [7680/19430 (40%)] Loss: 0.000158
Train Epoch: 15 [7840/19430 (40%)] Loss: 0.000716
Train Epoch: 15 [8000/19430 (41%)] Loss: 0.016490
Train Epoch: 15 [8160/19430 (42%)] Loss: 0.000638
Train Epoch: 15 [8320/19430 (43%)] Loss: 0.000212
Train Epoch: 15 [8480/19430 (44%)] Loss: 0.001887
Train Epoch: 15 [8640/19430 (44%)] Loss: 0.001969
Train Epoch: 15 [8800/19430 (45%)] Loss: 0.001095
Train Epoch: 15 [8960/19430 (46%)] Loss: 0.000670
Train Epoch: 15 [9120/19430 (47%)] Loss: 0.000568
Train Epoch: 15 [9280/19430 (48%)] Loss: 0.000750
Train Epoch: 15 [9440/19430 (49%)] Loss: 0.006477
Train Epoch: 15 [9600/19430 (49%)] Loss: 0.000180
Train Epoch: 15 [9760/19430 (50%)] Loss: 0.000616
Train Epoch: 15 [9920/19430 (51%)] Loss: 0.000106
Train Epoch: 15 [10080/19430 (52%)] Loss: 0.000804
Train Epoch: 15 [10240/19430 (53%)] Loss: 0.000813
Train Epoch: 15 [10400/19430 (54%)] Loss: 0.000597
Train Epoch: 15 [10560/19430 (54%)] Loss: 0.000812
Train Epoch: 15 [10720/19430 (55%)] Loss: 0.005140
Train Epoch: 15 [10880/19430 (56%)] Loss: 0.000438
Train Epoch: 15 [11040/19430 (57%)] Loss: 0.000315
Train Epoch: 15 [11200/19430 (58%)] Loss: 0.003543
Train Epoch: 15 [11360/19430 (58%)] Loss: 0.001019
Train Epoch: 15 [11520/19430 (59%)] Loss: 0.003875
Train Epoch: 15 [11680/19430 (60%)] Loss: 0.000409
Train Epoch: 15 [11840/19430 (61%)] Loss: 0.001995
Train Epoch: 15 [12000/19430 (62%)] Loss: 0.000480
Train Epoch: 15 [12160/19430 (63%)] Loss: 0.000128
Train Epoch: 15 [12320/19430 (63%)] Loss: 0.027500
Train Epoch: 15 [12480/19430 (64%)] Loss: 0.004084
Train Epoch: 15 [12640/19430 (65%)] Loss: 0.125894
Train Epoch: 15 [12800/19430 (66%)] Loss: 0.005640
Train Epoch: 15 [12960/19430 (67%)] Loss: 0.000938
Train Epoch: 15 [13120/19430 (68%)] Loss: 0.000226
Train Epoch: 15 [13280/19430 (68%)] Loss: 0.012762
Train Epoch: 15 [13440/19430 (69%)] Loss: 0.000163
Train Epoch: 15 [13600/19430 (70%)] Loss: 0.000354
Train Epoch: 15 [13760/19430 (71%)] Loss: 0.000398
Train Epoch: 15 [13920/19430 (72%)] Loss: 0.001310
Train Epoch: 15 [14080/19430 (72%)] Loss: 0.011115
Train Epoch: 15 [14240/19430 (73%)] Loss: 0.002235
Train Epoch: 15 [14400/19430 (74%)] Loss: 0.004036
Train Epoch: 15 [14560/19430 (75%)] Loss: 0.001823
Train Epoch: 15 [14720/19430 (76%)] Loss: 0.011184
Train Epoch: 15 [14880/19430 (77%)] Loss: 0.000195
Train Epoch: 15 [15040/19430 (77%)] Loss: 0.006530
Train Epoch: 15 [15200/19430 (78%)] Loss: 0.000345
Train Epoch: 15 [15360/19430 (79%)] Loss: 0.089200
Train Epoch: 15 [15520/19430 (80%)] Loss: 0.000270
Train Epoch: 15 [15680/19430 (81%)] Loss: 0.053621
Train Epoch: 15 [15840/19430 (82%)] Loss: 0.000900
Train Epoch: 15 [16000/19430 (82%)] Loss: 0.008165
Train Epoch: 15 [16160/19430 (83%)] Loss: 0.034383
Train Epoch: 15 [16320/19430 (84%)] Loss: 0.001523
Train Epoch: 15 [16480/19430 (85%)] Loss: 0.006887
Train Epoch: 15 [16640/19430 (86%)] Loss: 0.002038
Train Epoch: 15 [16800/19430 (86%)] Loss: 0.001646
Train Epoch: 15 [16960/19430 (87%)] Loss: 0.002971
Train Epoch: 15 [17120/19430 (88%)] Loss: 0.016185
Train Epoch: 15 [17280/19430 (89%)] Loss: 0.269824
Train Epoch: 15 [17440/19430 (90%)] Loss: 0.001934
Train Epoch: 15 [17600/19430 (91%)] Loss: 0.000850
Train Epoch: 15 [17760/19430 (91%)] Loss: 0.000675
Train Epoch: 15 [17920/19430 (92%)] Loss: 0.000680
Train Epoch: 15 [18080/19430 (93%)] Loss: 0.006313
Train Epoch: 15 [18240/19430 (94%)] Loss: 0.000368
Train Epoch: 15 [18400/19430 (95%)] Loss: 0.000700
Train Epoch: 15 [18560/19430 (96%)] Loss: 0.000844
Train Epoch: 15 [18720/19430 (96%)] Loss: 0.000365
Train Epoch: 15 [18880/19430 (97%)] Loss: 0.001661
Train Epoch: 15 [19040/19430 (98%)] Loss: 0.002086
Train Epoch: 15 [19200/19430 (99%)] Loss: 0.000517
Train Epoch: 15 [19360/19430 (100%)] Loss: 0.002893
    epoch          : 15
    Train_loss     : 0.007744587265661769
    Train_accuracy : 0.9976356907894737
    Train_f1_score : 0.9976357221603394
    Val_loss       : 0.030464905243313883
    Val_accuracy   : 0.9940257352941176
    Val_f1_score   : 0.9940257668495178
Train Epoch: 16 [0/19430 (0%)] Loss: 0.000558
Train Epoch: 16 [160/19430 (1%)] Loss: 0.000858
Train Epoch: 16 [320/19430 (2%)] Loss: 0.010302
Train Epoch: 16 [480/19430 (2%)] Loss: 0.000581
Train Epoch: 16 [640/19430 (3%)] Loss: 0.000576
Train Epoch: 16 [800/19430 (4%)] Loss: 0.000603
Train Epoch: 16 [960/19430 (5%)] Loss: 0.000900
Train Epoch: 16 [1120/19430 (6%)] Loss: 0.000182
Train Epoch: 16 [1280/19430 (7%)] Loss: 0.000447
Train Epoch: 16 [1440/19430 (7%)] Loss: 0.000169
Train Epoch: 16 [1600/19430 (8%)] Loss: 0.001297
Train Epoch: 16 [1760/19430 (9%)] Loss: 0.000390
Train Epoch: 16 [1920/19430 (10%)] Loss: 0.000580
Train Epoch: 16 [2080/19430 (11%)] Loss: 0.000586
Train Epoch: 16 [2240/19430 (12%)] Loss: 0.000905
Train Epoch: 16 [2400/19430 (12%)] Loss: 0.000304
Train Epoch: 16 [2560/19430 (13%)] Loss: 0.000214
Train Epoch: 16 [2720/19430 (14%)] Loss: 0.001004
Train Epoch: 16 [2880/19430 (15%)] Loss: 0.006096
Train Epoch: 16 [3040/19430 (16%)] Loss: 0.001159
Train Epoch: 16 [3200/19430 (16%)] Loss: 0.007768
Train Epoch: 16 [3360/19430 (17%)] Loss: 0.000467
Train Epoch: 16 [3520/19430 (18%)] Loss: 0.000188
Train Epoch: 16 [3680/19430 (19%)] Loss: 0.000219
Train Epoch: 16 [3840/19430 (20%)] Loss: 0.004820
Train Epoch: 16 [4000/19430 (21%)] Loss: 0.000249
Train Epoch: 16 [4160/19430 (21%)] Loss: 0.005230
Train Epoch: 16 [4320/19430 (22%)] Loss: 0.000583
Train Epoch: 16 [4480/19430 (23%)] Loss: 0.059088
Train Epoch: 16 [4640/19430 (24%)] Loss: 0.000658
Train Epoch: 16 [4800/19430 (25%)] Loss: 0.000772
Train Epoch: 16 [4960/19430 (26%)] Loss: 0.002346
Train Epoch: 16 [5120/19430 (26%)] Loss: 0.000498
Train Epoch: 16 [5280/19430 (27%)] Loss: 0.000677
Train Epoch: 16 [5440/19430 (28%)] Loss: 0.000197
Train Epoch: 16 [5600/19430 (29%)] Loss: 0.000475
Train Epoch: 16 [5760/19430 (30%)] Loss: 0.081538
Train Epoch: 16 [5920/19430 (30%)] Loss: 0.000846
Train Epoch: 16 [6080/19430 (31%)] Loss: 0.000776
Train Epoch: 16 [6240/19430 (32%)] Loss: 0.000481
Train Epoch: 16 [6400/19430 (33%)] Loss: 0.088932
Train Epoch: 16 [6560/19430 (34%)] Loss: 0.000195
Train Epoch: 16 [6720/19430 (35%)] Loss: 0.000220
Train Epoch: 16 [6880/19430 (35%)] Loss: 0.013943
Train Epoch: 16 [7040/19430 (36%)] Loss: 0.002973
Train Epoch: 16 [7200/19430 (37%)] Loss: 0.002855
Train Epoch: 16 [7360/19430 (38%)] Loss: 0.000168
Train Epoch: 16 [7520/19430 (39%)] Loss: 0.001495
Train Epoch: 16 [7680/19430 (40%)] Loss: 0.003011
Train Epoch: 16 [7840/19430 (40%)] Loss: 0.002553
Train Epoch: 16 [8000/19430 (41%)] Loss: 0.001685
Train Epoch: 16 [8160/19430 (42%)] Loss: 0.000595
Train Epoch: 16 [8320/19430 (43%)] Loss: 0.006008
Train Epoch: 16 [8480/19430 (44%)] Loss: 0.000666
Train Epoch: 16 [8640/19430 (44%)] Loss: 0.000875
Train Epoch: 16 [8800/19430 (45%)] Loss: 0.092133
Train Epoch: 16 [8960/19430 (46%)] Loss: 0.024420
Train Epoch: 16 [9120/19430 (47%)] Loss: 0.015018
Train Epoch: 16 [9280/19430 (48%)] Loss: 0.002778
Train Epoch: 16 [9440/19430 (49%)] Loss: 0.002728
Train Epoch: 16 [9600/19430 (49%)] Loss: 0.004440
Train Epoch: 16 [9760/19430 (50%)] Loss: 0.000455
Train Epoch: 16 [9920/19430 (51%)] Loss: 0.000348
Train Epoch: 16 [10080/19430 (52%)] Loss: 0.004921
Train Epoch: 16 [10240/19430 (53%)] Loss: 0.005097
Train Epoch: 16 [10400/19430 (54%)] Loss: 0.001306
Train Epoch: 16 [10560/19430 (54%)] Loss: 0.000353
Train Epoch: 16 [10720/19430 (55%)] Loss: 0.000563
Train Epoch: 16 [10880/19430 (56%)] Loss: 0.000501
Train Epoch: 16 [11040/19430 (57%)] Loss: 0.016084
Train Epoch: 16 [11200/19430 (58%)] Loss: 0.000499
Train Epoch: 16 [11360/19430 (58%)] Loss: 0.002444
Train Epoch: 16 [11520/19430 (59%)] Loss: 0.003066
Train Epoch: 16 [11680/19430 (60%)] Loss: 0.000243
Train Epoch: 16 [11840/19430 (61%)] Loss: 0.009986
Train Epoch: 16 [12000/19430 (62%)] Loss: 0.001739
Train Epoch: 16 [12160/19430 (63%)] Loss: 0.000381
Train Epoch: 16 [12320/19430 (63%)] Loss: 0.000546
Train Epoch: 16 [12480/19430 (64%)] Loss: 0.002457
Train Epoch: 16 [12640/19430 (65%)] Loss: 0.102262
Train Epoch: 16 [12800/19430 (66%)] Loss: 0.001883
Train Epoch: 16 [12960/19430 (67%)] Loss: 0.001488
Train Epoch: 16 [13120/19430 (68%)] Loss: 0.001432
Train Epoch: 16 [13280/19430 (68%)] Loss: 0.000081
Train Epoch: 16 [13440/19430 (69%)] Loss: 0.000439
Train Epoch: 16 [13600/19430 (70%)] Loss: 0.019000
Train Epoch: 16 [13760/19430 (71%)] Loss: 0.099703
Train Epoch: 16 [13920/19430 (72%)] Loss: 0.001769
Train Epoch: 16 [14080/19430 (72%)] Loss: 0.006102
Train Epoch: 16 [14240/19430 (73%)] Loss: 0.001558
Train Epoch: 16 [14400/19430 (74%)] Loss: 0.019894
Train Epoch: 16 [14560/19430 (75%)] Loss: 0.001338
Train Epoch: 16 [14720/19430 (76%)] Loss: 0.006759
Train Epoch: 16 [14880/19430 (77%)] Loss: 0.000705
Train Epoch: 16 [15040/19430 (77%)] Loss: 0.000896
Train Epoch: 16 [15200/19430 (78%)] Loss: 0.000510
Train Epoch: 16 [15360/19430 (79%)] Loss: 0.001300
Train Epoch: 16 [15520/19430 (80%)] Loss: 0.000189
Train Epoch: 16 [15680/19430 (81%)] Loss: 0.002183
Train Epoch: 16 [15840/19430 (82%)] Loss: 0.000784
Train Epoch: 16 [16000/19430 (82%)] Loss: 0.000168
Train Epoch: 16 [16160/19430 (83%)] Loss: 0.001439
Train Epoch: 16 [16320/19430 (84%)] Loss: 0.005197
Train Epoch: 16 [16480/19430 (85%)] Loss: 0.001910
Train Epoch: 16 [16640/19430 (86%)] Loss: 0.000316
Train Epoch: 16 [16800/19430 (86%)] Loss: 0.001099
Train Epoch: 16 [16960/19430 (87%)] Loss: 0.006737
Train Epoch: 16 [17120/19430 (88%)] Loss: 0.001317
Train Epoch: 16 [17280/19430 (89%)] Loss: 0.000081
Train Epoch: 16 [17440/19430 (90%)] Loss: 0.001259
Train Epoch: 16 [17600/19430 (91%)] Loss: 0.001937
Train Epoch: 16 [17760/19430 (91%)] Loss: 0.001363
Train Epoch: 16 [17920/19430 (92%)] Loss: 0.012978
Train Epoch: 16 [18080/19430 (93%)] Loss: 0.000555
Train Epoch: 16 [18240/19430 (94%)] Loss: 0.032652
Train Epoch: 16 [18400/19430 (95%)] Loss: 0.003205
Train Epoch: 16 [18560/19430 (96%)] Loss: 0.000110
Train Epoch: 16 [18720/19430 (96%)] Loss: 0.003660
Train Epoch: 16 [18880/19430 (97%)] Loss: 0.000281
Train Epoch: 16 [19040/19430 (98%)] Loss: 0.000439
Train Epoch: 16 [19200/19430 (99%)] Loss: 0.001967
Train Epoch: 16 [19360/19430 (100%)] Loss: 0.000890
    epoch          : 16
    Train_loss     : 0.00832273533050236
    Train_accuracy : 0.9974300986842105
    Train_f1_score : 0.9974300861358643
    Val_loss       : 0.037736977611797666
    Val_accuracy   : 0.9940257352941176
    Val_f1_score   : 0.9940257668495178
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1028_120308/checkpoint-epoch16.pth ...
Train Epoch: 17 [0/19430 (0%)] Loss: 0.000490
Train Epoch: 17 [160/19430 (1%)] Loss: 0.000500
Train Epoch: 17 [320/19430 (2%)] Loss: 0.000275
Train Epoch: 17 [480/19430 (2%)] Loss: 0.000188
Train Epoch: 17 [640/19430 (3%)] Loss: 0.000577
Train Epoch: 17 [800/19430 (4%)] Loss: 0.000927
Train Epoch: 17 [960/19430 (5%)] Loss: 0.002155
Train Epoch: 17 [1120/19430 (6%)] Loss: 0.060673
Train Epoch: 17 [1280/19430 (7%)] Loss: 0.001329
Train Epoch: 17 [1440/19430 (7%)] Loss: 0.101668
Train Epoch: 17 [1600/19430 (8%)] Loss: 0.000287
Train Epoch: 17 [1760/19430 (9%)] Loss: 0.000570
Train Epoch: 17 [1920/19430 (10%)] Loss: 0.000138
Train Epoch: 17 [2080/19430 (11%)] Loss: 0.000484
Train Epoch: 17 [2240/19430 (12%)] Loss: 0.000239
Train Epoch: 17 [2400/19430 (12%)] Loss: 0.000882
Train Epoch: 17 [2560/19430 (13%)] Loss: 0.001133
Train Epoch: 17 [2720/19430 (14%)] Loss: 0.000254
Train Epoch: 17 [2880/19430 (15%)] Loss: 0.005455
Train Epoch: 17 [3040/19430 (16%)] Loss: 0.000269
Train Epoch: 17 [3200/19430 (16%)] Loss: 0.000597
Train Epoch: 17 [3360/19430 (17%)] Loss: 0.022260
Train Epoch: 17 [3520/19430 (18%)] Loss: 0.000052
Train Epoch: 17 [3680/19430 (19%)] Loss: 0.000343
Train Epoch: 17 [3840/19430 (20%)] Loss: 0.002858
Train Epoch: 17 [4000/19430 (21%)] Loss: 0.001075
Train Epoch: 17 [4160/19430 (21%)] Loss: 0.000943
Train Epoch: 17 [4320/19430 (22%)] Loss: 0.000501
Train Epoch: 17 [4480/19430 (23%)] Loss: 0.001155
Train Epoch: 17 [4640/19430 (24%)] Loss: 0.000157
Train Epoch: 17 [4800/19430 (25%)] Loss: 0.002581
Train Epoch: 17 [4960/19430 (26%)] Loss: 0.000187
Train Epoch: 17 [5120/19430 (26%)] Loss: 0.000179
Train Epoch: 17 [5280/19430 (27%)] Loss: 0.028983
Train Epoch: 17 [5440/19430 (28%)] Loss: 0.000629
Train Epoch: 17 [5600/19430 (29%)] Loss: 0.000071
Train Epoch: 17 [5760/19430 (30%)] Loss: 0.003491
Train Epoch: 17 [5920/19430 (30%)] Loss: 0.000218
Train Epoch: 17 [6080/19430 (31%)] Loss: 0.008011
Train Epoch: 17 [6240/19430 (32%)] Loss: 0.000495
Train Epoch: 17 [6400/19430 (33%)] Loss: 0.017027
Train Epoch: 17 [6560/19430 (34%)] Loss: 0.000288
Train Epoch: 17 [6720/19430 (35%)] Loss: 0.000264
Train Epoch: 17 [6880/19430 (35%)] Loss: 0.126721
Train Epoch: 17 [7040/19430 (36%)] Loss: 0.000275
Train Epoch: 17 [7200/19430 (37%)] Loss: 0.000462
Train Epoch: 17 [7360/19430 (38%)] Loss: 0.063375
Train Epoch: 17 [7520/19430 (39%)] Loss: 0.000093
Train Epoch: 17 [7680/19430 (40%)] Loss: 0.001652
Train Epoch: 17 [7840/19430 (40%)] Loss: 0.000605
Train Epoch: 17 [8000/19430 (41%)] Loss: 0.000757
Train Epoch: 17 [8160/19430 (42%)] Loss: 0.000309
Train Epoch: 17 [8320/19430 (43%)] Loss: 0.000766
Train Epoch: 17 [8480/19430 (44%)] Loss: 0.003427
Train Epoch: 17 [8640/19430 (44%)] Loss: 0.000583
Train Epoch: 17 [8800/19430 (45%)] Loss: 0.002063
Train Epoch: 17 [8960/19430 (46%)] Loss: 0.000338
Train Epoch: 17 [9120/19430 (47%)] Loss: 0.000171
Train Epoch: 17 [9280/19430 (48%)] Loss: 0.009736
Train Epoch: 17 [9440/19430 (49%)] Loss: 0.038700
Train Epoch: 17 [9600/19430 (49%)] Loss: 0.001200
Train Epoch: 17 [9760/19430 (50%)] Loss: 0.000583
Train Epoch: 17 [9920/19430 (51%)] Loss: 0.000127
Train Epoch: 17 [10080/19430 (52%)] Loss: 0.000325
Train Epoch: 17 [10240/19430 (53%)] Loss: 0.048873
Train Epoch: 17 [10400/19430 (54%)] Loss: 0.000683
Train Epoch: 17 [10560/19430 (54%)] Loss: 0.000365
Train Epoch: 17 [10720/19430 (55%)] Loss: 0.119292
Train Epoch: 17 [10880/19430 (56%)] Loss: 0.000333
Train Epoch: 17 [11040/19430 (57%)] Loss: 0.000277
Train Epoch: 17 [11200/19430 (58%)] Loss: 0.000540
Train Epoch: 17 [11360/19430 (58%)] Loss: 0.009223
Train Epoch: 17 [11520/19430 (59%)] Loss: 0.031303
Train Epoch: 17 [11680/19430 (60%)] Loss: 0.000617
Train Epoch: 17 [11840/19430 (61%)] Loss: 0.000162
Train Epoch: 17 [12000/19430 (62%)] Loss: 0.000424
Train Epoch: 17 [12160/19430 (63%)] Loss: 0.000759
Train Epoch: 17 [12320/19430 (63%)] Loss: 0.008629
Train Epoch: 17 [12480/19430 (64%)] Loss: 0.000202
Train Epoch: 17 [12640/19430 (65%)] Loss: 0.003233
Train Epoch: 17 [12800/19430 (66%)] Loss: 0.000103
Train Epoch: 17 [12960/19430 (67%)] Loss: 0.000462
Train Epoch: 17 [13120/19430 (68%)] Loss: 0.002424
Train Epoch: 17 [13280/19430 (68%)] Loss: 0.000916
Train Epoch: 17 [13440/19430 (69%)] Loss: 0.000365
Train Epoch: 17 [13600/19430 (70%)] Loss: 0.000847
Train Epoch: 17 [13760/19430 (71%)] Loss: 0.001801
Train Epoch: 17 [13920/19430 (72%)] Loss: 0.001014
Train Epoch: 17 [14080/19430 (72%)] Loss: 0.000701
Train Epoch: 17 [14240/19430 (73%)] Loss: 0.000194
Train Epoch: 17 [14400/19430 (74%)] Loss: 0.001277
Train Epoch: 17 [14560/19430 (75%)] Loss: 0.000247
Train Epoch: 17 [14720/19430 (76%)] Loss: 0.000688
Train Epoch: 17 [14880/19430 (77%)] Loss: 0.012157
Train Epoch: 17 [15040/19430 (77%)] Loss: 0.001399
Train Epoch: 17 [15200/19430 (78%)] Loss: 0.000327
Train Epoch: 17 [15360/19430 (79%)] Loss: 0.001842
Train Epoch: 17 [15520/19430 (80%)] Loss: 0.003511
Train Epoch: 17 [15680/19430 (81%)] Loss: 0.000191
Train Epoch: 17 [15840/19430 (82%)] Loss: 0.000141
Train Epoch: 17 [16000/19430 (82%)] Loss: 0.000766
Train Epoch: 17 [16160/19430 (83%)] Loss: 0.000777
Train Epoch: 17 [16320/19430 (84%)] Loss: 0.001397
Train Epoch: 17 [16480/19430 (85%)] Loss: 0.004152
Train Epoch: 17 [16640/19430 (86%)] Loss: 0.000295
Train Epoch: 17 [16800/19430 (86%)] Loss: 0.001140
Train Epoch: 17 [16960/19430 (87%)] Loss: 0.000528
Train Epoch: 17 [17120/19430 (88%)] Loss: 0.000294
Train Epoch: 17 [17280/19430 (89%)] Loss: 0.000223
Train Epoch: 17 [17440/19430 (90%)] Loss: 0.000359
Train Epoch: 17 [17600/19430 (91%)] Loss: 0.004324
Train Epoch: 17 [17760/19430 (91%)] Loss: 0.000192
Train Epoch: 17 [17920/19430 (92%)] Loss: 0.000187
Train Epoch: 17 [18080/19430 (93%)] Loss: 0.001212
Train Epoch: 17 [18240/19430 (94%)] Loss: 0.000236
Train Epoch: 17 [18400/19430 (95%)] Loss: 0.002760
Train Epoch: 17 [18560/19430 (96%)] Loss: 0.000056
Train Epoch: 17 [18720/19430 (96%)] Loss: 0.000427
Train Epoch: 17 [18880/19430 (97%)] Loss: 0.004230
Train Epoch: 17 [19040/19430 (98%)] Loss: 0.000070
Train Epoch: 17 [19200/19430 (99%)] Loss: 0.000109
Train Epoch: 17 [19360/19430 (100%)] Loss: 0.000210
    epoch          : 17
    Train_loss     : 0.006025435507593245
    Train_accuracy : 0.9984066611842105
    Train_f1_score : 0.9984066486358643
    Val_loss       : 0.019768158087251377
    Val_accuracy   : 0.9954044117647058
    Val_f1_score   : 0.9954044222831726
Train Epoch: 18 [0/19430 (0%)] Loss: 0.000418
Train Epoch: 18 [160/19430 (1%)] Loss: 0.012510
Train Epoch: 18 [320/19430 (2%)] Loss: 0.000247
Train Epoch: 18 [480/19430 (2%)] Loss: 0.000048
Train Epoch: 18 [640/19430 (3%)] Loss: 0.000201
Train Epoch: 18 [800/19430 (4%)] Loss: 0.052651
Train Epoch: 18 [960/19430 (5%)] Loss: 0.000090
Train Epoch: 18 [1120/19430 (6%)] Loss: 0.001414
Train Epoch: 18 [1280/19430 (7%)] Loss: 0.012992
Train Epoch: 18 [1440/19430 (7%)] Loss: 0.010990
Train Epoch: 18 [1600/19430 (8%)] Loss: 0.001347
Train Epoch: 18 [1760/19430 (9%)] Loss: 0.001045
Train Epoch: 18 [1920/19430 (10%)] Loss: 0.000065
Train Epoch: 18 [2080/19430 (11%)] Loss: 0.006236
Train Epoch: 18 [2240/19430 (12%)] Loss: 0.000175
Train Epoch: 18 [2400/19430 (12%)] Loss: 0.001042
Train Epoch: 18 [2560/19430 (13%)] Loss: 0.000113
Train Epoch: 18 [2720/19430 (14%)] Loss: 0.011384
Train Epoch: 18 [2880/19430 (15%)] Loss: 0.000247
Train Epoch: 18 [3040/19430 (16%)] Loss: 0.000304
Train Epoch: 18 [3200/19430 (16%)] Loss: 0.000150
Train Epoch: 18 [3360/19430 (17%)] Loss: 0.000132
Train Epoch: 18 [3520/19430 (18%)] Loss: 0.000129
Train Epoch: 18 [3680/19430 (19%)] Loss: 0.000225
Train Epoch: 18 [3840/19430 (20%)] Loss: 0.182519
Train Epoch: 18 [4000/19430 (21%)] Loss: 0.000106
Train Epoch: 18 [4160/19430 (21%)] Loss: 0.000176
Train Epoch: 18 [4320/19430 (22%)] Loss: 0.000221
Train Epoch: 18 [4480/19430 (23%)] Loss: 0.000245
Train Epoch: 18 [4640/19430 (24%)] Loss: 0.000043
Train Epoch: 18 [4800/19430 (25%)] Loss: 0.000180
Train Epoch: 18 [4960/19430 (26%)] Loss: 0.000239
Train Epoch: 18 [5120/19430 (26%)] Loss: 0.000359
Train Epoch: 18 [5280/19430 (27%)] Loss: 0.000345
Train Epoch: 18 [5440/19430 (28%)] Loss: 0.000173
Train Epoch: 18 [5600/19430 (29%)] Loss: 0.000376
Train Epoch: 18 [5760/19430 (30%)] Loss: 0.000235
Train Epoch: 18 [5920/19430 (30%)] Loss: 0.000198
Train Epoch: 18 [6080/19430 (31%)] Loss: 0.000065
Train Epoch: 18 [6240/19430 (32%)] Loss: 0.001020
Train Epoch: 18 [6400/19430 (33%)] Loss: 0.014202
Train Epoch: 18 [6560/19430 (34%)] Loss: 0.002647
Train Epoch: 18 [6720/19430 (35%)] Loss: 0.000564
Train Epoch: 18 [6880/19430 (35%)] Loss: 0.000327
Train Epoch: 18 [7040/19430 (36%)] Loss: 0.000242
Train Epoch: 18 [7200/19430 (37%)] Loss: 0.000095
Train Epoch: 18 [7360/19430 (38%)] Loss: 0.000380
Train Epoch: 18 [7520/19430 (39%)] Loss: 0.000952
Train Epoch: 18 [7680/19430 (40%)] Loss: 0.000066
Train Epoch: 18 [7840/19430 (40%)] Loss: 0.001001
Train Epoch: 18 [8000/19430 (41%)] Loss: 0.001147
Train Epoch: 18 [8160/19430 (42%)] Loss: 0.005950
Train Epoch: 18 [8320/19430 (43%)] Loss: 0.000342
Train Epoch: 18 [8480/19430 (44%)] Loss: 0.011380
Train Epoch: 18 [8640/19430 (44%)] Loss: 0.335033
Train Epoch: 18 [8800/19430 (45%)] Loss: 0.007211
Train Epoch: 18 [8960/19430 (46%)] Loss: 0.000307
Train Epoch: 18 [9120/19430 (47%)] Loss: 0.000434
Train Epoch: 18 [9280/19430 (48%)] Loss: 0.000086
Train Epoch: 18 [9440/19430 (49%)] Loss: 0.003698
Train Epoch: 18 [9600/19430 (49%)] Loss: 0.000900
Train Epoch: 18 [9760/19430 (50%)] Loss: 0.000270
Train Epoch: 18 [9920/19430 (51%)] Loss: 0.000136
Train Epoch: 18 [10080/19430 (52%)] Loss: 0.000330
Train Epoch: 18 [10240/19430 (53%)] Loss: 0.000562
Train Epoch: 18 [10400/19430 (54%)] Loss: 0.001082
Train Epoch: 18 [10560/19430 (54%)] Loss: 0.000221
Train Epoch: 18 [10720/19430 (55%)] Loss: 0.000833
Train Epoch: 18 [10880/19430 (56%)] Loss: 0.000156
Train Epoch: 18 [11040/19430 (57%)] Loss: 0.002049
Train Epoch: 18 [11200/19430 (58%)] Loss: 0.000226
Train Epoch: 18 [11360/19430 (58%)] Loss: 0.002252
Train Epoch: 18 [11520/19430 (59%)] Loss: 0.001887
Train Epoch: 18 [11680/19430 (60%)] Loss: 0.000261
Train Epoch: 18 [11840/19430 (61%)] Loss: 0.003737
Train Epoch: 18 [12000/19430 (62%)] Loss: 0.000365
Train Epoch: 18 [12160/19430 (63%)] Loss: 0.000280
Train Epoch: 18 [12320/19430 (63%)] Loss: 0.000272
Train Epoch: 18 [12480/19430 (64%)] Loss: 0.013853
Train Epoch: 18 [12640/19430 (65%)] Loss: 0.000123
Train Epoch: 18 [12800/19430 (66%)] Loss: 0.000154
Train Epoch: 18 [12960/19430 (67%)] Loss: 0.008091
Train Epoch: 18 [13120/19430 (68%)] Loss: 0.000120
Train Epoch: 18 [13280/19430 (68%)] Loss: 0.000391
Train Epoch: 18 [13440/19430 (69%)] Loss: 0.000272
Train Epoch: 18 [13600/19430 (70%)] Loss: 0.000108
Train Epoch: 18 [13760/19430 (71%)] Loss: 0.002360
Train Epoch: 18 [13920/19430 (72%)] Loss: 0.000186
Train Epoch: 18 [14080/19430 (72%)] Loss: 0.000168
Train Epoch: 18 [14240/19430 (73%)] Loss: 0.002611
Train Epoch: 18 [14400/19430 (74%)] Loss: 0.000291
Train Epoch: 18 [14560/19430 (75%)] Loss: 0.000099
Train Epoch: 18 [14720/19430 (76%)] Loss: 0.000074
Train Epoch: 18 [14880/19430 (77%)] Loss: 0.001299
Train Epoch: 18 [15040/19430 (77%)] Loss: 0.000511
Train Epoch: 18 [15200/19430 (78%)] Loss: 0.000329
Train Epoch: 18 [15360/19430 (79%)] Loss: 0.000463
Train Epoch: 18 [15520/19430 (80%)] Loss: 0.001851
Train Epoch: 18 [15680/19430 (81%)] Loss: 0.000132
Train Epoch: 18 [15840/19430 (82%)] Loss: 0.000190
Train Epoch: 18 [16000/19430 (82%)] Loss: 0.000079
Train Epoch: 18 [16160/19430 (83%)] Loss: 0.000422
Train Epoch: 18 [16320/19430 (84%)] Loss: 0.000658
Train Epoch: 18 [16480/19430 (85%)] Loss: 0.000304
Train Epoch: 18 [16640/19430 (86%)] Loss: 0.000183
Train Epoch: 18 [16800/19430 (86%)] Loss: 0.000039
Train Epoch: 18 [16960/19430 (87%)] Loss: 0.000049
Train Epoch: 18 [17120/19430 (88%)] Loss: 0.000131
Train Epoch: 18 [17280/19430 (89%)] Loss: 0.000141
Train Epoch: 18 [17440/19430 (90%)] Loss: 0.000103
Train Epoch: 18 [17600/19430 (91%)] Loss: 0.000117
Train Epoch: 18 [17760/19430 (91%)] Loss: 0.000162
Train Epoch: 18 [17920/19430 (92%)] Loss: 0.000109
Train Epoch: 18 [18080/19430 (93%)] Loss: 0.004390
Train Epoch: 18 [18240/19430 (94%)] Loss: 0.000385
Train Epoch: 18 [18400/19430 (95%)] Loss: 0.000155
Train Epoch: 18 [18560/19430 (96%)] Loss: 0.000305
Train Epoch: 18 [18720/19430 (96%)] Loss: 0.000260
Train Epoch: 18 [18880/19430 (97%)] Loss: 0.000047
Train Epoch: 18 [19040/19430 (98%)] Loss: 0.000039
Train Epoch: 18 [19200/19430 (99%)] Loss: 0.000059
Train Epoch: 18 [19360/19430 (100%)] Loss: 0.000059
    epoch          : 18
    Train_loss     : 0.003104466886150859
    Train_accuracy : 0.9993832236842105
    Train_f1_score : 0.9993832111358643
    Val_loss       : 0.020405856017332018
    Val_accuracy   : 0.9961922268907564
    Val_f1_score   : 0.9961922764778137
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1028_120308/checkpoint-epoch18.pth ...
Train Epoch: 19 [0/19430 (0%)] Loss: 0.000137
Train Epoch: 19 [160/19430 (1%)] Loss: 0.001121
Train Epoch: 19 [320/19430 (2%)] Loss: 0.000182
Train Epoch: 19 [480/19430 (2%)] Loss: 0.000106
Train Epoch: 19 [640/19430 (3%)] Loss: 0.000141
Train Epoch: 19 [800/19430 (4%)] Loss: 0.000133
Train Epoch: 19 [960/19430 (5%)] Loss: 0.000205
Train Epoch: 19 [1120/19430 (6%)] Loss: 0.000166
Train Epoch: 19 [1280/19430 (7%)] Loss: 0.000833
Train Epoch: 19 [1440/19430 (7%)] Loss: 0.000069
Train Epoch: 19 [1600/19430 (8%)] Loss: 0.000057
Train Epoch: 19 [1760/19430 (9%)] Loss: 0.068981
Train Epoch: 19 [1920/19430 (10%)] Loss: 0.001254
Train Epoch: 19 [2080/19430 (11%)] Loss: 0.000310
Train Epoch: 19 [2240/19430 (12%)] Loss: 0.000437
Train Epoch: 19 [2400/19430 (12%)] Loss: 0.000031
Train Epoch: 19 [2560/19430 (13%)] Loss: 0.000063
Train Epoch: 19 [2720/19430 (14%)] Loss: 0.000535
Train Epoch: 19 [2880/19430 (15%)] Loss: 0.000114
Train Epoch: 19 [3040/19430 (16%)] Loss: 0.000613
Train Epoch: 19 [3200/19430 (16%)] Loss: 0.000050
Train Epoch: 19 [3360/19430 (17%)] Loss: 0.000058
Train Epoch: 19 [3520/19430 (18%)] Loss: 0.000259
Train Epoch: 19 [3680/19430 (19%)] Loss: 0.000142
Train Epoch: 19 [3840/19430 (20%)] Loss: 0.000201
Train Epoch: 19 [4000/19430 (21%)] Loss: 0.001230
Train Epoch: 19 [4160/19430 (21%)] Loss: 0.000242
Train Epoch: 19 [4320/19430 (22%)] Loss: 0.000466
Train Epoch: 19 [4480/19430 (23%)] Loss: 0.000223
Train Epoch: 19 [4640/19430 (24%)] Loss: 0.002496
Train Epoch: 19 [4800/19430 (25%)] Loss: 0.000344
Train Epoch: 19 [4960/19430 (26%)] Loss: 0.000351
Train Epoch: 19 [5120/19430 (26%)] Loss: 0.000903
Train Epoch: 19 [5280/19430 (27%)] Loss: 0.000316
Train Epoch: 19 [5440/19430 (28%)] Loss: 0.000109
Train Epoch: 19 [5600/19430 (29%)] Loss: 0.000131
Train Epoch: 19 [5760/19430 (30%)] Loss: 0.000211
Train Epoch: 19 [5920/19430 (30%)] Loss: 0.000058
Train Epoch: 19 [6080/19430 (31%)] Loss: 0.000041
Train Epoch: 19 [6240/19430 (32%)] Loss: 0.000066
Train Epoch: 19 [6400/19430 (33%)] Loss: 0.000290
Train Epoch: 19 [6560/19430 (34%)] Loss: 0.000155
Train Epoch: 19 [6720/19430 (35%)] Loss: 0.000789
Train Epoch: 19 [6880/19430 (35%)] Loss: 0.000084
Train Epoch: 19 [7040/19430 (36%)] Loss: 0.001493
Train Epoch: 19 [7200/19430 (37%)] Loss: 0.000533
Train Epoch: 19 [7360/19430 (38%)] Loss: 0.000087
Train Epoch: 19 [7520/19430 (39%)] Loss: 0.000375
Train Epoch: 19 [7680/19430 (40%)] Loss: 0.000172
Train Epoch: 19 [7840/19430 (40%)] Loss: 0.002563
Train Epoch: 19 [8000/19430 (41%)] Loss: 0.000186
Train Epoch: 19 [8160/19430 (42%)] Loss: 0.000082
Train Epoch: 19 [8320/19430 (43%)] Loss: 0.000382
Train Epoch: 19 [8480/19430 (44%)] Loss: 0.000077
Train Epoch: 19 [8640/19430 (44%)] Loss: 0.000027
Train Epoch: 19 [8800/19430 (45%)] Loss: 0.000077
Train Epoch: 19 [8960/19430 (46%)] Loss: 0.000254
Train Epoch: 19 [9120/19430 (47%)] Loss: 0.000490
Train Epoch: 19 [9280/19430 (48%)] Loss: 0.000435
Train Epoch: 19 [9440/19430 (49%)] Loss: 0.001780
Train Epoch: 19 [9600/19430 (49%)] Loss: 0.000046
Train Epoch: 19 [9760/19430 (50%)] Loss: 0.000156
Train Epoch: 19 [9920/19430 (51%)] Loss: 0.001081
Train Epoch: 19 [10080/19430 (52%)] Loss: 0.000826
Train Epoch: 19 [10240/19430 (53%)] Loss: 0.000869
Train Epoch: 19 [10400/19430 (54%)] Loss: 0.000279
Train Epoch: 19 [10560/19430 (54%)] Loss: 0.000044
Train Epoch: 19 [10720/19430 (55%)] Loss: 0.000096
Train Epoch: 19 [10880/19430 (56%)] Loss: 0.000206
Train Epoch: 19 [11040/19430 (57%)] Loss: 0.000067
Train Epoch: 19 [11200/19430 (58%)] Loss: 0.000400
Train Epoch: 19 [11360/19430 (58%)] Loss: 0.000036
Train Epoch: 19 [11520/19430 (59%)] Loss: 0.000045
Train Epoch: 19 [11680/19430 (60%)] Loss: 0.001157
Train Epoch: 19 [11840/19430 (61%)] Loss: 0.000471
Train Epoch: 19 [12000/19430 (62%)] Loss: 0.000117
Train Epoch: 19 [12160/19430 (63%)] Loss: 0.006088
Train Epoch: 19 [12320/19430 (63%)] Loss: 0.000285
Train Epoch: 19 [12480/19430 (64%)] Loss: 0.000181
Train Epoch: 19 [12640/19430 (65%)] Loss: 0.000110
Train Epoch: 19 [12800/19430 (66%)] Loss: 0.000186
Train Epoch: 19 [12960/19430 (67%)] Loss: 0.000350
Train Epoch: 19 [13120/19430 (68%)] Loss: 0.000212
Train Epoch: 19 [13280/19430 (68%)] Loss: 0.000189
Train Epoch: 19 [13440/19430 (69%)] Loss: 0.000125
Train Epoch: 19 [13600/19430 (70%)] Loss: 0.000101
Train Epoch: 19 [13760/19430 (71%)] Loss: 0.000119
Train Epoch: 19 [13920/19430 (72%)] Loss: 0.000219
Train Epoch: 19 [14080/19430 (72%)] Loss: 0.001734
Train Epoch: 19 [14240/19430 (73%)] Loss: 0.000092
Train Epoch: 19 [14400/19430 (74%)] Loss: 0.000296
Train Epoch: 19 [14560/19430 (75%)] Loss: 0.000565
Train Epoch: 19 [14720/19430 (76%)] Loss: 0.000517
Train Epoch: 19 [14880/19430 (77%)] Loss: 0.000495
Train Epoch: 19 [15040/19430 (77%)] Loss: 0.000415
Train Epoch: 19 [15200/19430 (78%)] Loss: 0.000176
Train Epoch: 19 [15360/19430 (79%)] Loss: 0.000619
Train Epoch: 19 [15520/19430 (80%)] Loss: 0.003084
Train Epoch: 19 [15680/19430 (81%)] Loss: 0.000504
Train Epoch: 19 [15840/19430 (82%)] Loss: 0.001729
Train Epoch: 19 [16000/19430 (82%)] Loss: 0.000171
Train Epoch: 19 [16160/19430 (83%)] Loss: 0.000074
Train Epoch: 19 [16320/19430 (84%)] Loss: 0.000207
Train Epoch: 19 [16480/19430 (85%)] Loss: 0.002707
Train Epoch: 19 [16640/19430 (86%)] Loss: 0.000155
Train Epoch: 19 [16800/19430 (86%)] Loss: 0.000122
Train Epoch: 19 [16960/19430 (87%)] Loss: 0.001217
Train Epoch: 19 [17120/19430 (88%)] Loss: 0.001728
Train Epoch: 19 [17280/19430 (89%)] Loss: 0.000493
Train Epoch: 19 [17440/19430 (90%)] Loss: 0.000366
Train Epoch: 19 [17600/19430 (91%)] Loss: 0.001602
Train Epoch: 19 [17760/19430 (91%)] Loss: 0.013234
Train Epoch: 19 [17920/19430 (92%)] Loss: 0.003696
Train Epoch: 19 [18080/19430 (93%)] Loss: 0.000074
Train Epoch: 19 [18240/19430 (94%)] Loss: 0.000377
Train Epoch: 19 [18400/19430 (95%)] Loss: 0.001196
Train Epoch: 19 [18560/19430 (96%)] Loss: 0.000281
Train Epoch: 19 [18720/19430 (96%)] Loss: 0.000232
Train Epoch: 19 [18880/19430 (97%)] Loss: 0.000785
Train Epoch: 19 [19040/19430 (98%)] Loss: 0.000100
Train Epoch: 19 [19200/19430 (99%)] Loss: 0.000375
Train Epoch: 19 [19360/19430 (100%)] Loss: 0.000246
    epoch          : 19
    Train_loss     : 0.0037625189857133016
    Train_accuracy : 0.9991262335526315
    Train_f1_score : 0.9991262555122375
    Val_loss       : 0.03315898433923875
    Val_accuracy   : 0.9935661764705882
    Val_f1_score   : 0.9935661554336548
Train Epoch: 20 [0/19430 (0%)] Loss: 0.001311
Train Epoch: 20 [160/19430 (1%)] Loss: 0.001149
Train Epoch: 20 [320/19430 (2%)] Loss: 0.000283
Train Epoch: 20 [480/19430 (2%)] Loss: 0.000253
Train Epoch: 20 [640/19430 (3%)] Loss: 0.000176
Train Epoch: 20 [800/19430 (4%)] Loss: 0.000381
Train Epoch: 20 [960/19430 (5%)] Loss: 0.000142
Train Epoch: 20 [1120/19430 (6%)] Loss: 0.000687
Train Epoch: 20 [1280/19430 (7%)] Loss: 0.007109
Train Epoch: 20 [1440/19430 (7%)] Loss: 0.000921
Train Epoch: 20 [1600/19430 (8%)] Loss: 0.000276
Train Epoch: 20 [1760/19430 (9%)] Loss: 0.000925
Train Epoch: 20 [1920/19430 (10%)] Loss: 0.000116
Train Epoch: 20 [2080/19430 (11%)] Loss: 0.001130
Train Epoch: 20 [2240/19430 (12%)] Loss: 0.010574
Train Epoch: 20 [2400/19430 (12%)] Loss: 0.000198
Train Epoch: 20 [2560/19430 (13%)] Loss: 0.001672
Train Epoch: 20 [2720/19430 (14%)] Loss: 0.000183
Train Epoch: 20 [2880/19430 (15%)] Loss: 0.002834
Train Epoch: 20 [3040/19430 (16%)] Loss: 0.000063
Train Epoch: 20 [3200/19430 (16%)] Loss: 0.000339
Train Epoch: 20 [3360/19430 (17%)] Loss: 0.000039
Train Epoch: 20 [3520/19430 (18%)] Loss: 0.000071
Train Epoch: 20 [3680/19430 (19%)] Loss: 0.000127
Train Epoch: 20 [3840/19430 (20%)] Loss: 0.000111
Train Epoch: 20 [4000/19430 (21%)] Loss: 0.000555
Train Epoch: 20 [4160/19430 (21%)] Loss: 0.000122
Train Epoch: 20 [4320/19430 (22%)] Loss: 0.000149
Train Epoch: 20 [4480/19430 (23%)] Loss: 0.000175
Train Epoch: 20 [4640/19430 (24%)] Loss: 0.000123
Train Epoch: 20 [4800/19430 (25%)] Loss: 0.000120
Train Epoch: 20 [4960/19430 (26%)] Loss: 0.001157
Train Epoch: 20 [5120/19430 (26%)] Loss: 0.000234
Train Epoch: 20 [5280/19430 (27%)] Loss: 0.000055
Train Epoch: 20 [5440/19430 (28%)] Loss: 0.001948
Train Epoch: 20 [5600/19430 (29%)] Loss: 0.000081
Train Epoch: 20 [5760/19430 (30%)] Loss: 0.001717
Train Epoch: 20 [5920/19430 (30%)] Loss: 0.000081
Train Epoch: 20 [6080/19430 (31%)] Loss: 0.000229
Train Epoch: 20 [6240/19430 (32%)] Loss: 0.000293
Train Epoch: 20 [6400/19430 (33%)] Loss: 0.000330
Train Epoch: 20 [6560/19430 (34%)] Loss: 0.000037
Train Epoch: 20 [6720/19430 (35%)] Loss: 0.000106
Train Epoch: 20 [6880/19430 (35%)] Loss: 0.000309
Train Epoch: 20 [7040/19430 (36%)] Loss: 0.000230
Train Epoch: 20 [7200/19430 (37%)] Loss: 0.000089
Train Epoch: 20 [7360/19430 (38%)] Loss: 0.001631
Train Epoch: 20 [7520/19430 (39%)] Loss: 0.001791
Train Epoch: 20 [7680/19430 (40%)] Loss: 0.000081
Train Epoch: 20 [7840/19430 (40%)] Loss: 0.001337
Train Epoch: 20 [8000/19430 (41%)] Loss: 0.000364
Train Epoch: 20 [8160/19430 (42%)] Loss: 0.000087
Train Epoch: 20 [8320/19430 (43%)] Loss: 0.000300
Train Epoch: 20 [8480/19430 (44%)] Loss: 0.000309
Train Epoch: 20 [8640/19430 (44%)] Loss: 0.000366
Train Epoch: 20 [8800/19430 (45%)] Loss: 0.000115
Train Epoch: 20 [8960/19430 (46%)] Loss: 0.000164
Train Epoch: 20 [9120/19430 (47%)] Loss: 0.000041
Train Epoch: 20 [9280/19430 (48%)] Loss: 0.000199
Train Epoch: 20 [9440/19430 (49%)] Loss: 0.000595
Train Epoch: 20 [9600/19430 (49%)] Loss: 0.000332
Train Epoch: 20 [9760/19430 (50%)] Loss: 0.000396
Train Epoch: 20 [9920/19430 (51%)] Loss: 0.000104
Train Epoch: 20 [10080/19430 (52%)] Loss: 0.000171
Train Epoch: 20 [10240/19430 (53%)] Loss: 0.000111
Train Epoch: 20 [10400/19430 (54%)] Loss: 0.000162
Train Epoch: 20 [10560/19430 (54%)] Loss: 0.000662
Train Epoch: 20 [10720/19430 (55%)] Loss: 0.000023
Train Epoch: 20 [10880/19430 (56%)] Loss: 0.000051
Train Epoch: 20 [11040/19430 (57%)] Loss: 0.000209
Train Epoch: 20 [11200/19430 (58%)] Loss: 0.000133
Train Epoch: 20 [11360/19430 (58%)] Loss: 0.000404
Train Epoch: 20 [11520/19430 (59%)] Loss: 0.000035
Train Epoch: 20 [11680/19430 (60%)] Loss: 0.000042
Train Epoch: 20 [11840/19430 (61%)] Loss: 0.000113
Train Epoch: 20 [12000/19430 (62%)] Loss: 0.000219
Train Epoch: 20 [12160/19430 (63%)] Loss: 0.000119
Train Epoch: 20 [12320/19430 (63%)] Loss: 0.000709
Train Epoch: 20 [12480/19430 (64%)] Loss: 0.000194
Train Epoch: 20 [12640/19430 (65%)] Loss: 0.000189
Train Epoch: 20 [12800/19430 (66%)] Loss: 0.000158
Train Epoch: 20 [12960/19430 (67%)] Loss: 0.000377
Train Epoch: 20 [13120/19430 (68%)] Loss: 0.000299
Train Epoch: 20 [13280/19430 (68%)] Loss: 0.000161
Train Epoch: 20 [13440/19430 (69%)] Loss: 0.185969
Train Epoch: 20 [13600/19430 (70%)] Loss: 0.000116
Train Epoch: 20 [13760/19430 (71%)] Loss: 0.001766
Train Epoch: 20 [13920/19430 (72%)] Loss: 0.000324
Train Epoch: 20 [14080/19430 (72%)] Loss: 0.142989
Train Epoch: 20 [14240/19430 (73%)] Loss: 0.000060
Train Epoch: 20 [14400/19430 (74%)] Loss: 0.000177
Train Epoch: 20 [14560/19430 (75%)] Loss: 0.000140
Train Epoch: 20 [14720/19430 (76%)] Loss: 0.089979
Train Epoch: 20 [14880/19430 (77%)] Loss: 0.001134
Train Epoch: 20 [15040/19430 (77%)] Loss: 0.013012
Train Epoch: 20 [15200/19430 (78%)] Loss: 0.002215
Train Epoch: 20 [15360/19430 (79%)] Loss: 0.004073
Train Epoch: 20 [15520/19430 (80%)] Loss: 0.000448
Train Epoch: 20 [15680/19430 (81%)] Loss: 0.001563
Train Epoch: 20 [15840/19430 (82%)] Loss: 0.000948
Train Epoch: 20 [16000/19430 (82%)] Loss: 0.000524
Train Epoch: 20 [16160/19430 (83%)] Loss: 0.000521
Train Epoch: 20 [16320/19430 (84%)] Loss: 0.001076
Train Epoch: 20 [16480/19430 (85%)] Loss: 0.000522
Train Epoch: 20 [16640/19430 (86%)] Loss: 0.004108
Train Epoch: 20 [16800/19430 (86%)] Loss: 0.000083
Train Epoch: 20 [16960/19430 (87%)] Loss: 0.000091
Train Epoch: 20 [17120/19430 (88%)] Loss: 0.001774
Train Epoch: 20 [17280/19430 (89%)] Loss: 0.000238
Train Epoch: 20 [17440/19430 (90%)] Loss: 0.000624
Train Epoch: 20 [17600/19430 (91%)] Loss: 0.000660
Train Epoch: 20 [17760/19430 (91%)] Loss: 0.000093
Train Epoch: 20 [17920/19430 (92%)] Loss: 0.000088
Train Epoch: 20 [18080/19430 (93%)] Loss: 0.000929
Train Epoch: 20 [18240/19430 (94%)] Loss: 0.001059
Train Epoch: 20 [18400/19430 (95%)] Loss: 0.019581
Train Epoch: 20 [18560/19430 (96%)] Loss: 0.000951
Train Epoch: 20 [18720/19430 (96%)] Loss: 0.001186
Train Epoch: 20 [18880/19430 (97%)] Loss: 0.000461
Train Epoch: 20 [19040/19430 (98%)] Loss: 0.000553
Train Epoch: 20 [19200/19430 (99%)] Loss: 0.000481
Train Epoch: 20 [19360/19430 (100%)] Loss: 0.000107
    epoch          : 20
    Train_loss     : 0.004876678770011422
    Train_accuracy : 0.998766447368421
    Train_f1_score : 0.9987664818763733
    Val_loss       : 0.04012695257602793
    Val_accuracy   : 0.9917279411764706
    Val_f1_score   : 0.9917279481887817
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1028_120308/checkpoint-epoch20.pth ...
/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Train_accuracy ‚ñá‚ñà‚ñà‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: Train_f1_score ‚ñá‚ñà‚ñà‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:     Train_loss ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   Val_accuracy ‚ñÉ‚ñá‚ñÇ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñÉ
wandb:   Val_f1_score ‚ñÉ‚ñá‚ñÇ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñÉ
wandb:       Val_loss ‚ñÑ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÖ‚ñÅ‚ñÅ‚ñÑ‚ñÜ
wandb: 
wandb: Run summary:
wandb: Train_accuracy 0.99877
wandb: Train_f1_score 0.99877
wandb:     Train_loss 0.00488
wandb:   Val_accuracy 0.99173
wandb:   Val_f1_score 0.99173
wandb:       Val_loss 0.04013
wandb: 
wandb: Synced floral-paper-37: https://wandb.ai/qwer55252/Boostcamp-lv1-cv1/runs/91ykk1ty
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221028_120305-91ykk1ty/logs
