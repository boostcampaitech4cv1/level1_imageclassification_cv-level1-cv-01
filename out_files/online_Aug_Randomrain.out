/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Currently logged in as: qwer55252. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /opt/ml/project-T4193/wandb/run-20221029_095831-covsy863
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-brook-87
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1
wandb: üöÄ View run at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1/runs/covsy863
Loaded pretrained weights for efficientnet-b7
EfficientNet(
  (_conv_stem): Conv2dStaticSamePadding(
    3, 64, kernel_size=(3, 3), stride=(2, 2), bias=False
    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
  )
  (_bn0): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_blocks): ModuleList(
    (0): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        64, 64, kernel_size=(3, 3), stride=[1, 1], groups=64, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        64, 16, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        16, 64, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (3): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (4): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        192, 192, kernel_size=(3, 3), stride=[2, 2], groups=192, bias=False
        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        192, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 192, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (5): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (6): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (7): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (8): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (9): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (10): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (11): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(5, 5), stride=[2, 2], groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (12): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (13): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (14): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (15): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (16): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (17): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (18): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(3, 3), stride=[2, 2], groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (19): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (20): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (21): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (22): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (23): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (24): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (25): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (26): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (27): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (28): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=[1, 1], groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (29): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (30): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (31): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (32): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (33): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (34): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (35): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (36): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (37): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (38): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=[2, 2], groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (39): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (40): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (41): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (42): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (43): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (44): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (45): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (46): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (47): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (48): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (49): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (50): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (51): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(3, 3), stride=[1, 1], groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (52): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (53): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (54): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (_conv_head): Conv2dStaticSamePadding(
    640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False
    (static_padding): Identity()
  )
  (_bn1): BatchNorm2d(2560, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)
  (_dropout): Dropout(p=0.5, inplace=False)
  (_fc): Linear(in_features=2560, out_features=18, bias=True)
  (_swish): MemoryEfficientSwish()
)
Train Epoch: 1 [0/17010 (0%)] Loss: 2.876418
Train Epoch: 1 [160/17010 (1%)] Loss: 2.027734
Train Epoch: 1 [320/17010 (2%)] Loss: 1.323727
Train Epoch: 1 [480/17010 (3%)] Loss: 0.948870
Train Epoch: 1 [640/17010 (4%)] Loss: 1.103250
Train Epoch: 1 [800/17010 (5%)] Loss: 0.919088
Train Epoch: 1 [960/17010 (6%)] Loss: 1.047104
Train Epoch: 1 [1120/17010 (7%)] Loss: 1.000319
Train Epoch: 1 [1280/17010 (8%)] Loss: 0.649856
Train Epoch: 1 [1440/17010 (8%)] Loss: 0.594666
Train Epoch: 1 [1600/17010 (9%)] Loss: 0.789768
Train Epoch: 1 [1760/17010 (10%)] Loss: 0.718236
Train Epoch: 1 [1920/17010 (11%)] Loss: 0.786607
Train Epoch: 1 [2080/17010 (12%)] Loss: 0.870672
Train Epoch: 1 [2240/17010 (13%)] Loss: 0.733931
Train Epoch: 1 [2400/17010 (14%)] Loss: 0.767486
Train Epoch: 1 [2560/17010 (15%)] Loss: 0.578812
Train Epoch: 1 [2720/17010 (16%)] Loss: 0.670828
Train Epoch: 1 [2880/17010 (17%)] Loss: 0.909524
Train Epoch: 1 [3040/17010 (18%)] Loss: 0.816314
Train Epoch: 1 [3200/17010 (19%)] Loss: 0.373359
Train Epoch: 1 [3360/17010 (20%)] Loss: 0.801054
Train Epoch: 1 [3520/17010 (21%)] Loss: 0.555452
Train Epoch: 1 [3680/17010 (22%)] Loss: 0.708493
Train Epoch: 1 [3840/17010 (23%)] Loss: 0.589190
Train Epoch: 1 [4000/17010 (24%)] Loss: 0.381201
Train Epoch: 1 [4160/17010 (24%)] Loss: 0.744152
Train Epoch: 1 [4320/17010 (25%)] Loss: 0.321500
Train Epoch: 1 [4480/17010 (26%)] Loss: 0.590665
Train Epoch: 1 [4640/17010 (27%)] Loss: 0.408880
Train Epoch: 1 [4800/17010 (28%)] Loss: 0.200073
Train Epoch: 1 [4960/17010 (29%)] Loss: 0.379901
Train Epoch: 1 [5120/17010 (30%)] Loss: 0.398564
Train Epoch: 1 [5280/17010 (31%)] Loss: 0.638431
Train Epoch: 1 [5440/17010 (32%)] Loss: 0.372054
Train Epoch: 1 [5600/17010 (33%)] Loss: 0.331238
Train Epoch: 1 [5760/17010 (34%)] Loss: 0.583517
Train Epoch: 1 [5920/17010 (35%)] Loss: 0.205584
Train Epoch: 1 [6080/17010 (36%)] Loss: 0.259290
Train Epoch: 1 [6240/17010 (37%)] Loss: 0.571154
Train Epoch: 1 [6400/17010 (38%)] Loss: 0.439357
Train Epoch: 1 [6560/17010 (39%)] Loss: 0.485382
Train Epoch: 1 [6720/17010 (40%)] Loss: 0.653908
Train Epoch: 1 [6880/17010 (40%)] Loss: 0.429507
Train Epoch: 1 [7040/17010 (41%)] Loss: 0.388662
Train Epoch: 1 [7200/17010 (42%)] Loss: 0.322068
Train Epoch: 1 [7360/17010 (43%)] Loss: 0.153407
Train Epoch: 1 [7520/17010 (44%)] Loss: 0.547338
Train Epoch: 1 [7680/17010 (45%)] Loss: 0.264033
Train Epoch: 1 [7840/17010 (46%)] Loss: 0.504073
Train Epoch: 1 [8000/17010 (47%)] Loss: 0.390443
Train Epoch: 1 [8160/17010 (48%)] Loss: 0.253605
Train Epoch: 1 [8320/17010 (49%)] Loss: 0.636235
Train Epoch: 1 [8480/17010 (50%)] Loss: 0.524402
Train Epoch: 1 [8640/17010 (51%)] Loss: 0.689663
Train Epoch: 1 [8800/17010 (52%)] Loss: 0.565215
Train Epoch: 1 [8960/17010 (53%)] Loss: 0.493655
Train Epoch: 1 [9120/17010 (54%)] Loss: 0.511468
Train Epoch: 1 [9280/17010 (55%)] Loss: 0.342798
Train Epoch: 1 [9440/17010 (55%)] Loss: 0.312458
Train Epoch: 1 [9600/17010 (56%)] Loss: 0.419249
Train Epoch: 1 [9760/17010 (57%)] Loss: 0.130818
Train Epoch: 1 [9920/17010 (58%)] Loss: 0.206910
Train Epoch: 1 [10080/17010 (59%)] Loss: 0.331454
Train Epoch: 1 [10240/17010 (60%)] Loss: 0.292528
Train Epoch: 1 [10400/17010 (61%)] Loss: 0.276970
Train Epoch: 1 [10560/17010 (62%)] Loss: 0.754656
Train Epoch: 1 [10720/17010 (63%)] Loss: 0.252636
Train Epoch: 1 [10880/17010 (64%)] Loss: 0.659080
Train Epoch: 1 [11040/17010 (65%)] Loss: 0.278850
Train Epoch: 1 [11200/17010 (66%)] Loss: 0.414198
Train Epoch: 1 [11360/17010 (67%)] Loss: 0.485981
Train Epoch: 1 [11520/17010 (68%)] Loss: 0.794346
Train Epoch: 1 [11680/17010 (69%)] Loss: 0.559502
Train Epoch: 1 [11840/17010 (70%)] Loss: 0.186207
Train Epoch: 1 [12000/17010 (71%)] Loss: 0.334099
Train Epoch: 1 [12160/17010 (71%)] Loss: 0.163039
Train Epoch: 1 [12320/17010 (72%)] Loss: 0.762799
Train Epoch: 1 [12480/17010 (73%)] Loss: 0.656290
Train Epoch: 1 [12640/17010 (74%)] Loss: 0.427766
Train Epoch: 1 [12800/17010 (75%)] Loss: 0.412119
Train Epoch: 1 [12960/17010 (76%)] Loss: 0.069571
Train Epoch: 1 [13120/17010 (77%)] Loss: 0.325558
Train Epoch: 1 [13280/17010 (78%)] Loss: 0.107908
Train Epoch: 1 [13440/17010 (79%)] Loss: 0.375199
Train Epoch: 1 [13600/17010 (80%)] Loss: 0.277329
Train Epoch: 1 [13760/17010 (81%)] Loss: 0.319277
Train Epoch: 1 [13920/17010 (82%)] Loss: 0.193342
Train Epoch: 1 [14080/17010 (83%)] Loss: 0.320808
Train Epoch: 1 [14240/17010 (84%)] Loss: 0.786875
Train Epoch: 1 [14400/17010 (85%)] Loss: 0.112037
Train Epoch: 1 [14560/17010 (86%)] Loss: 0.656732
Train Epoch: 1 [14720/17010 (87%)] Loss: 0.316759
Train Epoch: 1 [14880/17010 (87%)] Loss: 0.233591
Train Epoch: 1 [15040/17010 (88%)] Loss: 0.210495
Train Epoch: 1 [15200/17010 (89%)] Loss: 0.205471
Train Epoch: 1 [15360/17010 (90%)] Loss: 0.103204
Train Epoch: 1 [15520/17010 (91%)] Loss: 0.317238
Train Epoch: 1 [15680/17010 (92%)] Loss: 0.091370
Train Epoch: 1 [15840/17010 (93%)] Loss: 0.763715
Train Epoch: 1 [16000/17010 (94%)] Loss: 0.181734
Train Epoch: 1 [16160/17010 (95%)] Loss: 0.187350
Train Epoch: 1 [16320/17010 (96%)] Loss: 0.398908
Train Epoch: 1 [16480/17010 (97%)] Loss: 0.165985
Train Epoch: 1 [16640/17010 (98%)] Loss: 0.238797
Train Epoch: 1 [16800/17010 (99%)] Loss: 0.328756
Train Epoch: 1 [16960/17010 (100%)] Loss: 0.354721
    epoch          : 1
    Train_loss     : 0.5284097598641551
    Train_accuracy : 0.8354218880534671
    Train_f1_score : 0.8354219198226929
    Val_loss       : 0.27137778677085106
    Val_accuracy   : 0.9114583333333334
    Val_f1_score   : 0.9114583730697632
Warning: Metric 'val_loss' is not found. Model performance monitoring is disabled.
Train Epoch: 2 [0/17010 (0%)] Loss: 0.388379
Train Epoch: 2 [160/17010 (1%)] Loss: 0.063107
Train Epoch: 2 [320/17010 (2%)] Loss: 0.089188
Train Epoch: 2 [480/17010 (3%)] Loss: 0.216093
Train Epoch: 2 [640/17010 (4%)] Loss: 0.625235
Train Epoch: 2 [800/17010 (5%)] Loss: 0.076731
Train Epoch: 2 [960/17010 (6%)] Loss: 0.390627
Train Epoch: 2 [1120/17010 (7%)] Loss: 0.317090
Train Epoch: 2 [1280/17010 (8%)] Loss: 0.481257
Train Epoch: 2 [1440/17010 (8%)] Loss: 0.103079
Train Epoch: 2 [1600/17010 (9%)] Loss: 0.327493
Train Epoch: 2 [1760/17010 (10%)] Loss: 0.225016
Train Epoch: 2 [1920/17010 (11%)] Loss: 0.248177
Train Epoch: 2 [2080/17010 (12%)] Loss: 0.215479
Train Epoch: 2 [2240/17010 (13%)] Loss: 0.613472
Train Epoch: 2 [2400/17010 (14%)] Loss: 0.120145
Train Epoch: 2 [2560/17010 (15%)] Loss: 0.142920
Train Epoch: 2 [2720/17010 (16%)] Loss: 0.233295
Train Epoch: 2 [2880/17010 (17%)] Loss: 0.191524
Train Epoch: 2 [3040/17010 (18%)] Loss: 0.127813
Train Epoch: 2 [3200/17010 (19%)] Loss: 0.243087
Train Epoch: 2 [3360/17010 (20%)] Loss: 0.236356
Train Epoch: 2 [3520/17010 (21%)] Loss: 0.392309
Train Epoch: 2 [3680/17010 (22%)] Loss: 0.309703
Train Epoch: 2 [3840/17010 (23%)] Loss: 0.275173
Train Epoch: 2 [4000/17010 (24%)] Loss: 0.104668
Train Epoch: 2 [4160/17010 (24%)] Loss: 0.189350
Train Epoch: 2 [4320/17010 (25%)] Loss: 0.230968
Train Epoch: 2 [4480/17010 (26%)] Loss: 0.091001
Train Epoch: 2 [4640/17010 (27%)] Loss: 0.247939
Train Epoch: 2 [4800/17010 (28%)] Loss: 0.234142
Train Epoch: 2 [4960/17010 (29%)] Loss: 0.180849
Train Epoch: 2 [5120/17010 (30%)] Loss: 0.029017
Train Epoch: 2 [5280/17010 (31%)] Loss: 0.216625
Train Epoch: 2 [5440/17010 (32%)] Loss: 0.587858
Train Epoch: 2 [5600/17010 (33%)] Loss: 0.189002
Train Epoch: 2 [5760/17010 (34%)] Loss: 0.162310
Train Epoch: 2 [5920/17010 (35%)] Loss: 0.147579
Train Epoch: 2 [6080/17010 (36%)] Loss: 0.195593
Train Epoch: 2 [6240/17010 (37%)] Loss: 0.326553
Train Epoch: 2 [6400/17010 (38%)] Loss: 0.068615
Train Epoch: 2 [6560/17010 (39%)] Loss: 0.122735
Train Epoch: 2 [6720/17010 (40%)] Loss: 0.342423
Train Epoch: 2 [6880/17010 (40%)] Loss: 0.222784
Train Epoch: 2 [7040/17010 (41%)] Loss: 0.149964
Train Epoch: 2 [7200/17010 (42%)] Loss: 0.203039
Train Epoch: 2 [7360/17010 (43%)] Loss: 0.128696
Train Epoch: 2 [7520/17010 (44%)] Loss: 0.140214
Train Epoch: 2 [7680/17010 (45%)] Loss: 0.297515
Train Epoch: 2 [7840/17010 (46%)] Loss: 0.089290
Train Epoch: 2 [8000/17010 (47%)] Loss: 0.383594
Train Epoch: 2 [8160/17010 (48%)] Loss: 0.054027
Train Epoch: 2 [8320/17010 (49%)] Loss: 0.022176
Train Epoch: 2 [8480/17010 (50%)] Loss: 0.192383
Train Epoch: 2 [8640/17010 (51%)] Loss: 0.091631
Train Epoch: 2 [8800/17010 (52%)] Loss: 0.548259
Train Epoch: 2 [8960/17010 (53%)] Loss: 0.284563
Train Epoch: 2 [9120/17010 (54%)] Loss: 0.104786
Train Epoch: 2 [9280/17010 (55%)] Loss: 0.197496
Train Epoch: 2 [9440/17010 (55%)] Loss: 0.277873
Train Epoch: 2 [9600/17010 (56%)] Loss: 0.082804
Train Epoch: 2 [9760/17010 (57%)] Loss: 0.240222
Train Epoch: 2 [9920/17010 (58%)] Loss: 0.460775
Train Epoch: 2 [10080/17010 (59%)] Loss: 0.092069
Train Epoch: 2 [10240/17010 (60%)] Loss: 0.095878
Train Epoch: 2 [10400/17010 (61%)] Loss: 0.255074
Train Epoch: 2 [10560/17010 (62%)] Loss: 0.262549
Train Epoch: 2 [10720/17010 (63%)] Loss: 0.369605
Train Epoch: 2 [10880/17010 (64%)] Loss: 0.084134
Train Epoch: 2 [11040/17010 (65%)] Loss: 0.237579
Train Epoch: 2 [11200/17010 (66%)] Loss: 0.276158
Train Epoch: 2 [11360/17010 (67%)] Loss: 0.153042
Train Epoch: 2 [11520/17010 (68%)] Loss: 0.091295
Train Epoch: 2 [11680/17010 (69%)] Loss: 0.385448
Train Epoch: 2 [11840/17010 (70%)] Loss: 0.273830
Train Epoch: 2 [12000/17010 (71%)] Loss: 0.111769
Train Epoch: 2 [12160/17010 (71%)] Loss: 0.230346
Train Epoch: 2 [12320/17010 (72%)] Loss: 0.553850
Train Epoch: 2 [12480/17010 (73%)] Loss: 0.236514
Train Epoch: 2 [12640/17010 (74%)] Loss: 0.544654
Train Epoch: 2 [12800/17010 (75%)] Loss: 0.136168
Train Epoch: 2 [12960/17010 (76%)] Loss: 0.387310
Train Epoch: 2 [13120/17010 (77%)] Loss: 0.228781
Train Epoch: 2 [13280/17010 (78%)] Loss: 0.323198
Train Epoch: 2 [13440/17010 (79%)] Loss: 0.198588
Train Epoch: 2 [13600/17010 (80%)] Loss: 0.908205
Train Epoch: 2 [13760/17010 (81%)] Loss: 0.547451
Train Epoch: 2 [13920/17010 (82%)] Loss: 0.248063
Train Epoch: 2 [14080/17010 (83%)] Loss: 0.022832
Train Epoch: 2 [14240/17010 (84%)] Loss: 0.085235
Train Epoch: 2 [14400/17010 (85%)] Loss: 0.243008
Train Epoch: 2 [14560/17010 (86%)] Loss: 0.173491
Train Epoch: 2 [14720/17010 (87%)] Loss: 0.134861
Train Epoch: 2 [14880/17010 (87%)] Loss: 0.120247
Train Epoch: 2 [15040/17010 (88%)] Loss: 0.023369
Train Epoch: 2 [15200/17010 (89%)] Loss: 0.260813
Train Epoch: 2 [15360/17010 (90%)] Loss: 0.175584
Train Epoch: 2 [15520/17010 (91%)] Loss: 0.118248
Train Epoch: 2 [15680/17010 (92%)] Loss: 0.262776
Train Epoch: 2 [15840/17010 (93%)] Loss: 0.118897
Train Epoch: 2 [16000/17010 (94%)] Loss: 0.074997
Train Epoch: 2 [16160/17010 (95%)] Loss: 0.295915
Train Epoch: 2 [16320/17010 (96%)] Loss: 0.106888
Train Epoch: 2 [16480/17010 (97%)] Loss: 0.020454
Train Epoch: 2 [16640/17010 (98%)] Loss: 0.026258
Train Epoch: 2 [16800/17010 (99%)] Loss: 0.222588
Train Epoch: 2 [16960/17010 (100%)] Loss: 0.176726
    epoch          : 2
    Train_loss     : 0.21702528169686744
    Train_accuracy : 0.9295700187969925
    Train_f1_score : 0.9295700192451477
    Val_loss       : 0.2242881068222535
    Val_accuracy   : 0.9260416666666667
    Val_f1_score   : 0.9260417222976685
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch2.pth ...
Train Epoch: 3 [0/17010 (0%)] Loss: 0.055043
Train Epoch: 3 [160/17010 (1%)] Loss: 0.053211
Train Epoch: 3 [320/17010 (2%)] Loss: 0.189072
Train Epoch: 3 [480/17010 (3%)] Loss: 0.117935
Train Epoch: 3 [640/17010 (4%)] Loss: 0.190606
Train Epoch: 3 [800/17010 (5%)] Loss: 0.086897
Train Epoch: 3 [960/17010 (6%)] Loss: 0.115245
Train Epoch: 3 [1120/17010 (7%)] Loss: 0.173886
Train Epoch: 3 [1280/17010 (8%)] Loss: 0.055235
Train Epoch: 3 [1440/17010 (8%)] Loss: 0.272592
Train Epoch: 3 [1600/17010 (9%)] Loss: 0.030470
Train Epoch: 3 [1760/17010 (10%)] Loss: 0.312195
Train Epoch: 3 [1920/17010 (11%)] Loss: 0.430080
Train Epoch: 3 [2080/17010 (12%)] Loss: 0.085480
Train Epoch: 3 [2240/17010 (13%)] Loss: 0.109360
Train Epoch: 3 [2400/17010 (14%)] Loss: 0.214704
Train Epoch: 3 [2560/17010 (15%)] Loss: 0.057069
Train Epoch: 3 [2720/17010 (16%)] Loss: 0.036442
Train Epoch: 3 [2880/17010 (17%)] Loss: 0.074443
Train Epoch: 3 [3040/17010 (18%)] Loss: 0.035332
Train Epoch: 3 [3200/17010 (19%)] Loss: 0.185036
Train Epoch: 3 [3360/17010 (20%)] Loss: 0.037327
Train Epoch: 3 [3520/17010 (21%)] Loss: 0.074647
Train Epoch: 3 [3680/17010 (22%)] Loss: 0.091841
Train Epoch: 3 [3840/17010 (23%)] Loss: 0.134620
Train Epoch: 3 [4000/17010 (24%)] Loss: 0.411425
Train Epoch: 3 [4160/17010 (24%)] Loss: 0.059346
Train Epoch: 3 [4320/17010 (25%)] Loss: 0.042599
Train Epoch: 3 [4480/17010 (26%)] Loss: 0.174180
Train Epoch: 3 [4640/17010 (27%)] Loss: 0.082396
Train Epoch: 3 [4800/17010 (28%)] Loss: 0.099011
Train Epoch: 3 [4960/17010 (29%)] Loss: 0.196900
Train Epoch: 3 [5120/17010 (30%)] Loss: 0.148110
Train Epoch: 3 [5280/17010 (31%)] Loss: 0.279602
Train Epoch: 3 [5440/17010 (32%)] Loss: 0.267939
Train Epoch: 3 [5600/17010 (33%)] Loss: 0.202046
Train Epoch: 3 [5760/17010 (34%)] Loss: 0.295004
Train Epoch: 3 [5920/17010 (35%)] Loss: 0.212030
Train Epoch: 3 [6080/17010 (36%)] Loss: 0.066487
Train Epoch: 3 [6240/17010 (37%)] Loss: 0.204650
Train Epoch: 3 [6400/17010 (38%)] Loss: 0.072028
Train Epoch: 3 [6560/17010 (39%)] Loss: 0.247457
Train Epoch: 3 [6720/17010 (40%)] Loss: 0.113871
Train Epoch: 3 [6880/17010 (40%)] Loss: 0.126202
Train Epoch: 3 [7040/17010 (41%)] Loss: 0.179058
Train Epoch: 3 [7200/17010 (42%)] Loss: 0.122593
Train Epoch: 3 [7360/17010 (43%)] Loss: 0.283515
Train Epoch: 3 [7520/17010 (44%)] Loss: 0.195729
Train Epoch: 3 [7680/17010 (45%)] Loss: 0.070475
Train Epoch: 3 [7840/17010 (46%)] Loss: 0.166003
Train Epoch: 3 [8000/17010 (47%)] Loss: 0.050017
Train Epoch: 3 [8160/17010 (48%)] Loss: 0.118842
Train Epoch: 3 [8320/17010 (49%)] Loss: 0.091970
Train Epoch: 3 [8480/17010 (50%)] Loss: 0.056533
Train Epoch: 3 [8640/17010 (51%)] Loss: 0.069226
Train Epoch: 3 [8800/17010 (52%)] Loss: 0.130315
Train Epoch: 3 [8960/17010 (53%)] Loss: 0.277753
Train Epoch: 3 [9120/17010 (54%)] Loss: 0.113029
Train Epoch: 3 [9280/17010 (55%)] Loss: 0.141134
Train Epoch: 3 [9440/17010 (55%)] Loss: 0.023134
Train Epoch: 3 [9600/17010 (56%)] Loss: 0.142835
Train Epoch: 3 [9760/17010 (57%)] Loss: 0.041909
Train Epoch: 3 [9920/17010 (58%)] Loss: 0.057891
Train Epoch: 3 [10080/17010 (59%)] Loss: 0.138633
Train Epoch: 3 [10240/17010 (60%)] Loss: 0.039751
Train Epoch: 3 [10400/17010 (61%)] Loss: 0.054911
Train Epoch: 3 [10560/17010 (62%)] Loss: 0.029325
Train Epoch: 3 [10720/17010 (63%)] Loss: 0.110143
Train Epoch: 3 [10880/17010 (64%)] Loss: 0.008650
Train Epoch: 3 [11040/17010 (65%)] Loss: 0.009995
Train Epoch: 3 [11200/17010 (66%)] Loss: 0.181513
Train Epoch: 3 [11360/17010 (67%)] Loss: 0.068876
Train Epoch: 3 [11520/17010 (68%)] Loss: 0.011883
Train Epoch: 3 [11680/17010 (69%)] Loss: 0.009937
Train Epoch: 3 [11840/17010 (70%)] Loss: 0.043321
Train Epoch: 3 [12000/17010 (71%)] Loss: 0.374868
Train Epoch: 3 [12160/17010 (71%)] Loss: 0.081358
Train Epoch: 3 [12320/17010 (72%)] Loss: 0.318947
Train Epoch: 3 [12480/17010 (73%)] Loss: 0.025702
Train Epoch: 3 [12640/17010 (74%)] Loss: 0.217822
Train Epoch: 3 [12800/17010 (75%)] Loss: 0.025318
Train Epoch: 3 [12960/17010 (76%)] Loss: 0.178719
Train Epoch: 3 [13120/17010 (77%)] Loss: 0.192274
Train Epoch: 3 [13280/17010 (78%)] Loss: 0.099390
Train Epoch: 3 [13440/17010 (79%)] Loss: 0.103323
Train Epoch: 3 [13600/17010 (80%)] Loss: 0.022766
Train Epoch: 3 [13760/17010 (81%)] Loss: 0.046284
Train Epoch: 3 [13920/17010 (82%)] Loss: 0.538796
Train Epoch: 3 [14080/17010 (83%)] Loss: 0.191334
Train Epoch: 3 [14240/17010 (84%)] Loss: 0.237078
Train Epoch: 3 [14400/17010 (85%)] Loss: 0.235514
Train Epoch: 3 [14560/17010 (86%)] Loss: 0.393838
Train Epoch: 3 [14720/17010 (87%)] Loss: 0.071801
Train Epoch: 3 [14880/17010 (87%)] Loss: 0.210297
Train Epoch: 3 [15040/17010 (88%)] Loss: 0.212831
Train Epoch: 3 [15200/17010 (89%)] Loss: 0.171097
Train Epoch: 3 [15360/17010 (90%)] Loss: 0.092353
Train Epoch: 3 [15520/17010 (91%)] Loss: 0.189111
Train Epoch: 3 [15680/17010 (92%)] Loss: 0.304849
Train Epoch: 3 [15840/17010 (93%)] Loss: 0.080422
Train Epoch: 3 [16000/17010 (94%)] Loss: 0.133794
Train Epoch: 3 [16160/17010 (95%)] Loss: 0.215121
Train Epoch: 3 [16320/17010 (96%)] Loss: 0.195083
Train Epoch: 3 [16480/17010 (97%)] Loss: 0.100173
Train Epoch: 3 [16640/17010 (98%)] Loss: 0.196569
Train Epoch: 3 [16800/17010 (99%)] Loss: 0.068845
Train Epoch: 3 [16960/17010 (100%)] Loss: 0.300914
    epoch          : 3
    Train_loss     : 0.12930481028182894
    Train_accuracy : 0.9579090956558062
    Train_f1_score : 0.957909107208252
    Val_loss       : 0.20672533046035824
    Val_accuracy   : 0.9322916666666666
    Val_f1_score   : 0.9322916865348816
Train Epoch: 4 [0/17010 (0%)] Loss: 0.023081
Train Epoch: 4 [160/17010 (1%)] Loss: 0.220618
Train Epoch: 4 [320/17010 (2%)] Loss: 0.022780
Train Epoch: 4 [480/17010 (3%)] Loss: 0.132612
Train Epoch: 4 [640/17010 (4%)] Loss: 0.193671
Train Epoch: 4 [800/17010 (5%)] Loss: 0.024164
Train Epoch: 4 [960/17010 (6%)] Loss: 0.127615
Train Epoch: 4 [1120/17010 (7%)] Loss: 0.072074
Train Epoch: 4 [1280/17010 (8%)] Loss: 0.039685
Train Epoch: 4 [1440/17010 (8%)] Loss: 0.063648
Train Epoch: 4 [1600/17010 (9%)] Loss: 0.123945
Train Epoch: 4 [1760/17010 (10%)] Loss: 0.081387
Train Epoch: 4 [1920/17010 (11%)] Loss: 0.017173
Train Epoch: 4 [2080/17010 (12%)] Loss: 0.124076
Train Epoch: 4 [2240/17010 (13%)] Loss: 0.040277
Train Epoch: 4 [2400/17010 (14%)] Loss: 0.017668
Train Epoch: 4 [2560/17010 (15%)] Loss: 0.075779
Train Epoch: 4 [2720/17010 (16%)] Loss: 0.032353
Train Epoch: 4 [2880/17010 (17%)] Loss: 0.125204
Train Epoch: 4 [3040/17010 (18%)] Loss: 0.006871
Train Epoch: 4 [3200/17010 (19%)] Loss: 0.019508
Train Epoch: 4 [3360/17010 (20%)] Loss: 0.119005
Train Epoch: 4 [3520/17010 (21%)] Loss: 0.017649
Train Epoch: 4 [3680/17010 (22%)] Loss: 0.055733
Train Epoch: 4 [3840/17010 (23%)] Loss: 0.197909
Train Epoch: 4 [4000/17010 (24%)] Loss: 0.009117
Train Epoch: 4 [4160/17010 (24%)] Loss: 0.132381
Train Epoch: 4 [4320/17010 (25%)] Loss: 0.056721
Train Epoch: 4 [4480/17010 (26%)] Loss: 0.277730
Train Epoch: 4 [4640/17010 (27%)] Loss: 0.267388
Train Epoch: 4 [4800/17010 (28%)] Loss: 0.187374
Train Epoch: 4 [4960/17010 (29%)] Loss: 0.175222
Train Epoch: 4 [5120/17010 (30%)] Loss: 0.668400
Train Epoch: 4 [5280/17010 (31%)] Loss: 0.137890
Train Epoch: 4 [5440/17010 (32%)] Loss: 0.071353
Train Epoch: 4 [5600/17010 (33%)] Loss: 0.019200
Train Epoch: 4 [5760/17010 (34%)] Loss: 0.198675
Train Epoch: 4 [5920/17010 (35%)] Loss: 0.018674
Train Epoch: 4 [6080/17010 (36%)] Loss: 0.237835
Train Epoch: 4 [6240/17010 (37%)] Loss: 0.058194
Train Epoch: 4 [6400/17010 (38%)] Loss: 0.009054
Train Epoch: 4 [6560/17010 (39%)] Loss: 0.034569
Train Epoch: 4 [6720/17010 (40%)] Loss: 0.313259
Train Epoch: 4 [6880/17010 (40%)] Loss: 0.018611
Train Epoch: 4 [7040/17010 (41%)] Loss: 0.145394
Train Epoch: 4 [7200/17010 (42%)] Loss: 0.006593
Train Epoch: 4 [7360/17010 (43%)] Loss: 0.061768
Train Epoch: 4 [7520/17010 (44%)] Loss: 0.018434
Train Epoch: 4 [7680/17010 (45%)] Loss: 0.153555
Train Epoch: 4 [7840/17010 (46%)] Loss: 0.016106
Train Epoch: 4 [8000/17010 (47%)] Loss: 0.078377
Train Epoch: 4 [8160/17010 (48%)] Loss: 0.018224
Train Epoch: 4 [8320/17010 (49%)] Loss: 0.012585
Train Epoch: 4 [8480/17010 (50%)] Loss: 0.020717
Train Epoch: 4 [8640/17010 (51%)] Loss: 0.004537
Train Epoch: 4 [8800/17010 (52%)] Loss: 0.055211
Train Epoch: 4 [8960/17010 (53%)] Loss: 0.009694
Train Epoch: 4 [9120/17010 (54%)] Loss: 0.032866
Train Epoch: 4 [9280/17010 (55%)] Loss: 0.051775
Train Epoch: 4 [9440/17010 (55%)] Loss: 0.337282
Train Epoch: 4 [9600/17010 (56%)] Loss: 0.003638
Train Epoch: 4 [9760/17010 (57%)] Loss: 0.013129
Train Epoch: 4 [9920/17010 (58%)] Loss: 0.210766
Train Epoch: 4 [10080/17010 (59%)] Loss: 0.016004
Train Epoch: 4 [10240/17010 (60%)] Loss: 0.021296
Train Epoch: 4 [10400/17010 (61%)] Loss: 0.062638
Train Epoch: 4 [10560/17010 (62%)] Loss: 0.049919
Train Epoch: 4 [10720/17010 (63%)] Loss: 0.019258
Train Epoch: 4 [10880/17010 (64%)] Loss: 0.021710
Train Epoch: 4 [11040/17010 (65%)] Loss: 0.007657
Train Epoch: 4 [11200/17010 (66%)] Loss: 0.124222
Train Epoch: 4 [11360/17010 (67%)] Loss: 0.197671
Train Epoch: 4 [11520/17010 (68%)] Loss: 0.028265
Train Epoch: 4 [11680/17010 (69%)] Loss: 0.203518
Train Epoch: 4 [11840/17010 (70%)] Loss: 0.044393
Train Epoch: 4 [12000/17010 (71%)] Loss: 0.029154
Train Epoch: 4 [12160/17010 (71%)] Loss: 0.111547
Train Epoch: 4 [12320/17010 (72%)] Loss: 0.014056
Train Epoch: 4 [12480/17010 (73%)] Loss: 0.095221
Train Epoch: 4 [12640/17010 (74%)] Loss: 0.050050
Train Epoch: 4 [12800/17010 (75%)] Loss: 0.100635
Train Epoch: 4 [12960/17010 (76%)] Loss: 0.254371
Train Epoch: 4 [13120/17010 (77%)] Loss: 0.293121
Train Epoch: 4 [13280/17010 (78%)] Loss: 0.034855
Train Epoch: 4 [13440/17010 (79%)] Loss: 0.232362
Train Epoch: 4 [13600/17010 (80%)] Loss: 0.051337
Train Epoch: 4 [13760/17010 (81%)] Loss: 0.002537
Train Epoch: 4 [13920/17010 (82%)] Loss: 0.036819
Train Epoch: 4 [14080/17010 (83%)] Loss: 0.085225
Train Epoch: 4 [14240/17010 (84%)] Loss: 0.009615
Train Epoch: 4 [14400/17010 (85%)] Loss: 0.245206
Train Epoch: 4 [14560/17010 (86%)] Loss: 0.037810
Train Epoch: 4 [14720/17010 (87%)] Loss: 0.076533
Train Epoch: 4 [14880/17010 (87%)] Loss: 0.044229
Train Epoch: 4 [15040/17010 (88%)] Loss: 0.012970
Train Epoch: 4 [15200/17010 (89%)] Loss: 0.312572
Train Epoch: 4 [15360/17010 (90%)] Loss: 0.163745
Train Epoch: 4 [15520/17010 (91%)] Loss: 0.021821
Train Epoch: 4 [15680/17010 (92%)] Loss: 0.003279
Train Epoch: 4 [15840/17010 (93%)] Loss: 0.010095
Train Epoch: 4 [16000/17010 (94%)] Loss: 0.012446
Train Epoch: 4 [16160/17010 (95%)] Loss: 0.022081
Train Epoch: 4 [16320/17010 (96%)] Loss: 0.005005
Train Epoch: 4 [16480/17010 (97%)] Loss: 0.030782
Train Epoch: 4 [16640/17010 (98%)] Loss: 0.055844
Train Epoch: 4 [16800/17010 (99%)] Loss: 0.104087
Train Epoch: 4 [16960/17010 (100%)] Loss: 0.016601
    epoch          : 4
    Train_loss     : 0.08953717989712268
    Train_accuracy : 0.9713345864661654
    Train_f1_score : 0.9713346362113953
    Val_loss       : 0.13464896966652304
    Val_accuracy   : 0.9578125
    Val_f1_score   : 0.9578125476837158
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch4.pth ...
Train Epoch: 5 [0/17010 (0%)] Loss: 0.077361
Train Epoch: 5 [160/17010 (1%)] Loss: 0.060887
Train Epoch: 5 [320/17010 (2%)] Loss: 0.457145
Train Epoch: 5 [480/17010 (3%)] Loss: 0.241351
Train Epoch: 5 [640/17010 (4%)] Loss: 0.109084
Train Epoch: 5 [800/17010 (5%)] Loss: 0.059349
Train Epoch: 5 [960/17010 (6%)] Loss: 0.045503
Train Epoch: 5 [1120/17010 (7%)] Loss: 0.039481
Train Epoch: 5 [1280/17010 (8%)] Loss: 0.102204
Train Epoch: 5 [1440/17010 (8%)] Loss: 0.032162
Train Epoch: 5 [1600/17010 (9%)] Loss: 0.104466
Train Epoch: 5 [1760/17010 (10%)] Loss: 0.305807
Train Epoch: 5 [1920/17010 (11%)] Loss: 0.035050
Train Epoch: 5 [2080/17010 (12%)] Loss: 0.007440
Train Epoch: 5 [2240/17010 (13%)] Loss: 0.032945
Train Epoch: 5 [2400/17010 (14%)] Loss: 0.034924
Train Epoch: 5 [2560/17010 (15%)] Loss: 0.199387
Train Epoch: 5 [2720/17010 (16%)] Loss: 0.005848
Train Epoch: 5 [2880/17010 (17%)] Loss: 0.056434
Train Epoch: 5 [3040/17010 (18%)] Loss: 0.035412
Train Epoch: 5 [3200/17010 (19%)] Loss: 0.033045
Train Epoch: 5 [3360/17010 (20%)] Loss: 0.005978
Train Epoch: 5 [3520/17010 (21%)] Loss: 0.018322
Train Epoch: 5 [3680/17010 (22%)] Loss: 0.048945
Train Epoch: 5 [3840/17010 (23%)] Loss: 0.028567
Train Epoch: 5 [4000/17010 (24%)] Loss: 0.066272
Train Epoch: 5 [4160/17010 (24%)] Loss: 0.074119
Train Epoch: 5 [4320/17010 (25%)] Loss: 0.077135
Train Epoch: 5 [4480/17010 (26%)] Loss: 0.002490
Train Epoch: 5 [4640/17010 (27%)] Loss: 0.013849
Train Epoch: 5 [4800/17010 (28%)] Loss: 0.025481
Train Epoch: 5 [4960/17010 (29%)] Loss: 0.037955
Train Epoch: 5 [5120/17010 (30%)] Loss: 0.079030
Train Epoch: 5 [5280/17010 (31%)] Loss: 0.090481
Train Epoch: 5 [5440/17010 (32%)] Loss: 0.017143
Train Epoch: 5 [5600/17010 (33%)] Loss: 0.179640
Train Epoch: 5 [5760/17010 (34%)] Loss: 0.007973
Train Epoch: 5 [5920/17010 (35%)] Loss: 0.063101
Train Epoch: 5 [6080/17010 (36%)] Loss: 0.006049
Train Epoch: 5 [6240/17010 (37%)] Loss: 0.017836
Train Epoch: 5 [6400/17010 (38%)] Loss: 0.697149
Train Epoch: 5 [6560/17010 (39%)] Loss: 0.026694
Train Epoch: 5 [6720/17010 (40%)] Loss: 0.192512
Train Epoch: 5 [6880/17010 (40%)] Loss: 0.006722
Train Epoch: 5 [7040/17010 (41%)] Loss: 0.015721
Train Epoch: 5 [7200/17010 (42%)] Loss: 0.024705
Train Epoch: 5 [7360/17010 (43%)] Loss: 0.006158
Train Epoch: 5 [7520/17010 (44%)] Loss: 0.011931
Train Epoch: 5 [7680/17010 (45%)] Loss: 0.017589
Train Epoch: 5 [7840/17010 (46%)] Loss: 0.005563
Train Epoch: 5 [8000/17010 (47%)] Loss: 0.104640
Train Epoch: 5 [8160/17010 (48%)] Loss: 0.003424
Train Epoch: 5 [8320/17010 (49%)] Loss: 0.016236
Train Epoch: 5 [8480/17010 (50%)] Loss: 0.009151
Train Epoch: 5 [8640/17010 (51%)] Loss: 0.046825
Train Epoch: 5 [8800/17010 (52%)] Loss: 0.068696
Train Epoch: 5 [8960/17010 (53%)] Loss: 0.005986
Train Epoch: 5 [9120/17010 (54%)] Loss: 0.169095
Train Epoch: 5 [9280/17010 (55%)] Loss: 0.005743
Train Epoch: 5 [9440/17010 (55%)] Loss: 0.046441
Train Epoch: 5 [9600/17010 (56%)] Loss: 0.053852
Train Epoch: 5 [9760/17010 (57%)] Loss: 0.033130
Train Epoch: 5 [9920/17010 (58%)] Loss: 0.014178
Train Epoch: 5 [10080/17010 (59%)] Loss: 0.131773
Train Epoch: 5 [10240/17010 (60%)] Loss: 0.014547
Train Epoch: 5 [10400/17010 (61%)] Loss: 0.008245
Train Epoch: 5 [10560/17010 (62%)] Loss: 0.245339
Train Epoch: 5 [10720/17010 (63%)] Loss: 0.049666
Train Epoch: 5 [10880/17010 (64%)] Loss: 0.088556
Train Epoch: 5 [11040/17010 (65%)] Loss: 0.005532
Train Epoch: 5 [11200/17010 (66%)] Loss: 0.005956
Train Epoch: 5 [11360/17010 (67%)] Loss: 0.108454
Train Epoch: 5 [11520/17010 (68%)] Loss: 0.009727
Train Epoch: 5 [11680/17010 (69%)] Loss: 0.029313
Train Epoch: 5 [11840/17010 (70%)] Loss: 0.047509
Train Epoch: 5 [12000/17010 (71%)] Loss: 0.004210
Train Epoch: 5 [12160/17010 (71%)] Loss: 0.007746
Train Epoch: 5 [12320/17010 (72%)] Loss: 0.008021
Train Epoch: 5 [12480/17010 (73%)] Loss: 0.114835
Train Epoch: 5 [12640/17010 (74%)] Loss: 0.012136
Train Epoch: 5 [12800/17010 (75%)] Loss: 0.003876
Train Epoch: 5 [12960/17010 (76%)] Loss: 0.065065
Train Epoch: 5 [13120/17010 (77%)] Loss: 0.002633
Train Epoch: 5 [13280/17010 (78%)] Loss: 0.168122
Train Epoch: 5 [13440/17010 (79%)] Loss: 0.064004
Train Epoch: 5 [13600/17010 (80%)] Loss: 0.219168
Train Epoch: 5 [13760/17010 (81%)] Loss: 0.015439
Train Epoch: 5 [13920/17010 (82%)] Loss: 0.109590
Train Epoch: 5 [14080/17010 (83%)] Loss: 0.177293
Train Epoch: 5 [14240/17010 (84%)] Loss: 0.023691
Train Epoch: 5 [14400/17010 (85%)] Loss: 0.213281
Train Epoch: 5 [14560/17010 (86%)] Loss: 0.150409
Train Epoch: 5 [14720/17010 (87%)] Loss: 0.054394
Train Epoch: 5 [14880/17010 (87%)] Loss: 0.063229
Train Epoch: 5 [15040/17010 (88%)] Loss: 0.254398
Train Epoch: 5 [15200/17010 (89%)] Loss: 0.036576
Train Epoch: 5 [15360/17010 (90%)] Loss: 0.002039
Train Epoch: 5 [15520/17010 (91%)] Loss: 0.042341
Train Epoch: 5 [15680/17010 (92%)] Loss: 0.011576
Train Epoch: 5 [15840/17010 (93%)] Loss: 0.086180
Train Epoch: 5 [16000/17010 (94%)] Loss: 0.012194
Train Epoch: 5 [16160/17010 (95%)] Loss: 0.014390
Train Epoch: 5 [16320/17010 (96%)] Loss: 0.015348
Train Epoch: 5 [16480/17010 (97%)] Loss: 0.019537
Train Epoch: 5 [16640/17010 (98%)] Loss: 0.017845
Train Epoch: 5 [16800/17010 (99%)] Loss: 0.003882
Train Epoch: 5 [16960/17010 (100%)] Loss: 0.020016
    epoch          : 5
    Train_loss     : 0.0626635051597631
    Train_accuracy : 0.9815554511278195
    Train_f1_score : 0.9815554618835449
    Val_loss       : 0.11312041544200231
    Val_accuracy   : 0.9640625
    Val_f1_score   : 0.9640625715255737
Train Epoch: 6 [0/17010 (0%)] Loss: 0.032941
Train Epoch: 6 [160/17010 (1%)] Loss: 0.008942
Train Epoch: 6 [320/17010 (2%)] Loss: 0.006992
Train Epoch: 6 [480/17010 (3%)] Loss: 0.004317
Train Epoch: 6 [640/17010 (4%)] Loss: 0.006210
Train Epoch: 6 [800/17010 (5%)] Loss: 0.005037
Train Epoch: 6 [960/17010 (6%)] Loss: 0.002493
Train Epoch: 6 [1120/17010 (7%)] Loss: 0.023999
Train Epoch: 6 [1280/17010 (8%)] Loss: 0.019341
Train Epoch: 6 [1440/17010 (8%)] Loss: 0.031114
Train Epoch: 6 [1600/17010 (9%)] Loss: 0.161944
Train Epoch: 6 [1760/17010 (10%)] Loss: 0.035901
Train Epoch: 6 [1920/17010 (11%)] Loss: 0.006097
Train Epoch: 6 [2080/17010 (12%)] Loss: 0.040021
Train Epoch: 6 [2240/17010 (13%)] Loss: 0.001197
Train Epoch: 6 [2400/17010 (14%)] Loss: 0.058662
Train Epoch: 6 [2560/17010 (15%)] Loss: 0.149105
Train Epoch: 6 [2720/17010 (16%)] Loss: 0.004787
Train Epoch: 6 [2880/17010 (17%)] Loss: 0.120291
Train Epoch: 6 [3040/17010 (18%)] Loss: 0.066711
Train Epoch: 6 [3200/17010 (19%)] Loss: 0.001342
Train Epoch: 6 [3360/17010 (20%)] Loss: 0.123230
Train Epoch: 6 [3520/17010 (21%)] Loss: 0.097159
Train Epoch: 6 [3680/17010 (22%)] Loss: 0.169637
Train Epoch: 6 [3840/17010 (23%)] Loss: 0.265093
Train Epoch: 6 [4000/17010 (24%)] Loss: 0.006350
Train Epoch: 6 [4160/17010 (24%)] Loss: 0.118824
Train Epoch: 6 [4320/17010 (25%)] Loss: 0.042361
Train Epoch: 6 [4480/17010 (26%)] Loss: 0.020657
Train Epoch: 6 [4640/17010 (27%)] Loss: 0.008568
Train Epoch: 6 [4800/17010 (28%)] Loss: 0.012683
Train Epoch: 6 [4960/17010 (29%)] Loss: 0.007427
Train Epoch: 6 [5120/17010 (30%)] Loss: 0.011731
Train Epoch: 6 [5280/17010 (31%)] Loss: 0.004753
Train Epoch: 6 [5440/17010 (32%)] Loss: 0.210654
Train Epoch: 6 [5600/17010 (33%)] Loss: 0.505205
Train Epoch: 6 [5760/17010 (34%)] Loss: 0.026099
Train Epoch: 6 [5920/17010 (35%)] Loss: 0.061958
Train Epoch: 6 [6080/17010 (36%)] Loss: 0.041529
Train Epoch: 6 [6240/17010 (37%)] Loss: 0.021548
Train Epoch: 6 [6400/17010 (38%)] Loss: 0.018429
Train Epoch: 6 [6560/17010 (39%)] Loss: 0.024999
Train Epoch: 6 [6720/17010 (40%)] Loss: 0.018917
Train Epoch: 6 [6880/17010 (40%)] Loss: 0.126750
Train Epoch: 6 [7040/17010 (41%)] Loss: 0.005724
Train Epoch: 6 [7200/17010 (42%)] Loss: 0.015247
Train Epoch: 6 [7360/17010 (43%)] Loss: 0.007241
Train Epoch: 6 [7520/17010 (44%)] Loss: 0.003256
Train Epoch: 6 [7680/17010 (45%)] Loss: 0.054139
Train Epoch: 6 [7840/17010 (46%)] Loss: 0.034698
Train Epoch: 6 [8000/17010 (47%)] Loss: 0.028658
Train Epoch: 6 [8160/17010 (48%)] Loss: 0.103297
Train Epoch: 6 [8320/17010 (49%)] Loss: 0.010256
Train Epoch: 6 [8480/17010 (50%)] Loss: 0.157217
Train Epoch: 6 [8640/17010 (51%)] Loss: 0.037699
Train Epoch: 6 [8800/17010 (52%)] Loss: 0.005813
Train Epoch: 6 [8960/17010 (53%)] Loss: 0.114876
Train Epoch: 6 [9120/17010 (54%)] Loss: 0.004497
Train Epoch: 6 [9280/17010 (55%)] Loss: 0.086574
Train Epoch: 6 [9440/17010 (55%)] Loss: 0.004918
Train Epoch: 6 [9600/17010 (56%)] Loss: 0.013328
Train Epoch: 6 [9760/17010 (57%)] Loss: 0.020362
Train Epoch: 6 [9920/17010 (58%)] Loss: 0.077211
Train Epoch: 6 [10080/17010 (59%)] Loss: 0.095371
Train Epoch: 6 [10240/17010 (60%)] Loss: 0.092234
Train Epoch: 6 [10400/17010 (61%)] Loss: 0.072269
Train Epoch: 6 [10560/17010 (62%)] Loss: 0.004947
Train Epoch: 6 [10720/17010 (63%)] Loss: 0.024951
Train Epoch: 6 [10880/17010 (64%)] Loss: 0.003076
Train Epoch: 6 [11040/17010 (65%)] Loss: 0.243182
Train Epoch: 6 [11200/17010 (66%)] Loss: 0.013468
Train Epoch: 6 [11360/17010 (67%)] Loss: 0.033671
Train Epoch: 6 [11520/17010 (68%)] Loss: 0.003801
Train Epoch: 6 [11680/17010 (69%)] Loss: 0.016250
Train Epoch: 6 [11840/17010 (70%)] Loss: 0.017009
Train Epoch: 6 [12000/17010 (71%)] Loss: 0.011726
Train Epoch: 6 [12160/17010 (71%)] Loss: 0.068722
Train Epoch: 6 [12320/17010 (72%)] Loss: 0.003141
Train Epoch: 6 [12480/17010 (73%)] Loss: 0.006593
Train Epoch: 6 [12640/17010 (74%)] Loss: 0.383814
Train Epoch: 6 [12800/17010 (75%)] Loss: 0.028794
Train Epoch: 6 [12960/17010 (76%)] Loss: 0.011589
Train Epoch: 6 [13120/17010 (77%)] Loss: 0.162922
Train Epoch: 6 [13280/17010 (78%)] Loss: 0.003641
Train Epoch: 6 [13440/17010 (79%)] Loss: 0.009491
Train Epoch: 6 [13600/17010 (80%)] Loss: 0.079941
Train Epoch: 6 [13760/17010 (81%)] Loss: 0.022019
Train Epoch: 6 [13920/17010 (82%)] Loss: 0.031929
Train Epoch: 6 [14080/17010 (83%)] Loss: 0.015187
Train Epoch: 6 [14240/17010 (84%)] Loss: 0.066813
Train Epoch: 6 [14400/17010 (85%)] Loss: 0.168483
Train Epoch: 6 [14560/17010 (86%)] Loss: 0.003196
Train Epoch: 6 [14720/17010 (87%)] Loss: 0.008011
Train Epoch: 6 [14880/17010 (87%)] Loss: 0.017756
Train Epoch: 6 [15040/17010 (88%)] Loss: 0.004924
Train Epoch: 6 [15200/17010 (89%)] Loss: 0.037442
Train Epoch: 6 [15360/17010 (90%)] Loss: 0.022866
Train Epoch: 6 [15520/17010 (91%)] Loss: 0.077566
Train Epoch: 6 [15680/17010 (92%)] Loss: 0.001870
Train Epoch: 6 [15840/17010 (93%)] Loss: 0.145199
Train Epoch: 6 [16000/17010 (94%)] Loss: 0.155945
Train Epoch: 6 [16160/17010 (95%)] Loss: 0.062417
Train Epoch: 6 [16320/17010 (96%)] Loss: 0.019900
Train Epoch: 6 [16480/17010 (97%)] Loss: 0.033554
Train Epoch: 6 [16640/17010 (98%)] Loss: 0.024261
Train Epoch: 6 [16800/17010 (99%)] Loss: 0.015914
Train Epoch: 6 [16960/17010 (100%)] Loss: 0.011773
    epoch          : 6
    Train_loss     : 0.05341927156782765
    Train_accuracy : 0.9832589285714286
    Train_f1_score : 0.9832589626312256
    Val_loss       : 0.15252631900366395
    Val_accuracy   : 0.953125
    Val_f1_score   : 0.9531250596046448
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch6.pth ...
Train Epoch: 7 [0/17010 (0%)] Loss: 0.002406
Train Epoch: 7 [160/17010 (1%)] Loss: 0.239954
Train Epoch: 7 [320/17010 (2%)] Loss: 0.025574
Train Epoch: 7 [480/17010 (3%)] Loss: 0.060744
Train Epoch: 7 [640/17010 (4%)] Loss: 0.156793
Train Epoch: 7 [800/17010 (5%)] Loss: 0.005933
Train Epoch: 7 [960/17010 (6%)] Loss: 0.002867
Train Epoch: 7 [1120/17010 (7%)] Loss: 0.132486
Train Epoch: 7 [1280/17010 (8%)] Loss: 0.006138
Train Epoch: 7 [1440/17010 (8%)] Loss: 0.069607
Train Epoch: 7 [1600/17010 (9%)] Loss: 0.039817
Train Epoch: 7 [1760/17010 (10%)] Loss: 0.005104
Train Epoch: 7 [1920/17010 (11%)] Loss: 0.014904
Train Epoch: 7 [2080/17010 (12%)] Loss: 0.115395
Train Epoch: 7 [2240/17010 (13%)] Loss: 0.020899
Train Epoch: 7 [2400/17010 (14%)] Loss: 0.058452
Train Epoch: 7 [2560/17010 (15%)] Loss: 0.008084
Train Epoch: 7 [2720/17010 (16%)] Loss: 0.036991
Train Epoch: 7 [2880/17010 (17%)] Loss: 0.009223
Train Epoch: 7 [3040/17010 (18%)] Loss: 0.009077
Train Epoch: 7 [3200/17010 (19%)] Loss: 0.024275
Train Epoch: 7 [3360/17010 (20%)] Loss: 0.036536
Train Epoch: 7 [3520/17010 (21%)] Loss: 0.001962
Train Epoch: 7 [3680/17010 (22%)] Loss: 0.159109
Train Epoch: 7 [3840/17010 (23%)] Loss: 0.009053
Train Epoch: 7 [4000/17010 (24%)] Loss: 0.014469
Train Epoch: 7 [4160/17010 (24%)] Loss: 0.013075
Train Epoch: 7 [4320/17010 (25%)] Loss: 0.084831
Train Epoch: 7 [4480/17010 (26%)] Loss: 0.007290
Train Epoch: 7 [4640/17010 (27%)] Loss: 0.039049
Train Epoch: 7 [4800/17010 (28%)] Loss: 0.023838
Train Epoch: 7 [4960/17010 (29%)] Loss: 0.001568
Train Epoch: 7 [5120/17010 (30%)] Loss: 0.046898
Train Epoch: 7 [5280/17010 (31%)] Loss: 0.003672
Train Epoch: 7 [5440/17010 (32%)] Loss: 0.051514
Train Epoch: 7 [5600/17010 (33%)] Loss: 0.024565
Train Epoch: 7 [5760/17010 (34%)] Loss: 0.002830
Train Epoch: 7 [5920/17010 (35%)] Loss: 0.064995
Train Epoch: 7 [6080/17010 (36%)] Loss: 0.008094
Train Epoch: 7 [6240/17010 (37%)] Loss: 0.017478
Train Epoch: 7 [6400/17010 (38%)] Loss: 0.007051
Train Epoch: 7 [6560/17010 (39%)] Loss: 0.007786
Train Epoch: 7 [6720/17010 (40%)] Loss: 0.009601
Train Epoch: 7 [6880/17010 (40%)] Loss: 0.019425
Train Epoch: 7 [7040/17010 (41%)] Loss: 0.008469
Train Epoch: 7 [7200/17010 (42%)] Loss: 0.009278
Train Epoch: 7 [7360/17010 (43%)] Loss: 0.002499
Train Epoch: 7 [7520/17010 (44%)] Loss: 0.042059
Train Epoch: 7 [7680/17010 (45%)] Loss: 0.033683
Train Epoch: 7 [7840/17010 (46%)] Loss: 0.003561
Train Epoch: 7 [8000/17010 (47%)] Loss: 0.001926
Train Epoch: 7 [8160/17010 (48%)] Loss: 0.033417
Train Epoch: 7 [8320/17010 (49%)] Loss: 0.002136
Train Epoch: 7 [8480/17010 (50%)] Loss: 0.036074
Train Epoch: 7 [8640/17010 (51%)] Loss: 0.000749
Train Epoch: 7 [8800/17010 (52%)] Loss: 0.000572
Train Epoch: 7 [8960/17010 (53%)] Loss: 0.003274
Train Epoch: 7 [9120/17010 (54%)] Loss: 0.048410
Train Epoch: 7 [9280/17010 (55%)] Loss: 0.003560
Train Epoch: 7 [9440/17010 (55%)] Loss: 0.001128
Train Epoch: 7 [9600/17010 (56%)] Loss: 0.014178
Train Epoch: 7 [9760/17010 (57%)] Loss: 0.010012
Train Epoch: 7 [9920/17010 (58%)] Loss: 0.001714
Train Epoch: 7 [10080/17010 (59%)] Loss: 0.012092
Train Epoch: 7 [10240/17010 (60%)] Loss: 0.180965
Train Epoch: 7 [10400/17010 (61%)] Loss: 0.004292
Train Epoch: 7 [10560/17010 (62%)] Loss: 0.038385
Train Epoch: 7 [10720/17010 (63%)] Loss: 0.160986
Train Epoch: 7 [10880/17010 (64%)] Loss: 0.011338
Train Epoch: 7 [11040/17010 (65%)] Loss: 0.067142
Train Epoch: 7 [11200/17010 (66%)] Loss: 0.076260
Train Epoch: 7 [11360/17010 (67%)] Loss: 0.005512
Train Epoch: 7 [11520/17010 (68%)] Loss: 0.169777
Train Epoch: 7 [11680/17010 (69%)] Loss: 0.008200
Train Epoch: 7 [11840/17010 (70%)] Loss: 0.082124
Train Epoch: 7 [12000/17010 (71%)] Loss: 0.008206
Train Epoch: 7 [12160/17010 (71%)] Loss: 0.029379
Train Epoch: 7 [12320/17010 (72%)] Loss: 0.113968
Train Epoch: 7 [12480/17010 (73%)] Loss: 0.031919
Train Epoch: 7 [12640/17010 (74%)] Loss: 0.119846
Train Epoch: 7 [12800/17010 (75%)] Loss: 0.023846
Train Epoch: 7 [12960/17010 (76%)] Loss: 0.004331
Train Epoch: 7 [13120/17010 (77%)] Loss: 0.004376
Train Epoch: 7 [13280/17010 (78%)] Loss: 0.001380
Train Epoch: 7 [13440/17010 (79%)] Loss: 0.064709
Train Epoch: 7 [13600/17010 (80%)] Loss: 0.028577
Train Epoch: 7 [13760/17010 (81%)] Loss: 0.007058
Train Epoch: 7 [13920/17010 (82%)] Loss: 0.002223
Train Epoch: 7 [14080/17010 (83%)] Loss: 0.007654
Train Epoch: 7 [14240/17010 (84%)] Loss: 0.039595
Train Epoch: 7 [14400/17010 (85%)] Loss: 0.004491
Train Epoch: 7 [14560/17010 (86%)] Loss: 0.031931
Train Epoch: 7 [14720/17010 (87%)] Loss: 0.087104
Train Epoch: 7 [14880/17010 (87%)] Loss: 0.019385
Train Epoch: 7 [15040/17010 (88%)] Loss: 0.004656
Train Epoch: 7 [15200/17010 (89%)] Loss: 0.009159
Train Epoch: 7 [15360/17010 (90%)] Loss: 0.006946
Train Epoch: 7 [15520/17010 (91%)] Loss: 0.002072
Train Epoch: 7 [15680/17010 (92%)] Loss: 0.043544
Train Epoch: 7 [15840/17010 (93%)] Loss: 0.005254
Train Epoch: 7 [16000/17010 (94%)] Loss: 0.012573
Train Epoch: 7 [16160/17010 (95%)] Loss: 0.000704
Train Epoch: 7 [16320/17010 (96%)] Loss: 0.005533
Train Epoch: 7 [16480/17010 (97%)] Loss: 0.071682
Train Epoch: 7 [16640/17010 (98%)] Loss: 0.012997
Train Epoch: 7 [16800/17010 (99%)] Loss: 0.183496
Train Epoch: 7 [16960/17010 (100%)] Loss: 0.016232
    epoch          : 7
    Train_loss     : 0.03993920750749259
    Train_accuracy : 0.986078477443609
    Train_f1_score : 0.9860785007476807
    Val_loss       : 0.04525139083677156
    Val_accuracy   : 0.9848958333333333
    Val_f1_score   : 0.9848958849906921
Train Epoch: 8 [0/17010 (0%)] Loss: 0.004712
Train Epoch: 8 [160/17010 (1%)] Loss: 0.027554
Train Epoch: 8 [320/17010 (2%)] Loss: 0.004219
Train Epoch: 8 [480/17010 (3%)] Loss: 0.189704
Train Epoch: 8 [640/17010 (4%)] Loss: 0.003680
Train Epoch: 8 [800/17010 (5%)] Loss: 0.048632
Train Epoch: 8 [960/17010 (6%)] Loss: 0.003144
Train Epoch: 8 [1120/17010 (7%)] Loss: 0.002394
Train Epoch: 8 [1280/17010 (8%)] Loss: 0.008888
Train Epoch: 8 [1440/17010 (8%)] Loss: 0.009327
Train Epoch: 8 [1600/17010 (9%)] Loss: 0.131576
Train Epoch: 8 [1760/17010 (10%)] Loss: 0.006271
Train Epoch: 8 [1920/17010 (11%)] Loss: 0.035074
Train Epoch: 8 [2080/17010 (12%)] Loss: 0.000389
Train Epoch: 8 [2240/17010 (13%)] Loss: 0.011734
Train Epoch: 8 [2400/17010 (14%)] Loss: 0.023619
Train Epoch: 8 [2560/17010 (15%)] Loss: 0.000896
Train Epoch: 8 [2720/17010 (16%)] Loss: 0.005790
Train Epoch: 8 [2880/17010 (17%)] Loss: 0.032072
Train Epoch: 8 [3040/17010 (18%)] Loss: 0.001081
Train Epoch: 8 [3200/17010 (19%)] Loss: 0.171032
Train Epoch: 8 [3360/17010 (20%)] Loss: 0.000735
Train Epoch: 8 [3520/17010 (21%)] Loss: 0.024471
Train Epoch: 8 [3680/17010 (22%)] Loss: 0.000998
Train Epoch: 8 [3840/17010 (23%)] Loss: 0.013013
Train Epoch: 8 [4000/17010 (24%)] Loss: 0.003122
Train Epoch: 8 [4160/17010 (24%)] Loss: 0.006277
Train Epoch: 8 [4320/17010 (25%)] Loss: 0.001495
Train Epoch: 8 [4480/17010 (26%)] Loss: 0.006291
Train Epoch: 8 [4640/17010 (27%)] Loss: 0.069870
Train Epoch: 8 [4800/17010 (28%)] Loss: 0.004930
Train Epoch: 8 [4960/17010 (29%)] Loss: 0.112894
Train Epoch: 8 [5120/17010 (30%)] Loss: 0.005173
Train Epoch: 8 [5280/17010 (31%)] Loss: 0.073961
Train Epoch: 8 [5440/17010 (32%)] Loss: 0.049922
Train Epoch: 8 [5600/17010 (33%)] Loss: 0.009033
Train Epoch: 8 [5760/17010 (34%)] Loss: 0.043164
Train Epoch: 8 [5920/17010 (35%)] Loss: 0.185647
Train Epoch: 8 [6080/17010 (36%)] Loss: 0.015616
Train Epoch: 8 [6240/17010 (37%)] Loss: 0.003226
Train Epoch: 8 [6400/17010 (38%)] Loss: 0.015047
Train Epoch: 8 [6560/17010 (39%)] Loss: 0.013866
Train Epoch: 8 [6720/17010 (40%)] Loss: 0.006117
Train Epoch: 8 [6880/17010 (40%)] Loss: 0.002497
Train Epoch: 8 [7040/17010 (41%)] Loss: 0.001732
Train Epoch: 8 [7200/17010 (42%)] Loss: 0.006779
Train Epoch: 8 [7360/17010 (43%)] Loss: 0.186914
Train Epoch: 8 [7520/17010 (44%)] Loss: 0.034103
Train Epoch: 8 [7680/17010 (45%)] Loss: 0.205318
Train Epoch: 8 [7840/17010 (46%)] Loss: 0.070881
Train Epoch: 8 [8000/17010 (47%)] Loss: 0.039936
Train Epoch: 8 [8160/17010 (48%)] Loss: 0.026929
Train Epoch: 8 [8320/17010 (49%)] Loss: 0.019514
Train Epoch: 8 [8480/17010 (50%)] Loss: 0.055188
Train Epoch: 8 [8640/17010 (51%)] Loss: 0.007639
Train Epoch: 8 [8800/17010 (52%)] Loss: 0.001243
Train Epoch: 8 [8960/17010 (53%)] Loss: 0.091889
Train Epoch: 8 [9120/17010 (54%)] Loss: 0.006303
Train Epoch: 8 [9280/17010 (55%)] Loss: 0.077767
Train Epoch: 8 [9440/17010 (55%)] Loss: 0.077375
Train Epoch: 8 [9600/17010 (56%)] Loss: 0.018972
Train Epoch: 8 [9760/17010 (57%)] Loss: 0.220568
Train Epoch: 8 [9920/17010 (58%)] Loss: 0.001432
Train Epoch: 8 [10080/17010 (59%)] Loss: 0.006488
Train Epoch: 8 [10240/17010 (60%)] Loss: 0.005313
Train Epoch: 8 [10400/17010 (61%)] Loss: 0.029028
Train Epoch: 8 [10560/17010 (62%)] Loss: 0.006236
Train Epoch: 8 [10720/17010 (63%)] Loss: 0.214640
Train Epoch: 8 [10880/17010 (64%)] Loss: 0.006470
Train Epoch: 8 [11040/17010 (65%)] Loss: 0.063613
Train Epoch: 8 [11200/17010 (66%)] Loss: 0.001592
Train Epoch: 8 [11360/17010 (67%)] Loss: 0.072144
Train Epoch: 8 [11520/17010 (68%)] Loss: 0.029207
Train Epoch: 8 [11680/17010 (69%)] Loss: 0.060520
Train Epoch: 8 [11840/17010 (70%)] Loss: 0.001796
Train Epoch: 8 [12000/17010 (71%)] Loss: 0.002336
Train Epoch: 8 [12160/17010 (71%)] Loss: 0.006796
Train Epoch: 8 [12320/17010 (72%)] Loss: 0.018396
Train Epoch: 8 [12480/17010 (73%)] Loss: 0.000651
Train Epoch: 8 [12640/17010 (74%)] Loss: 0.008763
Train Epoch: 8 [12800/17010 (75%)] Loss: 0.000866
Train Epoch: 8 [12960/17010 (76%)] Loss: 0.004720
Train Epoch: 8 [13120/17010 (77%)] Loss: 0.018849
Train Epoch: 8 [13280/17010 (78%)] Loss: 0.002517
Train Epoch: 8 [13440/17010 (79%)] Loss: 0.053224
Train Epoch: 8 [13600/17010 (80%)] Loss: 0.006080
Train Epoch: 8 [13760/17010 (81%)] Loss: 0.003175
Train Epoch: 8 [13920/17010 (82%)] Loss: 0.015032
Train Epoch: 8 [14080/17010 (83%)] Loss: 0.001165
Train Epoch: 8 [14240/17010 (84%)] Loss: 0.000534
Train Epoch: 8 [14400/17010 (85%)] Loss: 0.000698
Train Epoch: 8 [14560/17010 (86%)] Loss: 0.003447
Train Epoch: 8 [14720/17010 (87%)] Loss: 0.105535
Train Epoch: 8 [14880/17010 (87%)] Loss: 0.006027
Train Epoch: 8 [15040/17010 (88%)] Loss: 0.001558
Train Epoch: 8 [15200/17010 (89%)] Loss: 0.002489
Train Epoch: 8 [15360/17010 (90%)] Loss: 0.002252
Train Epoch: 8 [15520/17010 (91%)] Loss: 0.035499
Train Epoch: 8 [15680/17010 (92%)] Loss: 0.072368
Train Epoch: 8 [15840/17010 (93%)] Loss: 0.002167
Train Epoch: 8 [16000/17010 (94%)] Loss: 0.000876
Train Epoch: 8 [16160/17010 (95%)] Loss: 0.144901
Train Epoch: 8 [16320/17010 (96%)] Loss: 0.081033
Train Epoch: 8 [16480/17010 (97%)] Loss: 0.016500
Train Epoch: 8 [16640/17010 (98%)] Loss: 0.011476
Train Epoch: 8 [16800/17010 (99%)] Loss: 0.369700
Train Epoch: 8 [16960/17010 (100%)] Loss: 0.148577
    epoch          : 8
    Train_loss     : 0.03618140498938169
    Train_accuracy : 0.9888392857142857
    Train_f1_score : 0.988839328289032
    Val_loss       : 0.08805964169635748
    Val_accuracy   : 0.9765625
    Val_f1_score   : 0.9765625596046448
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch8.pth ...
Train Epoch: 9 [0/17010 (0%)] Loss: 0.015746
Train Epoch: 9 [160/17010 (1%)] Loss: 0.010755
Train Epoch: 9 [320/17010 (2%)] Loss: 0.004740
Train Epoch: 9 [480/17010 (3%)] Loss: 0.186013
Train Epoch: 9 [640/17010 (4%)] Loss: 0.039276
Train Epoch: 9 [800/17010 (5%)] Loss: 0.008465
Train Epoch: 9 [960/17010 (6%)] Loss: 0.006824
Train Epoch: 9 [1120/17010 (7%)] Loss: 0.003364
Train Epoch: 9 [1280/17010 (8%)] Loss: 0.005939
Train Epoch: 9 [1440/17010 (8%)] Loss: 0.013466
Train Epoch: 9 [1600/17010 (9%)] Loss: 0.030117
Train Epoch: 9 [1760/17010 (10%)] Loss: 0.052082
Train Epoch: 9 [1920/17010 (11%)] Loss: 0.038566
Train Epoch: 9 [2080/17010 (12%)] Loss: 0.064525
Train Epoch: 9 [2240/17010 (13%)] Loss: 0.015799
Train Epoch: 9 [2400/17010 (14%)] Loss: 0.003074
Train Epoch: 9 [2560/17010 (15%)] Loss: 0.152160
Train Epoch: 9 [2720/17010 (16%)] Loss: 0.006349
Train Epoch: 9 [2880/17010 (17%)] Loss: 0.002312
Train Epoch: 9 [3040/17010 (18%)] Loss: 0.055536
Train Epoch: 9 [3200/17010 (19%)] Loss: 0.002594
Train Epoch: 9 [3360/17010 (20%)] Loss: 0.012129
Train Epoch: 9 [3520/17010 (21%)] Loss: 0.002600
Train Epoch: 9 [3680/17010 (22%)] Loss: 0.012805
Train Epoch: 9 [3840/17010 (23%)] Loss: 0.008526
Train Epoch: 9 [4000/17010 (24%)] Loss: 0.007957
Train Epoch: 9 [4160/17010 (24%)] Loss: 0.001986
Train Epoch: 9 [4320/17010 (25%)] Loss: 0.004482
Train Epoch: 9 [4480/17010 (26%)] Loss: 0.001900
Train Epoch: 9 [4640/17010 (27%)] Loss: 0.006533
Train Epoch: 9 [4800/17010 (28%)] Loss: 0.010558
Train Epoch: 9 [4960/17010 (29%)] Loss: 0.010651
Train Epoch: 9 [5120/17010 (30%)] Loss: 0.004024
Train Epoch: 9 [5280/17010 (31%)] Loss: 0.008274
Train Epoch: 9 [5440/17010 (32%)] Loss: 0.016348
Train Epoch: 9 [5600/17010 (33%)] Loss: 0.004431
Train Epoch: 9 [5760/17010 (34%)] Loss: 0.003536
Train Epoch: 9 [5920/17010 (35%)] Loss: 0.029262
Train Epoch: 9 [6080/17010 (36%)] Loss: 0.088471
Train Epoch: 9 [6240/17010 (37%)] Loss: 0.002261
Train Epoch: 9 [6400/17010 (38%)] Loss: 0.053478
Train Epoch: 9 [6560/17010 (39%)] Loss: 0.006239
Train Epoch: 9 [6720/17010 (40%)] Loss: 0.009368
Train Epoch: 9 [6880/17010 (40%)] Loss: 0.006057
Train Epoch: 9 [7040/17010 (41%)] Loss: 0.106935
Train Epoch: 9 [7200/17010 (42%)] Loss: 0.099695
Train Epoch: 9 [7360/17010 (43%)] Loss: 0.016699
Train Epoch: 9 [7520/17010 (44%)] Loss: 0.051564
Train Epoch: 9 [7680/17010 (45%)] Loss: 0.011264
Train Epoch: 9 [7840/17010 (46%)] Loss: 0.016647
Train Epoch: 9 [8000/17010 (47%)] Loss: 0.058732
Train Epoch: 9 [8160/17010 (48%)] Loss: 0.013973
Train Epoch: 9 [8320/17010 (49%)] Loss: 0.029428
Train Epoch: 9 [8480/17010 (50%)] Loss: 0.004296
Train Epoch: 9 [8640/17010 (51%)] Loss: 0.010572
Train Epoch: 9 [8800/17010 (52%)] Loss: 0.113405
Train Epoch: 9 [8960/17010 (53%)] Loss: 0.000718
Train Epoch: 9 [9120/17010 (54%)] Loss: 0.003937
Train Epoch: 9 [9280/17010 (55%)] Loss: 0.000456
Train Epoch: 9 [9440/17010 (55%)] Loss: 0.000592
Train Epoch: 9 [9600/17010 (56%)] Loss: 0.001531
Train Epoch: 9 [9760/17010 (57%)] Loss: 0.023439
Train Epoch: 9 [9920/17010 (58%)] Loss: 0.003293
Train Epoch: 9 [10080/17010 (59%)] Loss: 0.010397
Train Epoch: 9 [10240/17010 (60%)] Loss: 0.001406
Train Epoch: 9 [10400/17010 (61%)] Loss: 0.033077
Train Epoch: 9 [10560/17010 (62%)] Loss: 0.000621
Train Epoch: 9 [10720/17010 (63%)] Loss: 0.001194
Train Epoch: 9 [10880/17010 (64%)] Loss: 0.001802
Train Epoch: 9 [11040/17010 (65%)] Loss: 0.076455
Train Epoch: 9 [11200/17010 (66%)] Loss: 0.009093
Train Epoch: 9 [11360/17010 (67%)] Loss: 0.003095
Train Epoch: 9 [11520/17010 (68%)] Loss: 0.007443
Train Epoch: 9 [11680/17010 (69%)] Loss: 0.001055
Train Epoch: 9 [11840/17010 (70%)] Loss: 0.001354
Train Epoch: 9 [12000/17010 (71%)] Loss: 0.004169
Train Epoch: 9 [12160/17010 (71%)] Loss: 0.014484
Train Epoch: 9 [12320/17010 (72%)] Loss: 0.004060
Train Epoch: 9 [12480/17010 (73%)] Loss: 0.001658
Train Epoch: 9 [12640/17010 (74%)] Loss: 0.000605
Train Epoch: 9 [12800/17010 (75%)] Loss: 0.004751
Train Epoch: 9 [12960/17010 (76%)] Loss: 0.001564
Train Epoch: 9 [13120/17010 (77%)] Loss: 0.000969
Train Epoch: 9 [13280/17010 (78%)] Loss: 0.006059
Train Epoch: 9 [13440/17010 (79%)] Loss: 0.000775
Train Epoch: 9 [13600/17010 (80%)] Loss: 0.003634
Train Epoch: 9 [13760/17010 (81%)] Loss: 0.010069
Train Epoch: 9 [13920/17010 (82%)] Loss: 0.000326
Train Epoch: 9 [14080/17010 (83%)] Loss: 0.000332
Train Epoch: 9 [14240/17010 (84%)] Loss: 0.003277
Train Epoch: 9 [14400/17010 (85%)] Loss: 0.002996
Train Epoch: 9 [14560/17010 (86%)] Loss: 0.079081
Train Epoch: 9 [14720/17010 (87%)] Loss: 0.009545
Train Epoch: 9 [14880/17010 (87%)] Loss: 0.000626
Train Epoch: 9 [15040/17010 (88%)] Loss: 0.251326
Train Epoch: 9 [15200/17010 (89%)] Loss: 0.001680
Train Epoch: 9 [15360/17010 (90%)] Loss: 0.006250
Train Epoch: 9 [15520/17010 (91%)] Loss: 0.002564
Train Epoch: 9 [15680/17010 (92%)] Loss: 0.017240
Train Epoch: 9 [15840/17010 (93%)] Loss: 0.001145
Train Epoch: 9 [16000/17010 (94%)] Loss: 0.003604
Train Epoch: 9 [16160/17010 (95%)] Loss: 0.006126
Train Epoch: 9 [16320/17010 (96%)] Loss: 0.003480
Train Epoch: 9 [16480/17010 (97%)] Loss: 0.138823
Train Epoch: 9 [16640/17010 (98%)] Loss: 0.006219
Train Epoch: 9 [16800/17010 (99%)] Loss: 0.011201
Train Epoch: 9 [16960/17010 (100%)] Loss: 0.000386
    epoch          : 9
    Train_loss     : 0.0266836968417865
    Train_accuracy : 0.9913063909774437
    Train_f1_score : 0.9913064241409302
    Val_loss       : 0.05735049429804349
    Val_accuracy   : 0.9828125
    Val_f1_score   : 0.9828125238418579
Train Epoch: 10 [0/17010 (0%)] Loss: 0.041942
Train Epoch: 10 [160/17010 (1%)] Loss: 0.023659
Train Epoch: 10 [320/17010 (2%)] Loss: 0.000425
Train Epoch: 10 [480/17010 (3%)] Loss: 0.001491
Train Epoch: 10 [640/17010 (4%)] Loss: 0.006262
Train Epoch: 10 [800/17010 (5%)] Loss: 0.006364
Train Epoch: 10 [960/17010 (6%)] Loss: 0.000218
Train Epoch: 10 [1120/17010 (7%)] Loss: 0.003535
Train Epoch: 10 [1280/17010 (8%)] Loss: 0.004732
Train Epoch: 10 [1440/17010 (8%)] Loss: 0.001657
Train Epoch: 10 [1600/17010 (9%)] Loss: 0.003390
Train Epoch: 10 [1760/17010 (10%)] Loss: 0.021758
Train Epoch: 10 [1920/17010 (11%)] Loss: 0.007289
Train Epoch: 10 [2080/17010 (12%)] Loss: 0.126332
Train Epoch: 10 [2240/17010 (13%)] Loss: 0.000417
Train Epoch: 10 [2400/17010 (14%)] Loss: 0.024926
Train Epoch: 10 [2560/17010 (15%)] Loss: 0.041157
Train Epoch: 10 [2720/17010 (16%)] Loss: 0.000294
Train Epoch: 10 [2880/17010 (17%)] Loss: 0.004037
Train Epoch: 10 [3040/17010 (18%)] Loss: 0.020913
Train Epoch: 10 [3200/17010 (19%)] Loss: 0.000352
Train Epoch: 10 [3360/17010 (20%)] Loss: 0.013096
Train Epoch: 10 [3520/17010 (21%)] Loss: 0.014312
Train Epoch: 10 [3680/17010 (22%)] Loss: 0.005698
Train Epoch: 10 [3840/17010 (23%)] Loss: 0.038814
Train Epoch: 10 [4000/17010 (24%)] Loss: 0.001633
Train Epoch: 10 [4160/17010 (24%)] Loss: 0.001636
Train Epoch: 10 [4320/17010 (25%)] Loss: 0.001919
Train Epoch: 10 [4480/17010 (26%)] Loss: 0.000200
Train Epoch: 10 [4640/17010 (27%)] Loss: 0.000337
Train Epoch: 10 [4800/17010 (28%)] Loss: 0.000953
Train Epoch: 10 [4960/17010 (29%)] Loss: 0.001947
Train Epoch: 10 [5120/17010 (30%)] Loss: 0.005205
Train Epoch: 10 [5280/17010 (31%)] Loss: 0.001619
Train Epoch: 10 [5440/17010 (32%)] Loss: 0.070700
Train Epoch: 10 [5600/17010 (33%)] Loss: 0.000809
Train Epoch: 10 [5760/17010 (34%)] Loss: 0.002227
Train Epoch: 10 [5920/17010 (35%)] Loss: 0.000863
Train Epoch: 10 [6080/17010 (36%)] Loss: 0.006880
Train Epoch: 10 [6240/17010 (37%)] Loss: 0.001246
Train Epoch: 10 [6400/17010 (38%)] Loss: 0.000414
Train Epoch: 10 [6560/17010 (39%)] Loss: 0.000418
Train Epoch: 10 [6720/17010 (40%)] Loss: 0.002552
Train Epoch: 10 [6880/17010 (40%)] Loss: 0.006127
Train Epoch: 10 [7040/17010 (41%)] Loss: 0.000947
Train Epoch: 10 [7200/17010 (42%)] Loss: 0.032356
Train Epoch: 10 [7360/17010 (43%)] Loss: 0.000625
Train Epoch: 10 [7520/17010 (44%)] Loss: 0.000354
Train Epoch: 10 [7680/17010 (45%)] Loss: 0.004510
Train Epoch: 10 [7840/17010 (46%)] Loss: 0.003319
Train Epoch: 10 [8000/17010 (47%)] Loss: 0.000348
Train Epoch: 10 [8160/17010 (48%)] Loss: 0.043626
Train Epoch: 10 [8320/17010 (49%)] Loss: 0.000577
Train Epoch: 10 [8480/17010 (50%)] Loss: 0.000315
Train Epoch: 10 [8640/17010 (51%)] Loss: 0.000852
Train Epoch: 10 [8800/17010 (52%)] Loss: 0.027457
Train Epoch: 10 [8960/17010 (53%)] Loss: 0.002348
Train Epoch: 10 [9120/17010 (54%)] Loss: 0.000435
Train Epoch: 10 [9280/17010 (55%)] Loss: 0.004324
Train Epoch: 10 [9440/17010 (55%)] Loss: 0.002780
Train Epoch: 10 [9600/17010 (56%)] Loss: 0.000474
Train Epoch: 10 [9760/17010 (57%)] Loss: 0.002170
Train Epoch: 10 [9920/17010 (58%)] Loss: 0.015811
Train Epoch: 10 [10080/17010 (59%)] Loss: 0.000592
Train Epoch: 10 [10240/17010 (60%)] Loss: 0.000744
Train Epoch: 10 [10400/17010 (61%)] Loss: 0.052206
Train Epoch: 10 [10560/17010 (62%)] Loss: 0.000969
Train Epoch: 10 [10720/17010 (63%)] Loss: 0.006924
Train Epoch: 10 [10880/17010 (64%)] Loss: 0.218550
Train Epoch: 10 [11040/17010 (65%)] Loss: 0.000134
Train Epoch: 10 [11200/17010 (66%)] Loss: 0.001977
Train Epoch: 10 [11360/17010 (67%)] Loss: 0.001039
Train Epoch: 10 [11520/17010 (68%)] Loss: 0.002088
Train Epoch: 10 [11680/17010 (69%)] Loss: 0.001214
Train Epoch: 10 [11840/17010 (70%)] Loss: 0.002195
Train Epoch: 10 [12000/17010 (71%)] Loss: 0.033402
Train Epoch: 10 [12160/17010 (71%)] Loss: 0.015333
Train Epoch: 10 [12320/17010 (72%)] Loss: 0.004061
Train Epoch: 10 [12480/17010 (73%)] Loss: 0.004853
Train Epoch: 10 [12640/17010 (74%)] Loss: 0.006609
Train Epoch: 10 [12800/17010 (75%)] Loss: 0.042822
Train Epoch: 10 [12960/17010 (76%)] Loss: 0.000937
Train Epoch: 10 [13120/17010 (77%)] Loss: 0.046338
Train Epoch: 10 [13280/17010 (78%)] Loss: 0.003023
Train Epoch: 10 [13440/17010 (79%)] Loss: 0.001205
Train Epoch: 10 [13600/17010 (80%)] Loss: 0.003331
Train Epoch: 10 [13760/17010 (81%)] Loss: 0.000703
Train Epoch: 10 [13920/17010 (82%)] Loss: 0.002478
Train Epoch: 10 [14080/17010 (83%)] Loss: 0.190052
Train Epoch: 10 [14240/17010 (84%)] Loss: 0.155000
Train Epoch: 10 [14400/17010 (85%)] Loss: 0.049722
Train Epoch: 10 [14560/17010 (86%)] Loss: 0.002693
Train Epoch: 10 [14720/17010 (87%)] Loss: 0.003149
Train Epoch: 10 [14880/17010 (87%)] Loss: 0.084132
Train Epoch: 10 [15040/17010 (88%)] Loss: 0.047579
Train Epoch: 10 [15200/17010 (89%)] Loss: 0.000773
Train Epoch: 10 [15360/17010 (90%)] Loss: 0.023539
Train Epoch: 10 [15520/17010 (91%)] Loss: 0.026804
Train Epoch: 10 [15680/17010 (92%)] Loss: 0.003985
Train Epoch: 10 [15840/17010 (93%)] Loss: 0.015637
Train Epoch: 10 [16000/17010 (94%)] Loss: 0.005610
Train Epoch: 10 [16160/17010 (95%)] Loss: 0.012133
Train Epoch: 10 [16320/17010 (96%)] Loss: 0.002231
Train Epoch: 10 [16480/17010 (97%)] Loss: 0.001073
Train Epoch: 10 [16640/17010 (98%)] Loss: 0.004221
Train Epoch: 10 [16800/17010 (99%)] Loss: 0.004521
Train Epoch: 10 [16960/17010 (100%)] Loss: 0.007779
    epoch          : 10
    Train_loss     : 0.016087168695194942
    Train_accuracy : 0.9948895676691729
    Train_f1_score : 0.9948896169662476
    Val_loss       : 0.04134152217229712
    Val_accuracy   : 0.9875
    Val_f1_score   : 0.9875000715255737
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch10.pth ...
Train Epoch: 11 [0/17010 (0%)] Loss: 0.007008
Train Epoch: 11 [160/17010 (1%)] Loss: 0.003935
Train Epoch: 11 [320/17010 (2%)] Loss: 0.003703
Train Epoch: 11 [480/17010 (3%)] Loss: 0.000254
Train Epoch: 11 [640/17010 (4%)] Loss: 0.000893
Train Epoch: 11 [800/17010 (5%)] Loss: 0.010963
Train Epoch: 11 [960/17010 (6%)] Loss: 0.000589
Train Epoch: 11 [1120/17010 (7%)] Loss: 0.000417
Train Epoch: 11 [1280/17010 (8%)] Loss: 0.015890
Train Epoch: 11 [1440/17010 (8%)] Loss: 0.071868
Train Epoch: 11 [1600/17010 (9%)] Loss: 0.003133
Train Epoch: 11 [1760/17010 (10%)] Loss: 0.005263
Train Epoch: 11 [1920/17010 (11%)] Loss: 0.001367
Train Epoch: 11 [2080/17010 (12%)] Loss: 0.000808
Train Epoch: 11 [2240/17010 (13%)] Loss: 0.001245
Train Epoch: 11 [2400/17010 (14%)] Loss: 0.002867
Train Epoch: 11 [2560/17010 (15%)] Loss: 0.013209
Train Epoch: 11 [2720/17010 (16%)] Loss: 0.005516
Train Epoch: 11 [2880/17010 (17%)] Loss: 0.016737
Train Epoch: 11 [3040/17010 (18%)] Loss: 0.026806
Train Epoch: 11 [3200/17010 (19%)] Loss: 0.002826
Train Epoch: 11 [3360/17010 (20%)] Loss: 0.002496
Train Epoch: 11 [3520/17010 (21%)] Loss: 0.040090
Train Epoch: 11 [3680/17010 (22%)] Loss: 0.002545
Train Epoch: 11 [3840/17010 (23%)] Loss: 0.053366
Train Epoch: 11 [4000/17010 (24%)] Loss: 0.000295
Train Epoch: 11 [4160/17010 (24%)] Loss: 0.000478
Train Epoch: 11 [4320/17010 (25%)] Loss: 0.108402
Train Epoch: 11 [4480/17010 (26%)] Loss: 0.000633
Train Epoch: 11 [4640/17010 (27%)] Loss: 0.000558
Train Epoch: 11 [4800/17010 (28%)] Loss: 0.000370
Train Epoch: 11 [4960/17010 (29%)] Loss: 0.001607
Train Epoch: 11 [5120/17010 (30%)] Loss: 0.022199
Train Epoch: 11 [5280/17010 (31%)] Loss: 0.115629
Train Epoch: 11 [5440/17010 (32%)] Loss: 0.001112
Train Epoch: 11 [5600/17010 (33%)] Loss: 0.001948
Train Epoch: 11 [5760/17010 (34%)] Loss: 0.000526
Train Epoch: 11 [5920/17010 (35%)] Loss: 0.000667
Train Epoch: 11 [6080/17010 (36%)] Loss: 0.039835
Train Epoch: 11 [6240/17010 (37%)] Loss: 0.095400
Train Epoch: 11 [6400/17010 (38%)] Loss: 0.001504
Train Epoch: 11 [6560/17010 (39%)] Loss: 0.002772
Train Epoch: 11 [6720/17010 (40%)] Loss: 0.064837
Train Epoch: 11 [6880/17010 (40%)] Loss: 0.001534
Train Epoch: 11 [7040/17010 (41%)] Loss: 0.000847
Train Epoch: 11 [7200/17010 (42%)] Loss: 0.003019
Train Epoch: 11 [7360/17010 (43%)] Loss: 0.133233
Train Epoch: 11 [7520/17010 (44%)] Loss: 0.003098
Train Epoch: 11 [7680/17010 (45%)] Loss: 0.004600
Train Epoch: 11 [7840/17010 (46%)] Loss: 0.005060
Train Epoch: 11 [8000/17010 (47%)] Loss: 0.003583
Train Epoch: 11 [8160/17010 (48%)] Loss: 0.090728
Train Epoch: 11 [8320/17010 (49%)] Loss: 0.003600
Train Epoch: 11 [8480/17010 (50%)] Loss: 0.016923
Train Epoch: 11 [8640/17010 (51%)] Loss: 0.012542
Train Epoch: 11 [8800/17010 (52%)] Loss: 0.205702
Train Epoch: 11 [8960/17010 (53%)] Loss: 0.003304
Train Epoch: 11 [9120/17010 (54%)] Loss: 0.000436
Train Epoch: 11 [9280/17010 (55%)] Loss: 0.004511
Train Epoch: 11 [9440/17010 (55%)] Loss: 0.008011
Train Epoch: 11 [9600/17010 (56%)] Loss: 0.002890
Train Epoch: 11 [9760/17010 (57%)] Loss: 0.000449
Train Epoch: 11 [9920/17010 (58%)] Loss: 0.004810
Train Epoch: 11 [10080/17010 (59%)] Loss: 0.017208
Train Epoch: 11 [10240/17010 (60%)] Loss: 0.000261
Train Epoch: 11 [10400/17010 (61%)] Loss: 0.016575
Train Epoch: 11 [10560/17010 (62%)] Loss: 0.009770
Train Epoch: 11 [10720/17010 (63%)] Loss: 0.050044
Train Epoch: 11 [10880/17010 (64%)] Loss: 0.004641
Train Epoch: 11 [11040/17010 (65%)] Loss: 0.011270
Train Epoch: 11 [11200/17010 (66%)] Loss: 0.000904
Train Epoch: 11 [11360/17010 (67%)] Loss: 0.007731
Train Epoch: 11 [11520/17010 (68%)] Loss: 0.015659
Train Epoch: 11 [11680/17010 (69%)] Loss: 0.000590
Train Epoch: 11 [11840/17010 (70%)] Loss: 0.014486
Train Epoch: 11 [12000/17010 (71%)] Loss: 0.006976
Train Epoch: 11 [12160/17010 (71%)] Loss: 0.004076
Train Epoch: 11 [12320/17010 (72%)] Loss: 0.000897
Train Epoch: 11 [12480/17010 (73%)] Loss: 0.010574
Train Epoch: 11 [12640/17010 (74%)] Loss: 0.001171
Train Epoch: 11 [12800/17010 (75%)] Loss: 0.067195
Train Epoch: 11 [12960/17010 (76%)] Loss: 0.075511
Train Epoch: 11 [13120/17010 (77%)] Loss: 0.002065
Train Epoch: 11 [13280/17010 (78%)] Loss: 0.000433
Train Epoch: 11 [13440/17010 (79%)] Loss: 0.000449
Train Epoch: 11 [13600/17010 (80%)] Loss: 0.027292
Train Epoch: 11 [13760/17010 (81%)] Loss: 0.037120
Train Epoch: 11 [13920/17010 (82%)] Loss: 0.001312
Train Epoch: 11 [14080/17010 (83%)] Loss: 0.001009
Train Epoch: 11 [14240/17010 (84%)] Loss: 0.000991
Train Epoch: 11 [14400/17010 (85%)] Loss: 0.003601
Train Epoch: 11 [14560/17010 (86%)] Loss: 0.001474
Train Epoch: 11 [14720/17010 (87%)] Loss: 0.034883
Train Epoch: 11 [14880/17010 (87%)] Loss: 0.001016
Train Epoch: 11 [15040/17010 (88%)] Loss: 0.001452
Train Epoch: 11 [15200/17010 (89%)] Loss: 0.002230
Train Epoch: 11 [15360/17010 (90%)] Loss: 0.001266
Train Epoch: 11 [15520/17010 (91%)] Loss: 0.084033
Train Epoch: 11 [15680/17010 (92%)] Loss: 0.005362
Train Epoch: 11 [15840/17010 (93%)] Loss: 0.000356
Train Epoch: 11 [16000/17010 (94%)] Loss: 0.026493
Train Epoch: 11 [16160/17010 (95%)] Loss: 0.005322
Train Epoch: 11 [16320/17010 (96%)] Loss: 0.000522
Train Epoch: 11 [16480/17010 (97%)] Loss: 0.002163
Train Epoch: 11 [16640/17010 (98%)] Loss: 0.001301
Train Epoch: 11 [16800/17010 (99%)] Loss: 0.011620
Train Epoch: 11 [16960/17010 (100%)] Loss: 0.000909
    epoch          : 11
    Train_loss     : 0.021055447005283737
    Train_accuracy : 0.9937734962406015
    Train_f1_score : 0.9937735199928284
    Val_loss       : 0.0654110821664896
    Val_accuracy   : 0.9791666666666666
    Val_f1_score   : 0.9791667461395264
Train Epoch: 12 [0/17010 (0%)] Loss: 0.001110
Train Epoch: 12 [160/17010 (1%)] Loss: 0.002670
Train Epoch: 12 [320/17010 (2%)] Loss: 0.000172
Train Epoch: 12 [480/17010 (3%)] Loss: 0.019718
Train Epoch: 12 [640/17010 (4%)] Loss: 0.000394
Train Epoch: 12 [800/17010 (5%)] Loss: 0.039378
Train Epoch: 12 [960/17010 (6%)] Loss: 0.000131
Train Epoch: 12 [1120/17010 (7%)] Loss: 0.003674
Train Epoch: 12 [1280/17010 (8%)] Loss: 0.001912
Train Epoch: 12 [1440/17010 (8%)] Loss: 0.001549
Train Epoch: 12 [1600/17010 (9%)] Loss: 0.002765
Train Epoch: 12 [1760/17010 (10%)] Loss: 0.002527
Train Epoch: 12 [1920/17010 (11%)] Loss: 0.001151
Train Epoch: 12 [2080/17010 (12%)] Loss: 0.002101
Train Epoch: 12 [2240/17010 (13%)] Loss: 0.001835
Train Epoch: 12 [2400/17010 (14%)] Loss: 0.029369
Train Epoch: 12 [2560/17010 (15%)] Loss: 0.006417
Train Epoch: 12 [2720/17010 (16%)] Loss: 0.001957
Train Epoch: 12 [2880/17010 (17%)] Loss: 0.000376
Train Epoch: 12 [3040/17010 (18%)] Loss: 0.002473
Train Epoch: 12 [3200/17010 (19%)] Loss: 0.001243
Train Epoch: 12 [3360/17010 (20%)] Loss: 0.000231
Train Epoch: 12 [3520/17010 (21%)] Loss: 0.000435
Train Epoch: 12 [3680/17010 (22%)] Loss: 0.000941
Train Epoch: 12 [3840/17010 (23%)] Loss: 0.001921
Train Epoch: 12 [4000/17010 (24%)] Loss: 0.002840
Train Epoch: 12 [4160/17010 (24%)] Loss: 0.000787
Train Epoch: 12 [4320/17010 (25%)] Loss: 0.010192
Train Epoch: 12 [4480/17010 (26%)] Loss: 0.000942
Train Epoch: 12 [4640/17010 (27%)] Loss: 0.019168
Train Epoch: 12 [4800/17010 (28%)] Loss: 0.029354
Train Epoch: 12 [4960/17010 (29%)] Loss: 0.004252
Train Epoch: 12 [5120/17010 (30%)] Loss: 0.000530
Train Epoch: 12 [5280/17010 (31%)] Loss: 0.000687
Train Epoch: 12 [5440/17010 (32%)] Loss: 0.011262
Train Epoch: 12 [5600/17010 (33%)] Loss: 0.021116
Train Epoch: 12 [5760/17010 (34%)] Loss: 0.000921
Train Epoch: 12 [5920/17010 (35%)] Loss: 0.007631
Train Epoch: 12 [6080/17010 (36%)] Loss: 0.010439
Train Epoch: 12 [6240/17010 (37%)] Loss: 0.009532
Train Epoch: 12 [6400/17010 (38%)] Loss: 0.010892
Train Epoch: 12 [6560/17010 (39%)] Loss: 0.000931
Train Epoch: 12 [6720/17010 (40%)] Loss: 0.003085
Train Epoch: 12 [6880/17010 (40%)] Loss: 0.001627
Train Epoch: 12 [7040/17010 (41%)] Loss: 0.001095
Train Epoch: 12 [7200/17010 (42%)] Loss: 0.000418
Train Epoch: 12 [7360/17010 (43%)] Loss: 0.002915
Train Epoch: 12 [7520/17010 (44%)] Loss: 0.006598
Train Epoch: 12 [7680/17010 (45%)] Loss: 0.001026
Train Epoch: 12 [7840/17010 (46%)] Loss: 0.000748
Train Epoch: 12 [8000/17010 (47%)] Loss: 0.031160
Train Epoch: 12 [8160/17010 (48%)] Loss: 0.002928
Train Epoch: 12 [8320/17010 (49%)] Loss: 0.009726
Train Epoch: 12 [8480/17010 (50%)] Loss: 0.027242
Train Epoch: 12 [8640/17010 (51%)] Loss: 0.037793
Train Epoch: 12 [8800/17010 (52%)] Loss: 0.000813
Train Epoch: 12 [8960/17010 (53%)] Loss: 0.001518
Train Epoch: 12 [9120/17010 (54%)] Loss: 0.013300
Train Epoch: 12 [9280/17010 (55%)] Loss: 0.004663
Train Epoch: 12 [9440/17010 (55%)] Loss: 0.000158
Train Epoch: 12 [9600/17010 (56%)] Loss: 0.000996
Train Epoch: 12 [9760/17010 (57%)] Loss: 0.001344
Train Epoch: 12 [9920/17010 (58%)] Loss: 0.000626
Train Epoch: 12 [10080/17010 (59%)] Loss: 0.103597
Train Epoch: 12 [10240/17010 (60%)] Loss: 0.000478
Train Epoch: 12 [10400/17010 (61%)] Loss: 0.000410
Train Epoch: 12 [10560/17010 (62%)] Loss: 0.003598
Train Epoch: 12 [10720/17010 (63%)] Loss: 0.001204
Train Epoch: 12 [10880/17010 (64%)] Loss: 0.000843
Train Epoch: 12 [11040/17010 (65%)] Loss: 0.085833
Train Epoch: 12 [11200/17010 (66%)] Loss: 0.022302
Train Epoch: 12 [11360/17010 (67%)] Loss: 0.000948
Train Epoch: 12 [11520/17010 (68%)] Loss: 0.002019
Train Epoch: 12 [11680/17010 (69%)] Loss: 0.088768
Train Epoch: 12 [11840/17010 (70%)] Loss: 0.000472
Train Epoch: 12 [12000/17010 (71%)] Loss: 0.001698
Train Epoch: 12 [12160/17010 (71%)] Loss: 0.005660
Train Epoch: 12 [12320/17010 (72%)] Loss: 0.000752
Train Epoch: 12 [12480/17010 (73%)] Loss: 0.000998
Train Epoch: 12 [12640/17010 (74%)] Loss: 0.000961
Train Epoch: 12 [12800/17010 (75%)] Loss: 0.017759
Train Epoch: 12 [12960/17010 (76%)] Loss: 0.000338
Train Epoch: 12 [13120/17010 (77%)] Loss: 0.011017
Train Epoch: 12 [13280/17010 (78%)] Loss: 0.001718
Train Epoch: 12 [13440/17010 (79%)] Loss: 0.001020
Train Epoch: 12 [13600/17010 (80%)] Loss: 0.000980
Train Epoch: 12 [13760/17010 (81%)] Loss: 0.000215
Train Epoch: 12 [13920/17010 (82%)] Loss: 0.007721
Train Epoch: 12 [14080/17010 (83%)] Loss: 0.002140
Train Epoch: 12 [14240/17010 (84%)] Loss: 0.003826
Train Epoch: 12 [14400/17010 (85%)] Loss: 0.000514
Train Epoch: 12 [14560/17010 (86%)] Loss: 0.002946
Train Epoch: 12 [14720/17010 (87%)] Loss: 0.001216
Train Epoch: 12 [14880/17010 (87%)] Loss: 0.002563
Train Epoch: 12 [15040/17010 (88%)] Loss: 0.003658
Train Epoch: 12 [15200/17010 (89%)] Loss: 0.001269
Train Epoch: 12 [15360/17010 (90%)] Loss: 0.000243
Train Epoch: 12 [15520/17010 (91%)] Loss: 0.005804
Train Epoch: 12 [15680/17010 (92%)] Loss: 0.002221
Train Epoch: 12 [15840/17010 (93%)] Loss: 0.000288
Train Epoch: 12 [16000/17010 (94%)] Loss: 0.018937
Train Epoch: 12 [16160/17010 (95%)] Loss: 0.000526
Train Epoch: 12 [16320/17010 (96%)] Loss: 0.003380
Train Epoch: 12 [16480/17010 (97%)] Loss: 0.035369
Train Epoch: 12 [16640/17010 (98%)] Loss: 0.000438
Train Epoch: 12 [16800/17010 (99%)] Loss: 0.000506
Train Epoch: 12 [16960/17010 (100%)] Loss: 0.003232
    epoch          : 12
    Train_loss     : 0.015157904770275397
    Train_accuracy : 0.9953138053467001
    Train_f1_score : 0.9953138828277588
    Val_loss       : 0.03809761645449423
    Val_accuracy   : 0.9869791666666666
    Val_f1_score   : 0.9869792461395264
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch12.pth ...
Train Epoch: 13 [0/17010 (0%)] Loss: 0.018182
Train Epoch: 13 [160/17010 (1%)] Loss: 0.000329
Train Epoch: 13 [320/17010 (2%)] Loss: 0.002256
Train Epoch: 13 [480/17010 (3%)] Loss: 0.007673
Train Epoch: 13 [640/17010 (4%)] Loss: 0.011371
Train Epoch: 13 [800/17010 (5%)] Loss: 0.004654
Train Epoch: 13 [960/17010 (6%)] Loss: 0.002740
Train Epoch: 13 [1120/17010 (7%)] Loss: 0.020955
Train Epoch: 13 [1280/17010 (8%)] Loss: 0.043668
Train Epoch: 13 [1440/17010 (8%)] Loss: 0.000623
Train Epoch: 13 [1600/17010 (9%)] Loss: 0.037267
Train Epoch: 13 [1760/17010 (10%)] Loss: 0.095599
Train Epoch: 13 [1920/17010 (11%)] Loss: 0.040196
Train Epoch: 13 [2080/17010 (12%)] Loss: 0.002363
Train Epoch: 13 [2240/17010 (13%)] Loss: 0.006097
Train Epoch: 13 [2400/17010 (14%)] Loss: 0.000386
Train Epoch: 13 [2560/17010 (15%)] Loss: 0.000405
Train Epoch: 13 [2720/17010 (16%)] Loss: 0.021503
Train Epoch: 13 [2880/17010 (17%)] Loss: 0.001347
Train Epoch: 13 [3040/17010 (18%)] Loss: 0.002414
Train Epoch: 13 [3200/17010 (19%)] Loss: 0.031216
Train Epoch: 13 [3360/17010 (20%)] Loss: 0.000205
Train Epoch: 13 [3520/17010 (21%)] Loss: 0.000646
Train Epoch: 13 [3680/17010 (22%)] Loss: 0.004605
Train Epoch: 13 [3840/17010 (23%)] Loss: 0.145360
Train Epoch: 13 [4000/17010 (24%)] Loss: 0.050086
Train Epoch: 13 [4160/17010 (24%)] Loss: 0.008909
Train Epoch: 13 [4320/17010 (25%)] Loss: 0.001783
Train Epoch: 13 [4480/17010 (26%)] Loss: 0.006516
Train Epoch: 13 [4640/17010 (27%)] Loss: 0.008859
Train Epoch: 13 [4800/17010 (28%)] Loss: 0.002039
Train Epoch: 13 [4960/17010 (29%)] Loss: 0.006211
Train Epoch: 13 [5120/17010 (30%)] Loss: 0.075595
Train Epoch: 13 [5280/17010 (31%)] Loss: 0.015123
Train Epoch: 13 [5440/17010 (32%)] Loss: 0.008356
Train Epoch: 13 [5600/17010 (33%)] Loss: 0.010326
Train Epoch: 13 [5760/17010 (34%)] Loss: 0.000562
Train Epoch: 13 [5920/17010 (35%)] Loss: 0.006990
Train Epoch: 13 [6080/17010 (36%)] Loss: 0.000239
Train Epoch: 13 [6240/17010 (37%)] Loss: 0.000807
Train Epoch: 13 [6400/17010 (38%)] Loss: 0.003180
Train Epoch: 13 [6560/17010 (39%)] Loss: 0.088000
Train Epoch: 13 [6720/17010 (40%)] Loss: 0.000220
Train Epoch: 13 [6880/17010 (40%)] Loss: 0.008076
Train Epoch: 13 [7040/17010 (41%)] Loss: 0.005211
Train Epoch: 13 [7200/17010 (42%)] Loss: 0.004516
Train Epoch: 13 [7360/17010 (43%)] Loss: 0.002311
Train Epoch: 13 [7520/17010 (44%)] Loss: 0.000823
Train Epoch: 13 [7680/17010 (45%)] Loss: 0.005746
Train Epoch: 13 [7840/17010 (46%)] Loss: 0.000119
Train Epoch: 13 [8000/17010 (47%)] Loss: 0.001032
Train Epoch: 13 [8160/17010 (48%)] Loss: 0.004517
Train Epoch: 13 [8320/17010 (49%)] Loss: 0.001898
Train Epoch: 13 [8480/17010 (50%)] Loss: 0.004567
Train Epoch: 13 [8640/17010 (51%)] Loss: 0.004412
Train Epoch: 13 [8800/17010 (52%)] Loss: 0.009539
Train Epoch: 13 [8960/17010 (53%)] Loss: 0.001733
Train Epoch: 13 [9120/17010 (54%)] Loss: 0.003991
Train Epoch: 13 [9280/17010 (55%)] Loss: 0.004447
Train Epoch: 13 [9440/17010 (55%)] Loss: 0.002556
Train Epoch: 13 [9600/17010 (56%)] Loss: 0.000394
Train Epoch: 13 [9760/17010 (57%)] Loss: 0.001236
Train Epoch: 13 [9920/17010 (58%)] Loss: 0.004441
Train Epoch: 13 [10080/17010 (59%)] Loss: 0.000195
Train Epoch: 13 [10240/17010 (60%)] Loss: 0.000403
Train Epoch: 13 [10400/17010 (61%)] Loss: 0.000606
Train Epoch: 13 [10560/17010 (62%)] Loss: 0.072109
Train Epoch: 13 [10720/17010 (63%)] Loss: 0.002658
Train Epoch: 13 [10880/17010 (64%)] Loss: 0.142318
Train Epoch: 13 [11040/17010 (65%)] Loss: 0.000755
Train Epoch: 13 [11200/17010 (66%)] Loss: 0.005977
Train Epoch: 13 [11360/17010 (67%)] Loss: 0.002688
Train Epoch: 13 [11520/17010 (68%)] Loss: 0.000549
Train Epoch: 13 [11680/17010 (69%)] Loss: 0.046292
Train Epoch: 13 [11840/17010 (70%)] Loss: 0.001173
Train Epoch: 13 [12000/17010 (71%)] Loss: 0.001517
Train Epoch: 13 [12160/17010 (71%)] Loss: 0.005788
Train Epoch: 13 [12320/17010 (72%)] Loss: 0.005435
Train Epoch: 13 [12480/17010 (73%)] Loss: 0.001452
Train Epoch: 13 [12640/17010 (74%)] Loss: 0.000505
Train Epoch: 13 [12800/17010 (75%)] Loss: 0.000385
Train Epoch: 13 [12960/17010 (76%)] Loss: 0.001273
Train Epoch: 13 [13120/17010 (77%)] Loss: 0.000862
Train Epoch: 13 [13280/17010 (78%)] Loss: 0.002922
Train Epoch: 13 [13440/17010 (79%)] Loss: 0.000887
Train Epoch: 13 [13600/17010 (80%)] Loss: 0.006445
Train Epoch: 13 [13760/17010 (81%)] Loss: 0.000250
Train Epoch: 13 [13920/17010 (82%)] Loss: 0.000249
Train Epoch: 13 [14080/17010 (83%)] Loss: 0.001440
Train Epoch: 13 [14240/17010 (84%)] Loss: 0.016291
Train Epoch: 13 [14400/17010 (85%)] Loss: 0.001553
Train Epoch: 13 [14560/17010 (86%)] Loss: 0.011222
Train Epoch: 13 [14720/17010 (87%)] Loss: 0.001252
Train Epoch: 13 [14880/17010 (87%)] Loss: 0.002612
Train Epoch: 13 [15040/17010 (88%)] Loss: 0.001153
Train Epoch: 13 [15200/17010 (89%)] Loss: 0.000228
Train Epoch: 13 [15360/17010 (90%)] Loss: 0.001486
Train Epoch: 13 [15520/17010 (91%)] Loss: 0.061743
Train Epoch: 13 [15680/17010 (92%)] Loss: 0.000842
Train Epoch: 13 [15840/17010 (93%)] Loss: 0.001538
Train Epoch: 13 [16000/17010 (94%)] Loss: 0.002571
Train Epoch: 13 [16160/17010 (95%)] Loss: 0.007609
Train Epoch: 13 [16320/17010 (96%)] Loss: 0.001066
Train Epoch: 13 [16480/17010 (97%)] Loss: 0.004347
Train Epoch: 13 [16640/17010 (98%)] Loss: 0.044373
Train Epoch: 13 [16800/17010 (99%)] Loss: 0.001609
Train Epoch: 13 [16960/17010 (100%)] Loss: 0.000064
    epoch          : 13
    Train_loss     : 0.013498864069073793
    Train_accuracy : 0.9957706766917294
    Train_f1_score : 0.9957706928253174
    Val_loss       : 0.05743078921229123
    Val_accuracy   : 0.9885416666666667
    Val_f1_score   : 0.9885417222976685
Train Epoch: 14 [0/17010 (0%)] Loss: 0.007290
Train Epoch: 14 [160/17010 (1%)] Loss: 0.001249
Train Epoch: 14 [320/17010 (2%)] Loss: 0.000229
Train Epoch: 14 [480/17010 (3%)] Loss: 0.010906
Train Epoch: 14 [640/17010 (4%)] Loss: 0.143760
Train Epoch: 14 [800/17010 (5%)] Loss: 0.003625
Train Epoch: 14 [960/17010 (6%)] Loss: 0.000478
Train Epoch: 14 [1120/17010 (7%)] Loss: 0.004024
Train Epoch: 14 [1280/17010 (8%)] Loss: 0.000170
Train Epoch: 14 [1440/17010 (8%)] Loss: 0.001510
Train Epoch: 14 [1600/17010 (9%)] Loss: 0.000495
Train Epoch: 14 [1760/17010 (10%)] Loss: 0.017551
Train Epoch: 14 [1920/17010 (11%)] Loss: 0.008033
Train Epoch: 14 [2080/17010 (12%)] Loss: 0.002761
Train Epoch: 14 [2240/17010 (13%)] Loss: 0.001596
Train Epoch: 14 [2400/17010 (14%)] Loss: 0.000823
Train Epoch: 14 [2560/17010 (15%)] Loss: 0.078506
Train Epoch: 14 [2720/17010 (16%)] Loss: 0.002301
Train Epoch: 14 [2880/17010 (17%)] Loss: 0.000781
Train Epoch: 14 [3040/17010 (18%)] Loss: 0.002292
Train Epoch: 14 [3200/17010 (19%)] Loss: 0.018456
Train Epoch: 14 [3360/17010 (20%)] Loss: 0.000593
Train Epoch: 14 [3520/17010 (21%)] Loss: 0.000274
Train Epoch: 14 [3680/17010 (22%)] Loss: 0.001213
Train Epoch: 14 [3840/17010 (23%)] Loss: 0.001496
Train Epoch: 14 [4000/17010 (24%)] Loss: 0.000178
Train Epoch: 14 [4160/17010 (24%)] Loss: 0.001614
Train Epoch: 14 [4320/17010 (25%)] Loss: 0.000806
Train Epoch: 14 [4480/17010 (26%)] Loss: 0.004324
Train Epoch: 14 [4640/17010 (27%)] Loss: 0.000588
Train Epoch: 14 [4800/17010 (28%)] Loss: 0.003504
Train Epoch: 14 [4960/17010 (29%)] Loss: 0.000406
Train Epoch: 14 [5120/17010 (30%)] Loss: 0.001394
Train Epoch: 14 [5280/17010 (31%)] Loss: 0.002963
Train Epoch: 14 [5440/17010 (32%)] Loss: 0.008831
Train Epoch: 14 [5600/17010 (33%)] Loss: 0.000413
Train Epoch: 14 [5760/17010 (34%)] Loss: 0.002038
Train Epoch: 14 [5920/17010 (35%)] Loss: 0.000585
Train Epoch: 14 [6080/17010 (36%)] Loss: 0.001748
Train Epoch: 14 [6240/17010 (37%)] Loss: 0.000450
Train Epoch: 14 [6400/17010 (38%)] Loss: 0.000539
Train Epoch: 14 [6560/17010 (39%)] Loss: 0.003076
Train Epoch: 14 [6720/17010 (40%)] Loss: 0.001358
Train Epoch: 14 [6880/17010 (40%)] Loss: 0.000664
Train Epoch: 14 [7040/17010 (41%)] Loss: 0.000198
Train Epoch: 14 [7200/17010 (42%)] Loss: 0.006231
Train Epoch: 14 [7360/17010 (43%)] Loss: 0.000442
Train Epoch: 14 [7520/17010 (44%)] Loss: 0.000527
Train Epoch: 14 [7680/17010 (45%)] Loss: 0.001986
Train Epoch: 14 [7840/17010 (46%)] Loss: 0.003860
Train Epoch: 14 [8000/17010 (47%)] Loss: 0.000265
Train Epoch: 14 [8160/17010 (48%)] Loss: 0.000432
Train Epoch: 14 [8320/17010 (49%)] Loss: 0.000212
Train Epoch: 14 [8480/17010 (50%)] Loss: 0.000131
Train Epoch: 14 [8640/17010 (51%)] Loss: 0.001430
Train Epoch: 14 [8800/17010 (52%)] Loss: 0.000214
Train Epoch: 14 [8960/17010 (53%)] Loss: 0.000134
Train Epoch: 14 [9120/17010 (54%)] Loss: 0.000128
Train Epoch: 14 [9280/17010 (55%)] Loss: 0.000291
Train Epoch: 14 [9440/17010 (55%)] Loss: 0.000097
Train Epoch: 14 [9600/17010 (56%)] Loss: 0.000226
Train Epoch: 14 [9760/17010 (57%)] Loss: 0.002463
Train Epoch: 14 [9920/17010 (58%)] Loss: 0.019273
Train Epoch: 14 [10080/17010 (59%)] Loss: 0.000810
Train Epoch: 14 [10240/17010 (60%)] Loss: 0.000159
Train Epoch: 14 [10400/17010 (61%)] Loss: 0.007908
Train Epoch: 14 [10560/17010 (62%)] Loss: 0.002313
Train Epoch: 14 [10720/17010 (63%)] Loss: 0.006207
Train Epoch: 14 [10880/17010 (64%)] Loss: 0.000110
Train Epoch: 14 [11040/17010 (65%)] Loss: 0.005266
Train Epoch: 14 [11200/17010 (66%)] Loss: 0.001031
Train Epoch: 14 [11360/17010 (67%)] Loss: 0.000434
Train Epoch: 14 [11520/17010 (68%)] Loss: 0.000804
Train Epoch: 14 [11680/17010 (69%)] Loss: 0.005541
Train Epoch: 14 [11840/17010 (70%)] Loss: 0.019818
Train Epoch: 14 [12000/17010 (71%)] Loss: 0.000407
Train Epoch: 14 [12160/17010 (71%)] Loss: 0.002998
Train Epoch: 14 [12320/17010 (72%)] Loss: 0.002659
Train Epoch: 14 [12480/17010 (73%)] Loss: 0.000769
Train Epoch: 14 [12640/17010 (74%)] Loss: 0.000275
Train Epoch: 14 [12800/17010 (75%)] Loss: 0.001301
Train Epoch: 14 [12960/17010 (76%)] Loss: 0.000312
Train Epoch: 14 [13120/17010 (77%)] Loss: 0.000290
Train Epoch: 14 [13280/17010 (78%)] Loss: 0.008838
Train Epoch: 14 [13440/17010 (79%)] Loss: 0.025661
Train Epoch: 14 [13600/17010 (80%)] Loss: 0.000795
Train Epoch: 14 [13760/17010 (81%)] Loss: 0.000247
Train Epoch: 14 [13920/17010 (82%)] Loss: 0.001465
Train Epoch: 14 [14080/17010 (83%)] Loss: 0.000181
Train Epoch: 14 [14240/17010 (84%)] Loss: 0.000283
Train Epoch: 14 [14400/17010 (85%)] Loss: 0.000629
Train Epoch: 14 [14560/17010 (86%)] Loss: 0.000898
Train Epoch: 14 [14720/17010 (87%)] Loss: 0.000566
Train Epoch: 14 [14880/17010 (87%)] Loss: 0.000195
Train Epoch: 14 [15040/17010 (88%)] Loss: 0.000615
Train Epoch: 14 [15200/17010 (89%)] Loss: 0.000345
Train Epoch: 14 [15360/17010 (90%)] Loss: 0.000172
Train Epoch: 14 [15520/17010 (91%)] Loss: 0.001762
Train Epoch: 14 [15680/17010 (92%)] Loss: 0.027031
Train Epoch: 14 [15840/17010 (93%)] Loss: 0.063282
Train Epoch: 14 [16000/17010 (94%)] Loss: 0.001196
Train Epoch: 14 [16160/17010 (95%)] Loss: 0.000489
Train Epoch: 14 [16320/17010 (96%)] Loss: 0.001231
Train Epoch: 14 [16480/17010 (97%)] Loss: 0.000327
Train Epoch: 14 [16640/17010 (98%)] Loss: 0.001285
Train Epoch: 14 [16800/17010 (99%)] Loss: 0.001143
Train Epoch: 14 [16960/17010 (100%)] Loss: 0.002045
    epoch          : 14
    Train_loss     : 0.008681047176603323
    Train_accuracy : 0.9974741541353384
    Train_f1_score : 0.997474193572998
    Val_loss       : 0.06384697313340743
    Val_accuracy   : 0.9838541666666667
    Val_f1_score   : 0.9838542342185974
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch14.pth ...
Train Epoch: 15 [0/17010 (0%)] Loss: 0.064687
Train Epoch: 15 [160/17010 (1%)] Loss: 0.000292
Train Epoch: 15 [320/17010 (2%)] Loss: 0.000221
Train Epoch: 15 [480/17010 (3%)] Loss: 0.004052
Train Epoch: 15 [640/17010 (4%)] Loss: 0.000547
Train Epoch: 15 [800/17010 (5%)] Loss: 0.000403
Train Epoch: 15 [960/17010 (6%)] Loss: 0.001579
Train Epoch: 15 [1120/17010 (7%)] Loss: 0.001614
Train Epoch: 15 [1280/17010 (8%)] Loss: 0.091436
Train Epoch: 15 [1440/17010 (8%)] Loss: 0.003244
Train Epoch: 15 [1600/17010 (9%)] Loss: 0.000477
Train Epoch: 15 [1760/17010 (10%)] Loss: 0.006344
Train Epoch: 15 [1920/17010 (11%)] Loss: 0.003031
Train Epoch: 15 [2080/17010 (12%)] Loss: 0.002980
Train Epoch: 15 [2240/17010 (13%)] Loss: 0.051282
Train Epoch: 15 [2400/17010 (14%)] Loss: 0.000626
Train Epoch: 15 [2560/17010 (15%)] Loss: 0.218467
Train Epoch: 15 [2720/17010 (16%)] Loss: 0.002670
Train Epoch: 15 [2880/17010 (17%)] Loss: 0.003636
Train Epoch: 15 [3040/17010 (18%)] Loss: 0.001218
Train Epoch: 15 [3200/17010 (19%)] Loss: 0.000947
Train Epoch: 15 [3360/17010 (20%)] Loss: 0.018599
Train Epoch: 15 [3520/17010 (21%)] Loss: 0.052872
Train Epoch: 15 [3680/17010 (22%)] Loss: 0.013396
Train Epoch: 15 [3840/17010 (23%)] Loss: 0.004867
Train Epoch: 15 [4000/17010 (24%)] Loss: 0.000781
Train Epoch: 15 [4160/17010 (24%)] Loss: 0.002130
Train Epoch: 15 [4320/17010 (25%)] Loss: 0.006652
Train Epoch: 15 [4480/17010 (26%)] Loss: 0.001524
Train Epoch: 15 [4640/17010 (27%)] Loss: 0.000649
Train Epoch: 15 [4800/17010 (28%)] Loss: 0.001980
Train Epoch: 15 [4960/17010 (29%)] Loss: 0.001875
Train Epoch: 15 [5120/17010 (30%)] Loss: 0.009495
Train Epoch: 15 [5280/17010 (31%)] Loss: 0.007144
Train Epoch: 15 [5440/17010 (32%)] Loss: 0.005247
Train Epoch: 15 [5600/17010 (33%)] Loss: 0.001097
Train Epoch: 15 [5760/17010 (34%)] Loss: 0.002087
Train Epoch: 15 [5920/17010 (35%)] Loss: 0.010043
Train Epoch: 15 [6080/17010 (36%)] Loss: 0.198062
Train Epoch: 15 [6240/17010 (37%)] Loss: 0.000648
Train Epoch: 15 [6400/17010 (38%)] Loss: 0.002343
Train Epoch: 15 [6560/17010 (39%)] Loss: 0.000970
Train Epoch: 15 [6720/17010 (40%)] Loss: 0.001012
Train Epoch: 15 [6880/17010 (40%)] Loss: 0.000390
Train Epoch: 15 [7040/17010 (41%)] Loss: 0.000742
Train Epoch: 15 [7200/17010 (42%)] Loss: 0.000108
Train Epoch: 15 [7360/17010 (43%)] Loss: 0.005709
Train Epoch: 15 [7520/17010 (44%)] Loss: 0.017343
Train Epoch: 15 [7680/17010 (45%)] Loss: 0.003428
Train Epoch: 15 [7840/17010 (46%)] Loss: 0.014036
Train Epoch: 15 [8000/17010 (47%)] Loss: 0.001057
Train Epoch: 15 [8160/17010 (48%)] Loss: 0.000462
Train Epoch: 15 [8320/17010 (49%)] Loss: 0.001452
Train Epoch: 15 [8480/17010 (50%)] Loss: 0.004496
Train Epoch: 15 [8640/17010 (51%)] Loss: 0.126293
Train Epoch: 15 [8800/17010 (52%)] Loss: 0.001140
Train Epoch: 15 [8960/17010 (53%)] Loss: 0.000366
Train Epoch: 15 [9120/17010 (54%)] Loss: 0.007796
Train Epoch: 15 [9280/17010 (55%)] Loss: 0.017819
Train Epoch: 15 [9440/17010 (55%)] Loss: 0.005368
Train Epoch: 15 [9600/17010 (56%)] Loss: 0.011118
Train Epoch: 15 [9760/17010 (57%)] Loss: 0.020811
Train Epoch: 15 [9920/17010 (58%)] Loss: 0.002066
Train Epoch: 15 [10080/17010 (59%)] Loss: 0.031972
Train Epoch: 15 [10240/17010 (60%)] Loss: 0.001286
Train Epoch: 15 [10400/17010 (61%)] Loss: 0.006700
Train Epoch: 15 [10560/17010 (62%)] Loss: 0.022142
Train Epoch: 15 [10720/17010 (63%)] Loss: 0.005807
Train Epoch: 15 [10880/17010 (64%)] Loss: 0.000130
Train Epoch: 15 [11040/17010 (65%)] Loss: 0.000834
Train Epoch: 15 [11200/17010 (66%)] Loss: 0.005608
Train Epoch: 15 [11360/17010 (67%)] Loss: 0.000773
Train Epoch: 15 [11520/17010 (68%)] Loss: 0.020116
Train Epoch: 15 [11680/17010 (69%)] Loss: 0.007387
Train Epoch: 15 [11840/17010 (70%)] Loss: 0.000247
Train Epoch: 15 [12000/17010 (71%)] Loss: 0.000534
Train Epoch: 15 [12160/17010 (71%)] Loss: 0.005347
Train Epoch: 15 [12320/17010 (72%)] Loss: 0.009365
Train Epoch: 15 [12480/17010 (73%)] Loss: 0.000400
Train Epoch: 15 [12640/17010 (74%)] Loss: 0.000181
Train Epoch: 15 [12800/17010 (75%)] Loss: 0.001141
Train Epoch: 15 [12960/17010 (76%)] Loss: 0.003625
Train Epoch: 15 [13120/17010 (77%)] Loss: 0.000982
Train Epoch: 15 [13280/17010 (78%)] Loss: 0.000141
Train Epoch: 15 [13440/17010 (79%)] Loss: 0.000229
Train Epoch: 15 [13600/17010 (80%)] Loss: 0.001059
Train Epoch: 15 [13760/17010 (81%)] Loss: 0.020858
Train Epoch: 15 [13920/17010 (82%)] Loss: 0.001629
Train Epoch: 15 [14080/17010 (83%)] Loss: 0.000905
Train Epoch: 15 [14240/17010 (84%)] Loss: 0.000925
Train Epoch: 15 [14400/17010 (85%)] Loss: 0.002740
Train Epoch: 15 [14560/17010 (86%)] Loss: 0.000126
Train Epoch: 15 [14720/17010 (87%)] Loss: 0.003428
Train Epoch: 15 [14880/17010 (87%)] Loss: 0.011593
Train Epoch: 15 [15040/17010 (88%)] Loss: 0.116264
Train Epoch: 15 [15200/17010 (89%)] Loss: 0.000238
Train Epoch: 15 [15360/17010 (90%)] Loss: 0.000707
Train Epoch: 15 [15520/17010 (91%)] Loss: 0.000448
Train Epoch: 15 [15680/17010 (92%)] Loss: 0.001110
Train Epoch: 15 [15840/17010 (93%)] Loss: 0.073243
Train Epoch: 15 [16000/17010 (94%)] Loss: 0.000498
Train Epoch: 15 [16160/17010 (95%)] Loss: 0.012710
Train Epoch: 15 [16320/17010 (96%)] Loss: 0.002556
Train Epoch: 15 [16480/17010 (97%)] Loss: 0.006070
Train Epoch: 15 [16640/17010 (98%)] Loss: 0.006129
Train Epoch: 15 [16800/17010 (99%)] Loss: 0.003065
Train Epoch: 15 [16960/17010 (100%)] Loss: 0.003704
    epoch          : 15
    Train_loss     : 0.017908717358705986
    Train_accuracy : 0.9948895676691729
    Train_f1_score : 0.9948896169662476
    Val_loss       : 0.05564323457426023
    Val_accuracy   : 0.9869791666666666
    Val_f1_score   : 0.9869792461395264
Train Epoch: 16 [0/17010 (0%)] Loss: 0.000345
Train Epoch: 16 [160/17010 (1%)] Loss: 0.053704
Train Epoch: 16 [320/17010 (2%)] Loss: 0.000345
Train Epoch: 16 [480/17010 (3%)] Loss: 0.001790
Train Epoch: 16 [640/17010 (4%)] Loss: 0.000668
Train Epoch: 16 [800/17010 (5%)] Loss: 0.071580
Train Epoch: 16 [960/17010 (6%)] Loss: 0.000387
Train Epoch: 16 [1120/17010 (7%)] Loss: 0.001641
Train Epoch: 16 [1280/17010 (8%)] Loss: 0.000650
Train Epoch: 16 [1440/17010 (8%)] Loss: 0.016643
Train Epoch: 16 [1600/17010 (9%)] Loss: 0.000470
Train Epoch: 16 [1760/17010 (10%)] Loss: 0.016720
Train Epoch: 16 [1920/17010 (11%)] Loss: 0.000508
Train Epoch: 16 [2080/17010 (12%)] Loss: 0.002133
Train Epoch: 16 [2240/17010 (13%)] Loss: 0.008013
Train Epoch: 16 [2400/17010 (14%)] Loss: 0.000636
Train Epoch: 16 [2560/17010 (15%)] Loss: 0.000945
Train Epoch: 16 [2720/17010 (16%)] Loss: 0.241879
Train Epoch: 16 [2880/17010 (17%)] Loss: 0.001719
Train Epoch: 16 [3040/17010 (18%)] Loss: 0.148411
Train Epoch: 16 [3200/17010 (19%)] Loss: 0.002844
Train Epoch: 16 [3360/17010 (20%)] Loss: 0.002028
Train Epoch: 16 [3520/17010 (21%)] Loss: 0.002770
Train Epoch: 16 [3680/17010 (22%)] Loss: 0.056217
Train Epoch: 16 [3840/17010 (23%)] Loss: 0.005135
Train Epoch: 16 [4000/17010 (24%)] Loss: 0.000505
Train Epoch: 16 [4160/17010 (24%)] Loss: 0.102067
Train Epoch: 16 [4320/17010 (25%)] Loss: 0.006056
Train Epoch: 16 [4480/17010 (26%)] Loss: 0.005491
Train Epoch: 16 [4640/17010 (27%)] Loss: 0.005490
Train Epoch: 16 [4800/17010 (28%)] Loss: 0.000632
Train Epoch: 16 [4960/17010 (29%)] Loss: 0.001214
Train Epoch: 16 [5120/17010 (30%)] Loss: 0.002160
Train Epoch: 16 [5280/17010 (31%)] Loss: 0.002287
Train Epoch: 16 [5440/17010 (32%)] Loss: 0.001463
Train Epoch: 16 [5600/17010 (33%)] Loss: 0.013611
Train Epoch: 16 [5760/17010 (34%)] Loss: 0.002268
Train Epoch: 16 [5920/17010 (35%)] Loss: 0.000980
Train Epoch: 16 [6080/17010 (36%)] Loss: 0.004568
Train Epoch: 16 [6240/17010 (37%)] Loss: 0.001074
Train Epoch: 16 [6400/17010 (38%)] Loss: 0.001325
Train Epoch: 16 [6560/17010 (39%)] Loss: 0.013537
Train Epoch: 16 [6720/17010 (40%)] Loss: 0.001936
Train Epoch: 16 [6880/17010 (40%)] Loss: 0.000544
Train Epoch: 16 [7040/17010 (41%)] Loss: 0.018853
Train Epoch: 16 [7200/17010 (42%)] Loss: 0.000419
Train Epoch: 16 [7360/17010 (43%)] Loss: 0.015234
Train Epoch: 16 [7520/17010 (44%)] Loss: 0.000096
Train Epoch: 16 [7680/17010 (45%)] Loss: 0.095947
Train Epoch: 16 [7840/17010 (46%)] Loss: 0.215221
Train Epoch: 16 [8000/17010 (47%)] Loss: 0.002992
Train Epoch: 16 [8160/17010 (48%)] Loss: 0.061351
Train Epoch: 16 [8320/17010 (49%)] Loss: 0.085023
Train Epoch: 16 [8480/17010 (50%)] Loss: 0.014769
Train Epoch: 16 [8640/17010 (51%)] Loss: 0.032976
Train Epoch: 16 [8800/17010 (52%)] Loss: 0.013599
Train Epoch: 16 [8960/17010 (53%)] Loss: 0.000261
Train Epoch: 16 [9120/17010 (54%)] Loss: 0.001941
Train Epoch: 16 [9280/17010 (55%)] Loss: 0.000534
Train Epoch: 16 [9440/17010 (55%)] Loss: 0.002654
Train Epoch: 16 [9600/17010 (56%)] Loss: 0.000315
Train Epoch: 16 [9760/17010 (57%)] Loss: 0.001919
Train Epoch: 16 [9920/17010 (58%)] Loss: 0.000379
Train Epoch: 16 [10080/17010 (59%)] Loss: 0.022307
Train Epoch: 16 [10240/17010 (60%)] Loss: 0.016105
Train Epoch: 16 [10400/17010 (61%)] Loss: 0.000616
Train Epoch: 16 [10560/17010 (62%)] Loss: 0.003345
Train Epoch: 16 [10720/17010 (63%)] Loss: 0.001426
Train Epoch: 16 [10880/17010 (64%)] Loss: 0.002824
Train Epoch: 16 [11040/17010 (65%)] Loss: 0.000219
Train Epoch: 16 [11200/17010 (66%)] Loss: 0.031342
Train Epoch: 16 [11360/17010 (67%)] Loss: 0.000118
Train Epoch: 16 [11520/17010 (68%)] Loss: 0.002525
Train Epoch: 16 [11680/17010 (69%)] Loss: 0.002624
Train Epoch: 16 [11840/17010 (70%)] Loss: 0.003396
Train Epoch: 16 [12000/17010 (71%)] Loss: 0.002928
Train Epoch: 16 [12160/17010 (71%)] Loss: 0.004921
Train Epoch: 16 [12320/17010 (72%)] Loss: 0.006281
Train Epoch: 16 [12480/17010 (73%)] Loss: 0.071156
Train Epoch: 16 [12640/17010 (74%)] Loss: 0.002467
Train Epoch: 16 [12800/17010 (75%)] Loss: 0.028732
Train Epoch: 16 [12960/17010 (76%)] Loss: 0.000216
Train Epoch: 16 [13120/17010 (77%)] Loss: 0.003268
Train Epoch: 16 [13280/17010 (78%)] Loss: 0.013005
Train Epoch: 16 [13440/17010 (79%)] Loss: 0.001383
Train Epoch: 16 [13600/17010 (80%)] Loss: 0.002217
Train Epoch: 16 [13760/17010 (81%)] Loss: 0.003618
Train Epoch: 16 [13920/17010 (82%)] Loss: 0.004185
Train Epoch: 16 [14080/17010 (83%)] Loss: 0.090693
Train Epoch: 16 [14240/17010 (84%)] Loss: 0.042185
Train Epoch: 16 [14400/17010 (85%)] Loss: 0.003692
Train Epoch: 16 [14560/17010 (86%)] Loss: 0.006910
Train Epoch: 16 [14720/17010 (87%)] Loss: 0.022257
Train Epoch: 16 [14880/17010 (87%)] Loss: 0.001390
Train Epoch: 16 [15040/17010 (88%)] Loss: 0.077979
Train Epoch: 16 [15200/17010 (89%)] Loss: 0.008571
Train Epoch: 16 [15360/17010 (90%)] Loss: 0.000221
Train Epoch: 16 [15520/17010 (91%)] Loss: 0.000162
Train Epoch: 16 [15680/17010 (92%)] Loss: 0.000614
Train Epoch: 16 [15840/17010 (93%)] Loss: 0.000755
Train Epoch: 16 [16000/17010 (94%)] Loss: 0.000584
Train Epoch: 16 [16160/17010 (95%)] Loss: 0.001047
Train Epoch: 16 [16320/17010 (96%)] Loss: 0.000374
Train Epoch: 16 [16480/17010 (97%)] Loss: 0.000497
Train Epoch: 16 [16640/17010 (98%)] Loss: 0.000498
Train Epoch: 16 [16800/17010 (99%)] Loss: 0.012323
Train Epoch: 16 [16960/17010 (100%)] Loss: 0.007473
    epoch          : 16
    Train_loss     : 0.013844192488318356
    Train_accuracy : 0.9961231203007519
    Train_f1_score : 0.9961231350898743
    Val_loss       : 0.03805185178953252
    Val_accuracy   : 0.9875
    Val_f1_score   : 0.9875000715255737
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch16.pth ...
Train Epoch: 17 [0/17010 (0%)] Loss: 0.000187
Train Epoch: 17 [160/17010 (1%)] Loss: 0.001122
Train Epoch: 17 [320/17010 (2%)] Loss: 0.000649
Train Epoch: 17 [480/17010 (3%)] Loss: 0.081613
Train Epoch: 17 [640/17010 (4%)] Loss: 0.001389
Train Epoch: 17 [800/17010 (5%)] Loss: 0.001283
Train Epoch: 17 [960/17010 (6%)] Loss: 0.001148
Train Epoch: 17 [1120/17010 (7%)] Loss: 0.000379
Train Epoch: 17 [1280/17010 (8%)] Loss: 0.001149
Train Epoch: 17 [1440/17010 (8%)] Loss: 0.000217
Train Epoch: 17 [1600/17010 (9%)] Loss: 0.000310
Train Epoch: 17 [1760/17010 (10%)] Loss: 0.001137
Train Epoch: 17 [1920/17010 (11%)] Loss: 0.004025
Train Epoch: 17 [2080/17010 (12%)] Loss: 0.000336
Train Epoch: 17 [2240/17010 (13%)] Loss: 0.000466
Train Epoch: 17 [2400/17010 (14%)] Loss: 0.003926
Train Epoch: 17 [2560/17010 (15%)] Loss: 0.296682
Train Epoch: 17 [2720/17010 (16%)] Loss: 0.080509
Train Epoch: 17 [2880/17010 (17%)] Loss: 0.010362
Train Epoch: 17 [3040/17010 (18%)] Loss: 0.005748
Train Epoch: 17 [3200/17010 (19%)] Loss: 0.000787
Train Epoch: 17 [3360/17010 (20%)] Loss: 0.004286
Train Epoch: 17 [3520/17010 (21%)] Loss: 0.000171
Train Epoch: 17 [3680/17010 (22%)] Loss: 0.001013
Train Epoch: 17 [3840/17010 (23%)] Loss: 0.012035
Train Epoch: 17 [4000/17010 (24%)] Loss: 0.011885
Train Epoch: 17 [4160/17010 (24%)] Loss: 0.003238
Train Epoch: 17 [4320/17010 (25%)] Loss: 0.000832
Train Epoch: 17 [4480/17010 (26%)] Loss: 0.035461
Train Epoch: 17 [4640/17010 (27%)] Loss: 0.002124
Train Epoch: 17 [4800/17010 (28%)] Loss: 0.000573
Train Epoch: 17 [4960/17010 (29%)] Loss: 0.010113
Train Epoch: 17 [5120/17010 (30%)] Loss: 0.002140
Train Epoch: 17 [5280/17010 (31%)] Loss: 0.017107
Train Epoch: 17 [5440/17010 (32%)] Loss: 0.000589
Train Epoch: 17 [5600/17010 (33%)] Loss: 0.004897
Train Epoch: 17 [5760/17010 (34%)] Loss: 0.000507
Train Epoch: 17 [5920/17010 (35%)] Loss: 0.001550
Train Epoch: 17 [6080/17010 (36%)] Loss: 0.000456
Train Epoch: 17 [6240/17010 (37%)] Loss: 0.026287
Train Epoch: 17 [6400/17010 (38%)] Loss: 0.000784
Train Epoch: 17 [6560/17010 (39%)] Loss: 0.027309
Train Epoch: 17 [6720/17010 (40%)] Loss: 0.001973
Train Epoch: 17 [6880/17010 (40%)] Loss: 0.000160
Train Epoch: 17 [7040/17010 (41%)] Loss: 0.001863
Train Epoch: 17 [7200/17010 (42%)] Loss: 0.024582
Train Epoch: 17 [7360/17010 (43%)] Loss: 0.000591
Train Epoch: 17 [7520/17010 (44%)] Loss: 0.001354
Train Epoch: 17 [7680/17010 (45%)] Loss: 0.000544
Train Epoch: 17 [7840/17010 (46%)] Loss: 0.000268
Train Epoch: 17 [8000/17010 (47%)] Loss: 0.000343
Train Epoch: 17 [8160/17010 (48%)] Loss: 0.015024
Train Epoch: 17 [8320/17010 (49%)] Loss: 0.000324
Train Epoch: 17 [8480/17010 (50%)] Loss: 0.001104
Train Epoch: 17 [8640/17010 (51%)] Loss: 0.000380
Train Epoch: 17 [8800/17010 (52%)] Loss: 0.012979
Train Epoch: 17 [8960/17010 (53%)] Loss: 0.004804
Train Epoch: 17 [9120/17010 (54%)] Loss: 0.005028
Train Epoch: 17 [9280/17010 (55%)] Loss: 0.000274
Train Epoch: 17 [9440/17010 (55%)] Loss: 0.001010
Train Epoch: 17 [9600/17010 (56%)] Loss: 0.000465
Train Epoch: 17 [9760/17010 (57%)] Loss: 0.045309
Train Epoch: 17 [9920/17010 (58%)] Loss: 0.000188
Train Epoch: 17 [10080/17010 (59%)] Loss: 0.001524
Train Epoch: 17 [10240/17010 (60%)] Loss: 0.000152
Train Epoch: 17 [10400/17010 (61%)] Loss: 0.000355
Train Epoch: 17 [10560/17010 (62%)] Loss: 0.013906
Train Epoch: 17 [10720/17010 (63%)] Loss: 0.001900
Train Epoch: 17 [10880/17010 (64%)] Loss: 0.003095
Train Epoch: 17 [11040/17010 (65%)] Loss: 0.008942
Train Epoch: 17 [11200/17010 (66%)] Loss: 0.006950
Train Epoch: 17 [11360/17010 (67%)] Loss: 0.000985
Train Epoch: 17 [11520/17010 (68%)] Loss: 0.000789
Train Epoch: 17 [11680/17010 (69%)] Loss: 0.001894
Train Epoch: 17 [11840/17010 (70%)] Loss: 0.006090
Train Epoch: 17 [12000/17010 (71%)] Loss: 0.020682
Train Epoch: 17 [12160/17010 (71%)] Loss: 0.000269
Train Epoch: 17 [12320/17010 (72%)] Loss: 0.000444
Train Epoch: 17 [12480/17010 (73%)] Loss: 0.001608
Train Epoch: 17 [12640/17010 (74%)] Loss: 0.000037
Train Epoch: 17 [12800/17010 (75%)] Loss: 0.000882
Train Epoch: 17 [12960/17010 (76%)] Loss: 0.004774
Train Epoch: 17 [13120/17010 (77%)] Loss: 0.000306
Train Epoch: 17 [13280/17010 (78%)] Loss: 0.000970
Train Epoch: 17 [13440/17010 (79%)] Loss: 0.000112
Train Epoch: 17 [13600/17010 (80%)] Loss: 0.002050
Train Epoch: 17 [13760/17010 (81%)] Loss: 0.002671
Train Epoch: 17 [13920/17010 (82%)] Loss: 0.000540
Train Epoch: 17 [14080/17010 (83%)] Loss: 0.000202
Train Epoch: 17 [14240/17010 (84%)] Loss: 0.000399
Train Epoch: 17 [14400/17010 (85%)] Loss: 0.036096
Train Epoch: 17 [14560/17010 (86%)] Loss: 0.001263
Train Epoch: 17 [14720/17010 (87%)] Loss: 0.000237
Train Epoch: 17 [14880/17010 (87%)] Loss: 0.000367
Train Epoch: 17 [15040/17010 (88%)] Loss: 0.000083
Train Epoch: 17 [15200/17010 (89%)] Loss: 0.001135
Train Epoch: 17 [15360/17010 (90%)] Loss: 0.002252
Train Epoch: 17 [15520/17010 (91%)] Loss: 0.000111
Train Epoch: 17 [15680/17010 (92%)] Loss: 0.000618
Train Epoch: 17 [15840/17010 (93%)] Loss: 0.000291
Train Epoch: 17 [16000/17010 (94%)] Loss: 0.001231
Train Epoch: 17 [16160/17010 (95%)] Loss: 0.000397
Train Epoch: 17 [16320/17010 (96%)] Loss: 0.070586
Train Epoch: 17 [16480/17010 (97%)] Loss: 0.096627
Train Epoch: 17 [16640/17010 (98%)] Loss: 0.000396
Train Epoch: 17 [16800/17010 (99%)] Loss: 0.000529
Train Epoch: 17 [16960/17010 (100%)] Loss: 0.007747
    epoch          : 17
    Train_loss     : 0.00801879492896694
    Train_accuracy : 0.9972391917293233
    Train_f1_score : 0.9972392320632935
    Val_loss       : 0.039473609511151156
    Val_accuracy   : 0.990625
    Val_f1_score   : 0.9906250238418579
Train Epoch: 18 [0/17010 (0%)] Loss: 0.013674
Train Epoch: 18 [160/17010 (1%)] Loss: 0.001305
Train Epoch: 18 [320/17010 (2%)] Loss: 0.002775
Train Epoch: 18 [480/17010 (3%)] Loss: 0.001075
Train Epoch: 18 [640/17010 (4%)] Loss: 0.000486
Train Epoch: 18 [800/17010 (5%)] Loss: 0.000716
Train Epoch: 18 [960/17010 (6%)] Loss: 0.002019
Train Epoch: 18 [1120/17010 (7%)] Loss: 0.001954
Train Epoch: 18 [1280/17010 (8%)] Loss: 0.000500
Train Epoch: 18 [1440/17010 (8%)] Loss: 0.000183
Train Epoch: 18 [1600/17010 (9%)] Loss: 0.000550
Train Epoch: 18 [1760/17010 (10%)] Loss: 0.000211
Train Epoch: 18 [1920/17010 (11%)] Loss: 0.000094
Train Epoch: 18 [2080/17010 (12%)] Loss: 0.000203
Train Epoch: 18 [2240/17010 (13%)] Loss: 0.001952
Train Epoch: 18 [2400/17010 (14%)] Loss: 0.001558
Train Epoch: 18 [2560/17010 (15%)] Loss: 0.000065
Train Epoch: 18 [2720/17010 (16%)] Loss: 0.000136
Train Epoch: 18 [2880/17010 (17%)] Loss: 0.000366
Train Epoch: 18 [3040/17010 (18%)] Loss: 0.000394
Train Epoch: 18 [3200/17010 (19%)] Loss: 0.002798
Train Epoch: 18 [3360/17010 (20%)] Loss: 0.000642
Train Epoch: 18 [3520/17010 (21%)] Loss: 0.002007
Train Epoch: 18 [3680/17010 (22%)] Loss: 0.000189
Train Epoch: 18 [3840/17010 (23%)] Loss: 0.001570
Train Epoch: 18 [4000/17010 (24%)] Loss: 0.021157
Train Epoch: 18 [4160/17010 (24%)] Loss: 0.000751
Train Epoch: 18 [4320/17010 (25%)] Loss: 0.001257
Train Epoch: 18 [4480/17010 (26%)] Loss: 0.008544
Train Epoch: 18 [4640/17010 (27%)] Loss: 0.000943
Train Epoch: 18 [4800/17010 (28%)] Loss: 0.004442
Train Epoch: 18 [4960/17010 (29%)] Loss: 0.008654
Train Epoch: 18 [5120/17010 (30%)] Loss: 0.000308
Train Epoch: 18 [5280/17010 (31%)] Loss: 0.000368
Train Epoch: 18 [5440/17010 (32%)] Loss: 0.002650
Train Epoch: 18 [5600/17010 (33%)] Loss: 0.000589
Train Epoch: 18 [5760/17010 (34%)] Loss: 0.173374
Train Epoch: 18 [5920/17010 (35%)] Loss: 0.034508
Train Epoch: 18 [6080/17010 (36%)] Loss: 0.000468
Train Epoch: 18 [6240/17010 (37%)] Loss: 0.003020
Train Epoch: 18 [6400/17010 (38%)] Loss: 0.002129
Train Epoch: 18 [6560/17010 (39%)] Loss: 0.001539
Train Epoch: 18 [6720/17010 (40%)] Loss: 0.000548
Train Epoch: 18 [6880/17010 (40%)] Loss: 0.000655
Train Epoch: 18 [7040/17010 (41%)] Loss: 0.000238
Train Epoch: 18 [7200/17010 (42%)] Loss: 0.000113
Train Epoch: 18 [7360/17010 (43%)] Loss: 0.000250
Train Epoch: 18 [7520/17010 (44%)] Loss: 0.000785
Train Epoch: 18 [7680/17010 (45%)] Loss: 0.007516
Train Epoch: 18 [7840/17010 (46%)] Loss: 0.004170
Train Epoch: 18 [8000/17010 (47%)] Loss: 0.000442
Train Epoch: 18 [8160/17010 (48%)] Loss: 0.001951
Train Epoch: 18 [8320/17010 (49%)] Loss: 0.001744
Train Epoch: 18 [8480/17010 (50%)] Loss: 0.004888
Train Epoch: 18 [8640/17010 (51%)] Loss: 0.000203
Train Epoch: 18 [8800/17010 (52%)] Loss: 0.000481
Train Epoch: 18 [8960/17010 (53%)] Loss: 0.000776
Train Epoch: 18 [9120/17010 (54%)] Loss: 0.249347
Train Epoch: 18 [9280/17010 (55%)] Loss: 0.000405
Train Epoch: 18 [9440/17010 (55%)] Loss: 0.003076
Train Epoch: 18 [9600/17010 (56%)] Loss: 0.006133
Train Epoch: 18 [9760/17010 (57%)] Loss: 0.000263
Train Epoch: 18 [9920/17010 (58%)] Loss: 0.055844
Train Epoch: 18 [10080/17010 (59%)] Loss: 0.000204
Train Epoch: 18 [10240/17010 (60%)] Loss: 0.402196
Train Epoch: 18 [10400/17010 (61%)] Loss: 0.009504
Train Epoch: 18 [10560/17010 (62%)] Loss: 0.006480
Train Epoch: 18 [10720/17010 (63%)] Loss: 0.000741
Train Epoch: 18 [10880/17010 (64%)] Loss: 0.056483
Train Epoch: 18 [11040/17010 (65%)] Loss: 0.001426
Train Epoch: 18 [11200/17010 (66%)] Loss: 0.001623
Train Epoch: 18 [11360/17010 (67%)] Loss: 0.002313
Train Epoch: 18 [11520/17010 (68%)] Loss: 0.013295
Train Epoch: 18 [11680/17010 (69%)] Loss: 0.003242
Train Epoch: 18 [11840/17010 (70%)] Loss: 0.005101
Train Epoch: 18 [12000/17010 (71%)] Loss: 0.000363
Train Epoch: 18 [12160/17010 (71%)] Loss: 0.002162
Train Epoch: 18 [12320/17010 (72%)] Loss: 0.000579
Train Epoch: 18 [12480/17010 (73%)] Loss: 0.005259
Train Epoch: 18 [12640/17010 (74%)] Loss: 0.005181
Train Epoch: 18 [12800/17010 (75%)] Loss: 0.002415
Train Epoch: 18 [12960/17010 (76%)] Loss: 0.008121
Train Epoch: 18 [13120/17010 (77%)] Loss: 0.010136
Train Epoch: 18 [13280/17010 (78%)] Loss: 0.000501
Train Epoch: 18 [13440/17010 (79%)] Loss: 0.002182
Train Epoch: 18 [13600/17010 (80%)] Loss: 0.001647
Train Epoch: 18 [13760/17010 (81%)] Loss: 0.002561
Train Epoch: 18 [13920/17010 (82%)] Loss: 0.016357
Train Epoch: 18 [14080/17010 (83%)] Loss: 0.006353
Train Epoch: 18 [14240/17010 (84%)] Loss: 0.010315
Train Epoch: 18 [14400/17010 (85%)] Loss: 0.008983
Train Epoch: 18 [14560/17010 (86%)] Loss: 0.039266
Train Epoch: 18 [14720/17010 (87%)] Loss: 0.001099
Train Epoch: 18 [14880/17010 (87%)] Loss: 0.008313
Train Epoch: 18 [15040/17010 (88%)] Loss: 0.000689
Train Epoch: 18 [15200/17010 (89%)] Loss: 0.002274
Train Epoch: 18 [15360/17010 (90%)] Loss: 0.000281
Train Epoch: 18 [15520/17010 (91%)] Loss: 0.015490
Train Epoch: 18 [15680/17010 (92%)] Loss: 0.001032
Train Epoch: 18 [15840/17010 (93%)] Loss: 0.004785
Train Epoch: 18 [16000/17010 (94%)] Loss: 0.000393
Train Epoch: 18 [16160/17010 (95%)] Loss: 0.001623
Train Epoch: 18 [16320/17010 (96%)] Loss: 0.001132
Train Epoch: 18 [16480/17010 (97%)] Loss: 0.000777
Train Epoch: 18 [16640/17010 (98%)] Loss: 0.015369
Train Epoch: 18 [16800/17010 (99%)] Loss: 0.011036
Train Epoch: 18 [16960/17010 (100%)] Loss: 0.004989
    epoch          : 18
    Train_loss     : 0.014921551747456283
    Train_accuracy : 0.9959468984962406
    Train_f1_score : 0.9959469437599182
    Val_loss       : 0.049083939936826936
    Val_accuracy   : 0.9833333333333333
    Val_f1_score   : 0.98333340883255
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch18.pth ...
Train Epoch: 19 [0/17010 (0%)] Loss: 0.000922
Train Epoch: 19 [160/17010 (1%)] Loss: 0.016869
Train Epoch: 19 [320/17010 (2%)] Loss: 0.263929
Train Epoch: 19 [480/17010 (3%)] Loss: 0.002014
Train Epoch: 19 [640/17010 (4%)] Loss: 0.001007
Train Epoch: 19 [800/17010 (5%)] Loss: 0.066393
Train Epoch: 19 [960/17010 (6%)] Loss: 0.038236
Train Epoch: 19 [1120/17010 (7%)] Loss: 0.000488
Train Epoch: 19 [1280/17010 (8%)] Loss: 0.001673
Train Epoch: 19 [1440/17010 (8%)] Loss: 0.003158
Train Epoch: 19 [1600/17010 (9%)] Loss: 0.000331
Train Epoch: 19 [1760/17010 (10%)] Loss: 0.016386
Train Epoch: 19 [1920/17010 (11%)] Loss: 0.001984
Train Epoch: 19 [2080/17010 (12%)] Loss: 0.011020
Train Epoch: 19 [2240/17010 (13%)] Loss: 0.003798
Train Epoch: 19 [2400/17010 (14%)] Loss: 0.000466
Train Epoch: 19 [2560/17010 (15%)] Loss: 0.003152
Train Epoch: 19 [2720/17010 (16%)] Loss: 0.001201
Train Epoch: 19 [2880/17010 (17%)] Loss: 0.000362
Train Epoch: 19 [3040/17010 (18%)] Loss: 0.000156
Train Epoch: 19 [3200/17010 (19%)] Loss: 0.000921
Train Epoch: 19 [3360/17010 (20%)] Loss: 0.000152
Train Epoch: 19 [3520/17010 (21%)] Loss: 0.001201
Train Epoch: 19 [3680/17010 (22%)] Loss: 0.000554
Train Epoch: 19 [3840/17010 (23%)] Loss: 0.001143
Train Epoch: 19 [4000/17010 (24%)] Loss: 0.000660
Train Epoch: 19 [4160/17010 (24%)] Loss: 0.000995
Train Epoch: 19 [4320/17010 (25%)] Loss: 0.000456
Train Epoch: 19 [4480/17010 (26%)] Loss: 0.053305
Train Epoch: 19 [4640/17010 (27%)] Loss: 0.004353
Train Epoch: 19 [4800/17010 (28%)] Loss: 0.003540
Train Epoch: 19 [4960/17010 (29%)] Loss: 0.061930
Train Epoch: 19 [5120/17010 (30%)] Loss: 0.001564
Train Epoch: 19 [5280/17010 (31%)] Loss: 0.014573
Train Epoch: 19 [5440/17010 (32%)] Loss: 0.002505
Train Epoch: 19 [5600/17010 (33%)] Loss: 0.000494
Train Epoch: 19 [5760/17010 (34%)] Loss: 0.011944
Train Epoch: 19 [5920/17010 (35%)] Loss: 0.000632
Train Epoch: 19 [6080/17010 (36%)] Loss: 0.009705
Train Epoch: 19 [6240/17010 (37%)] Loss: 0.025699
Train Epoch: 19 [6400/17010 (38%)] Loss: 0.003546
Train Epoch: 19 [6560/17010 (39%)] Loss: 0.000904
Train Epoch: 19 [6720/17010 (40%)] Loss: 0.000607
Train Epoch: 19 [6880/17010 (40%)] Loss: 0.004182
Train Epoch: 19 [7040/17010 (41%)] Loss: 0.000619
Train Epoch: 19 [7200/17010 (42%)] Loss: 0.000270
Train Epoch: 19 [7360/17010 (43%)] Loss: 0.001617
Train Epoch: 19 [7520/17010 (44%)] Loss: 0.000151
Train Epoch: 19 [7680/17010 (45%)] Loss: 0.015904
Train Epoch: 19 [7840/17010 (46%)] Loss: 0.000110
Train Epoch: 19 [8000/17010 (47%)] Loss: 0.038201
Train Epoch: 19 [8160/17010 (48%)] Loss: 0.000267
Train Epoch: 19 [8320/17010 (49%)] Loss: 0.023261
Train Epoch: 19 [8480/17010 (50%)] Loss: 0.000135
Train Epoch: 19 [8640/17010 (51%)] Loss: 0.000180
Train Epoch: 19 [8800/17010 (52%)] Loss: 0.000279
Train Epoch: 19 [8960/17010 (53%)] Loss: 0.000214
Train Epoch: 19 [9120/17010 (54%)] Loss: 0.000224
Train Epoch: 19 [9280/17010 (55%)] Loss: 0.001029
Train Epoch: 19 [9440/17010 (55%)] Loss: 0.002326
Train Epoch: 19 [9600/17010 (56%)] Loss: 0.000422
Train Epoch: 19 [9760/17010 (57%)] Loss: 0.001660
Train Epoch: 19 [9920/17010 (58%)] Loss: 0.000185
Train Epoch: 19 [10080/17010 (59%)] Loss: 0.010073
Train Epoch: 19 [10240/17010 (60%)] Loss: 0.000119
Train Epoch: 19 [10400/17010 (61%)] Loss: 0.002015
Train Epoch: 19 [10560/17010 (62%)] Loss: 0.000166
Train Epoch: 19 [10720/17010 (63%)] Loss: 0.000250
Train Epoch: 19 [10880/17010 (64%)] Loss: 0.000406
Train Epoch: 19 [11040/17010 (65%)] Loss: 0.000102
Train Epoch: 19 [11200/17010 (66%)] Loss: 0.000226
Train Epoch: 19 [11360/17010 (67%)] Loss: 0.000408
Train Epoch: 19 [11520/17010 (68%)] Loss: 0.000958
Train Epoch: 19 [11680/17010 (69%)] Loss: 0.000492
Train Epoch: 19 [11840/17010 (70%)] Loss: 0.000325
Train Epoch: 19 [12000/17010 (71%)] Loss: 0.000250
Train Epoch: 19 [12160/17010 (71%)] Loss: 0.000502
Train Epoch: 19 [12320/17010 (72%)] Loss: 0.004116
Train Epoch: 19 [12480/17010 (73%)] Loss: 0.000365
Train Epoch: 19 [12640/17010 (74%)] Loss: 0.000340
Train Epoch: 19 [12800/17010 (75%)] Loss: 0.000991
Train Epoch: 19 [12960/17010 (76%)] Loss: 0.000742
Train Epoch: 19 [13120/17010 (77%)] Loss: 0.001601
Train Epoch: 19 [13280/17010 (78%)] Loss: 0.000621
Train Epoch: 19 [13440/17010 (79%)] Loss: 0.048929
Train Epoch: 19 [13600/17010 (80%)] Loss: 0.000261
Train Epoch: 19 [13760/17010 (81%)] Loss: 0.029728
Train Epoch: 19 [13920/17010 (82%)] Loss: 0.018719
Train Epoch: 19 [14080/17010 (83%)] Loss: 0.000541
Train Epoch: 19 [14240/17010 (84%)] Loss: 0.001632
Train Epoch: 19 [14400/17010 (85%)] Loss: 0.001658
Train Epoch: 19 [14560/17010 (86%)] Loss: 0.005710
Train Epoch: 19 [14720/17010 (87%)] Loss: 0.000444
Train Epoch: 19 [14880/17010 (87%)] Loss: 0.001337
Train Epoch: 19 [15040/17010 (88%)] Loss: 0.003667
Train Epoch: 19 [15200/17010 (89%)] Loss: 0.015341
Train Epoch: 19 [15360/17010 (90%)] Loss: 0.005464
Train Epoch: 19 [15520/17010 (91%)] Loss: 0.000152
Train Epoch: 19 [15680/17010 (92%)] Loss: 0.002307
Train Epoch: 19 [15840/17010 (93%)] Loss: 0.006197
Train Epoch: 19 [16000/17010 (94%)] Loss: 0.000679
Train Epoch: 19 [16160/17010 (95%)] Loss: 0.000536
Train Epoch: 19 [16320/17010 (96%)] Loss: 0.009563
Train Epoch: 19 [16480/17010 (97%)] Loss: 0.000324
Train Epoch: 19 [16640/17010 (98%)] Loss: 0.000373
Train Epoch: 19 [16800/17010 (99%)] Loss: 0.020055
Train Epoch: 19 [16960/17010 (100%)] Loss: 0.000571
    epoch          : 19
    Train_loss     : 0.007487943301754072
    Train_accuracy : 0.9973566729323309
    Train_f1_score : 0.9973567128181458
    Val_loss       : 0.022212664133257932
    Val_accuracy   : 0.9942708333333333
    Val_f1_score   : 0.9942708611488342
Train Epoch: 20 [0/17010 (0%)] Loss: 0.042197
Train Epoch: 20 [160/17010 (1%)] Loss: 0.010801
Train Epoch: 20 [320/17010 (2%)] Loss: 0.000073
Train Epoch: 20 [480/17010 (3%)] Loss: 0.040411
Train Epoch: 20 [640/17010 (4%)] Loss: 0.001303
Train Epoch: 20 [800/17010 (5%)] Loss: 0.000055
Train Epoch: 20 [960/17010 (6%)] Loss: 0.000563
Train Epoch: 20 [1120/17010 (7%)] Loss: 0.000111
Train Epoch: 20 [1280/17010 (8%)] Loss: 0.000432
Train Epoch: 20 [1440/17010 (8%)] Loss: 0.000129
Train Epoch: 20 [1600/17010 (9%)] Loss: 0.001494
Train Epoch: 20 [1760/17010 (10%)] Loss: 0.001563
Train Epoch: 20 [1920/17010 (11%)] Loss: 0.000197
Train Epoch: 20 [2080/17010 (12%)] Loss: 0.000341
Train Epoch: 20 [2240/17010 (13%)] Loss: 0.000542
Train Epoch: 20 [2400/17010 (14%)] Loss: 0.000416
Train Epoch: 20 [2560/17010 (15%)] Loss: 0.000317
Train Epoch: 20 [2720/17010 (16%)] Loss: 0.000258
Train Epoch: 20 [2880/17010 (17%)] Loss: 0.000752
Train Epoch: 20 [3040/17010 (18%)] Loss: 0.000282
Train Epoch: 20 [3200/17010 (19%)] Loss: 0.003337
Train Epoch: 20 [3360/17010 (20%)] Loss: 0.000227
Train Epoch: 20 [3520/17010 (21%)] Loss: 0.000268
Train Epoch: 20 [3680/17010 (22%)] Loss: 0.000323
Train Epoch: 20 [3840/17010 (23%)] Loss: 0.000103
Train Epoch: 20 [4000/17010 (24%)] Loss: 0.000088
Train Epoch: 20 [4160/17010 (24%)] Loss: 0.000864
Train Epoch: 20 [4320/17010 (25%)] Loss: 0.000869
Train Epoch: 20 [4480/17010 (26%)] Loss: 0.000042
Train Epoch: 20 [4640/17010 (27%)] Loss: 0.000414
Train Epoch: 20 [4800/17010 (28%)] Loss: 0.000524
Train Epoch: 20 [4960/17010 (29%)] Loss: 0.003353
Train Epoch: 20 [5120/17010 (30%)] Loss: 0.000235
Train Epoch: 20 [5280/17010 (31%)] Loss: 0.000526
Train Epoch: 20 [5440/17010 (32%)] Loss: 0.020763
Train Epoch: 20 [5600/17010 (33%)] Loss: 0.000284
Train Epoch: 20 [5760/17010 (34%)] Loss: 0.000947
Train Epoch: 20 [5920/17010 (35%)] Loss: 0.000080
Train Epoch: 20 [6080/17010 (36%)] Loss: 0.004902
Train Epoch: 20 [6240/17010 (37%)] Loss: 0.000556
Train Epoch: 20 [6400/17010 (38%)] Loss: 0.000308
Train Epoch: 20 [6560/17010 (39%)] Loss: 0.003348
Train Epoch: 20 [6720/17010 (40%)] Loss: 0.000385
Train Epoch: 20 [6880/17010 (40%)] Loss: 0.000085
Train Epoch: 20 [7040/17010 (41%)] Loss: 0.000166
Train Epoch: 20 [7200/17010 (42%)] Loss: 0.000252
Train Epoch: 20 [7360/17010 (43%)] Loss: 0.000121
Train Epoch: 20 [7520/17010 (44%)] Loss: 0.032294
Train Epoch: 20 [7680/17010 (45%)] Loss: 0.000981
Train Epoch: 20 [7840/17010 (46%)] Loss: 0.000603
Train Epoch: 20 [8000/17010 (47%)] Loss: 0.000080
Train Epoch: 20 [8160/17010 (48%)] Loss: 0.000240
Train Epoch: 20 [8320/17010 (49%)] Loss: 0.000128
Train Epoch: 20 [8480/17010 (50%)] Loss: 0.212648
Train Epoch: 20 [8640/17010 (51%)] Loss: 0.025767
Train Epoch: 20 [8800/17010 (52%)] Loss: 0.004217
Train Epoch: 20 [8960/17010 (53%)] Loss: 0.000743
Train Epoch: 20 [9120/17010 (54%)] Loss: 0.000070
Train Epoch: 20 [9280/17010 (55%)] Loss: 0.000116
Train Epoch: 20 [9440/17010 (55%)] Loss: 0.000131
Train Epoch: 20 [9600/17010 (56%)] Loss: 0.001339
Train Epoch: 20 [9760/17010 (57%)] Loss: 0.001984
Train Epoch: 20 [9920/17010 (58%)] Loss: 0.007798
Train Epoch: 20 [10080/17010 (59%)] Loss: 0.003331
Train Epoch: 20 [10240/17010 (60%)] Loss: 0.000390
Train Epoch: 20 [10400/17010 (61%)] Loss: 0.000346
Train Epoch: 20 [10560/17010 (62%)] Loss: 0.000491
Train Epoch: 20 [10720/17010 (63%)] Loss: 0.000273
Train Epoch: 20 [10880/17010 (64%)] Loss: 0.000113
Train Epoch: 20 [11040/17010 (65%)] Loss: 0.000416
Train Epoch: 20 [11200/17010 (66%)] Loss: 0.001241
Train Epoch: 20 [11360/17010 (67%)] Loss: 0.000063
Train Epoch: 20 [11520/17010 (68%)] Loss: 0.000157
Train Epoch: 20 [11680/17010 (69%)] Loss: 0.000123
Train Epoch: 20 [11840/17010 (70%)] Loss: 0.000387
Train Epoch: 20 [12000/17010 (71%)] Loss: 0.000639
Train Epoch: 20 [12160/17010 (71%)] Loss: 0.017215
Train Epoch: 20 [12320/17010 (72%)] Loss: 0.051159
Train Epoch: 20 [12480/17010 (73%)] Loss: 0.000138
Train Epoch: 20 [12640/17010 (74%)] Loss: 0.000070
Train Epoch: 20 [12800/17010 (75%)] Loss: 0.000537
Train Epoch: 20 [12960/17010 (76%)] Loss: 0.002044
Train Epoch: 20 [13120/17010 (77%)] Loss: 0.000283
Train Epoch: 20 [13280/17010 (78%)] Loss: 0.000097
Train Epoch: 20 [13440/17010 (79%)] Loss: 0.050659
Train Epoch: 20 [13600/17010 (80%)] Loss: 0.000861
Train Epoch: 20 [13760/17010 (81%)] Loss: 0.001447
Train Epoch: 20 [13920/17010 (82%)] Loss: 0.066916
Train Epoch: 20 [14080/17010 (83%)] Loss: 0.000425
Train Epoch: 20 [14240/17010 (84%)] Loss: 0.001360
Train Epoch: 20 [14400/17010 (85%)] Loss: 0.004185
Train Epoch: 20 [14560/17010 (86%)] Loss: 0.034393
Train Epoch: 20 [14720/17010 (87%)] Loss: 0.002429
Train Epoch: 20 [14880/17010 (87%)] Loss: 0.000271
Train Epoch: 20 [15040/17010 (88%)] Loss: 0.000406
Train Epoch: 20 [15200/17010 (89%)] Loss: 0.000274
Train Epoch: 20 [15360/17010 (90%)] Loss: 0.000161
Train Epoch: 20 [15520/17010 (91%)] Loss: 0.000132
Train Epoch: 20 [15680/17010 (92%)] Loss: 0.001172
Train Epoch: 20 [15840/17010 (93%)] Loss: 0.000363
Train Epoch: 20 [16000/17010 (94%)] Loss: 0.000093
Train Epoch: 20 [16160/17010 (95%)] Loss: 0.000088
Train Epoch: 20 [16320/17010 (96%)] Loss: 0.000556
Train Epoch: 20 [16480/17010 (97%)] Loss: 0.000107
Train Epoch: 20 [16640/17010 (98%)] Loss: 0.000272
Train Epoch: 20 [16800/17010 (99%)] Loss: 0.051334
Train Epoch: 20 [16960/17010 (100%)] Loss: 0.000065
    epoch          : 20
    Train_loss     : 0.00301098371776727
    Train_accuracy : 0.9990014097744361
    Train_f1_score : 0.9990014433860779
    Val_loss       : 0.015359982172882761
    Val_accuracy   : 0.9963541666666667
    Val_f1_score   : 0.9963542222976685
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch20.pth ...
Train Epoch: 21 [0/17010 (0%)] Loss: 0.037252
Train Epoch: 21 [160/17010 (1%)] Loss: 0.000620
Train Epoch: 21 [320/17010 (2%)] Loss: 0.091617
Train Epoch: 21 [480/17010 (3%)] Loss: 0.000107
Train Epoch: 21 [640/17010 (4%)] Loss: 0.000284
Train Epoch: 21 [800/17010 (5%)] Loss: 0.000304
Train Epoch: 21 [960/17010 (6%)] Loss: 0.000264
Train Epoch: 21 [1120/17010 (7%)] Loss: 0.000059
Train Epoch: 21 [1280/17010 (8%)] Loss: 0.000553
Train Epoch: 21 [1440/17010 (8%)] Loss: 0.000129
Train Epoch: 21 [1600/17010 (9%)] Loss: 0.000047
Train Epoch: 21 [1760/17010 (10%)] Loss: 0.000070
Train Epoch: 21 [1920/17010 (11%)] Loss: 0.000354
Train Epoch: 21 [2080/17010 (12%)] Loss: 0.000455
Train Epoch: 21 [2240/17010 (13%)] Loss: 0.000324
Train Epoch: 21 [2400/17010 (14%)] Loss: 0.002433
Train Epoch: 21 [2560/17010 (15%)] Loss: 0.000678
Train Epoch: 21 [2720/17010 (16%)] Loss: 0.000530
Train Epoch: 21 [2880/17010 (17%)] Loss: 0.000111
Train Epoch: 21 [3040/17010 (18%)] Loss: 0.000053
Train Epoch: 21 [3200/17010 (19%)] Loss: 0.000580
Train Epoch: 21 [3360/17010 (20%)] Loss: 0.000797
Train Epoch: 21 [3520/17010 (21%)] Loss: 0.001538
Train Epoch: 21 [3680/17010 (22%)] Loss: 0.000176
Train Epoch: 21 [3840/17010 (23%)] Loss: 0.000080
Train Epoch: 21 [4000/17010 (24%)] Loss: 0.000180
Train Epoch: 21 [4160/17010 (24%)] Loss: 0.000122
Train Epoch: 21 [4320/17010 (25%)] Loss: 0.000199
Train Epoch: 21 [4480/17010 (26%)] Loss: 0.000169
Train Epoch: 21 [4640/17010 (27%)] Loss: 0.000171
Train Epoch: 21 [4800/17010 (28%)] Loss: 0.000065
Train Epoch: 21 [4960/17010 (29%)] Loss: 0.000412
Train Epoch: 21 [5120/17010 (30%)] Loss: 0.000122
Train Epoch: 21 [5280/17010 (31%)] Loss: 0.000147
Train Epoch: 21 [5440/17010 (32%)] Loss: 0.000057
Train Epoch: 21 [5600/17010 (33%)] Loss: 0.001181
Train Epoch: 21 [5760/17010 (34%)] Loss: 0.000234
Train Epoch: 21 [5920/17010 (35%)] Loss: 0.000422
Train Epoch: 21 [6080/17010 (36%)] Loss: 0.005346
Train Epoch: 21 [6240/17010 (37%)] Loss: 0.001109
Train Epoch: 21 [6400/17010 (38%)] Loss: 0.120295
Train Epoch: 21 [6560/17010 (39%)] Loss: 0.000072
Train Epoch: 21 [6720/17010 (40%)] Loss: 0.007998
Train Epoch: 21 [6880/17010 (40%)] Loss: 0.000393
Train Epoch: 21 [7040/17010 (41%)] Loss: 0.000158
Train Epoch: 21 [7200/17010 (42%)] Loss: 0.000059
Train Epoch: 21 [7360/17010 (43%)] Loss: 0.000128
Train Epoch: 21 [7520/17010 (44%)] Loss: 0.000067
Train Epoch: 21 [7680/17010 (45%)] Loss: 0.000437
Train Epoch: 21 [7840/17010 (46%)] Loss: 0.000027
Train Epoch: 21 [8000/17010 (47%)] Loss: 0.000073
Train Epoch: 21 [8160/17010 (48%)] Loss: 0.000360
Train Epoch: 21 [8320/17010 (49%)] Loss: 0.000137
Train Epoch: 21 [8480/17010 (50%)] Loss: 0.000106
Train Epoch: 21 [8640/17010 (51%)] Loss: 0.000305
Train Epoch: 21 [8800/17010 (52%)] Loss: 0.003107
Train Epoch: 21 [8960/17010 (53%)] Loss: 0.000511
Train Epoch: 21 [9120/17010 (54%)] Loss: 0.008235
Train Epoch: 21 [9280/17010 (55%)] Loss: 0.000169
Train Epoch: 21 [9440/17010 (55%)] Loss: 0.001208
Train Epoch: 21 [9600/17010 (56%)] Loss: 0.000577
Train Epoch: 21 [9760/17010 (57%)] Loss: 0.000670
Train Epoch: 21 [9920/17010 (58%)] Loss: 0.000164
Train Epoch: 21 [10080/17010 (59%)] Loss: 0.000085
Train Epoch: 21 [10240/17010 (60%)] Loss: 0.000156
Train Epoch: 21 [10400/17010 (61%)] Loss: 0.059913
Train Epoch: 21 [10560/17010 (62%)] Loss: 0.000218
Train Epoch: 21 [10720/17010 (63%)] Loss: 0.002508
Train Epoch: 21 [10880/17010 (64%)] Loss: 0.004902
Train Epoch: 21 [11040/17010 (65%)] Loss: 0.000367
Train Epoch: 21 [11200/17010 (66%)] Loss: 0.001431
Train Epoch: 21 [11360/17010 (67%)] Loss: 0.003562
Train Epoch: 21 [11520/17010 (68%)] Loss: 0.003297
Train Epoch: 21 [11680/17010 (69%)] Loss: 0.000186
Train Epoch: 21 [11840/17010 (70%)] Loss: 0.001586
Train Epoch: 21 [12000/17010 (71%)] Loss: 0.000654
Train Epoch: 21 [12160/17010 (71%)] Loss: 0.000232
Train Epoch: 21 [12320/17010 (72%)] Loss: 0.000462
Train Epoch: 21 [12480/17010 (73%)] Loss: 0.000335
Train Epoch: 21 [12640/17010 (74%)] Loss: 0.000095
Train Epoch: 21 [12800/17010 (75%)] Loss: 0.000384
Train Epoch: 21 [12960/17010 (76%)] Loss: 0.000173
Train Epoch: 21 [13120/17010 (77%)] Loss: 0.000120
Train Epoch: 21 [13280/17010 (78%)] Loss: 0.002803
Train Epoch: 21 [13440/17010 (79%)] Loss: 0.001594
Train Epoch: 21 [13600/17010 (80%)] Loss: 0.001396
Train Epoch: 21 [13760/17010 (81%)] Loss: 0.107229
Train Epoch: 21 [13920/17010 (82%)] Loss: 0.000201
Train Epoch: 21 [14080/17010 (83%)] Loss: 0.000097
Train Epoch: 21 [14240/17010 (84%)] Loss: 0.001054
Train Epoch: 21 [14400/17010 (85%)] Loss: 0.001405
Train Epoch: 21 [14560/17010 (86%)] Loss: 0.000387
Train Epoch: 21 [14720/17010 (87%)] Loss: 0.000073
Train Epoch: 21 [14880/17010 (87%)] Loss: 0.001987
Train Epoch: 21 [15040/17010 (88%)] Loss: 0.000169
Train Epoch: 21 [15200/17010 (89%)] Loss: 0.001139
Train Epoch: 21 [15360/17010 (90%)] Loss: 0.000712
Train Epoch: 21 [15520/17010 (91%)] Loss: 0.000325
Train Epoch: 21 [15680/17010 (92%)] Loss: 0.000190
Train Epoch: 21 [15840/17010 (93%)] Loss: 0.005786
Train Epoch: 21 [16000/17010 (94%)] Loss: 0.000780
Train Epoch: 21 [16160/17010 (95%)] Loss: 0.000154
Train Epoch: 21 [16320/17010 (96%)] Loss: 0.000100
Train Epoch: 21 [16480/17010 (97%)] Loss: 0.000223
Train Epoch: 21 [16640/17010 (98%)] Loss: 0.120843
Train Epoch: 21 [16800/17010 (99%)] Loss: 0.001909
Train Epoch: 21 [16960/17010 (100%)] Loss: 0.000291
    epoch          : 21
    Train_loss     : 0.004367642228270823
    Train_accuracy : 0.9987077067669173
    Train_f1_score : 0.9987077116966248
    Val_loss       : 0.02307577055968674
    Val_accuracy   : 0.9927083333333333
    Val_f1_score   : 0.9927083849906921
Train Epoch: 22 [0/17010 (0%)] Loss: 0.000770
Train Epoch: 22 [160/17010 (1%)] Loss: 0.002495
Train Epoch: 22 [320/17010 (2%)] Loss: 0.003703
Train Epoch: 22 [480/17010 (3%)] Loss: 0.000209
Train Epoch: 22 [640/17010 (4%)] Loss: 0.012485
Train Epoch: 22 [800/17010 (5%)] Loss: 0.001794
Train Epoch: 22 [960/17010 (6%)] Loss: 0.002450
Train Epoch: 22 [1120/17010 (7%)] Loss: 0.000083
Train Epoch: 22 [1280/17010 (8%)] Loss: 0.000126
Train Epoch: 22 [1440/17010 (8%)] Loss: 0.003114
Train Epoch: 22 [1600/17010 (9%)] Loss: 0.001442
Train Epoch: 22 [1760/17010 (10%)] Loss: 0.002802
Train Epoch: 22 [1920/17010 (11%)] Loss: 0.001527
Train Epoch: 22 [2080/17010 (12%)] Loss: 0.000374
Train Epoch: 22 [2240/17010 (13%)] Loss: 0.000063
Train Epoch: 22 [2400/17010 (14%)] Loss: 0.001698
Train Epoch: 22 [2560/17010 (15%)] Loss: 0.000430
Train Epoch: 22 [2720/17010 (16%)] Loss: 0.077643
Train Epoch: 22 [2880/17010 (17%)] Loss: 0.000070
Train Epoch: 22 [3040/17010 (18%)] Loss: 0.001238
Train Epoch: 22 [3200/17010 (19%)] Loss: 0.016010
Train Epoch: 22 [3360/17010 (20%)] Loss: 0.007272
Train Epoch: 22 [3520/17010 (21%)] Loss: 0.011609
Train Epoch: 22 [3680/17010 (22%)] Loss: 0.000724
Train Epoch: 22 [3840/17010 (23%)] Loss: 0.045938
Train Epoch: 22 [4000/17010 (24%)] Loss: 0.002932
Train Epoch: 22 [4160/17010 (24%)] Loss: 0.033428
Train Epoch: 22 [4320/17010 (25%)] Loss: 0.000727
Train Epoch: 22 [4480/17010 (26%)] Loss: 0.000166
Train Epoch: 22 [4640/17010 (27%)] Loss: 0.000045
Train Epoch: 22 [4800/17010 (28%)] Loss: 0.000280
Train Epoch: 22 [4960/17010 (29%)] Loss: 0.016133
Train Epoch: 22 [5120/17010 (30%)] Loss: 0.006880
Train Epoch: 22 [5280/17010 (31%)] Loss: 0.015674
Train Epoch: 22 [5440/17010 (32%)] Loss: 0.000160
Train Epoch: 22 [5600/17010 (33%)] Loss: 0.000743
Train Epoch: 22 [5760/17010 (34%)] Loss: 0.000318
Train Epoch: 22 [5920/17010 (35%)] Loss: 0.000161
Train Epoch: 22 [6080/17010 (36%)] Loss: 0.001515
Train Epoch: 22 [6240/17010 (37%)] Loss: 0.005514
Train Epoch: 22 [6400/17010 (38%)] Loss: 0.000261
Train Epoch: 22 [6560/17010 (39%)] Loss: 0.000503
Train Epoch: 22 [6720/17010 (40%)] Loss: 0.000466
Train Epoch: 22 [6880/17010 (40%)] Loss: 0.000620
Train Epoch: 22 [7040/17010 (41%)] Loss: 0.000652
Train Epoch: 22 [7200/17010 (42%)] Loss: 0.000196
Train Epoch: 22 [7360/17010 (43%)] Loss: 0.000634
Train Epoch: 22 [7520/17010 (44%)] Loss: 0.000223
Train Epoch: 22 [7680/17010 (45%)] Loss: 0.000125
Train Epoch: 22 [7840/17010 (46%)] Loss: 0.000171
Train Epoch: 22 [8000/17010 (47%)] Loss: 0.000516
Train Epoch: 22 [8160/17010 (48%)] Loss: 0.013403
Train Epoch: 22 [8320/17010 (49%)] Loss: 0.005233
Train Epoch: 22 [8480/17010 (50%)] Loss: 0.009023
Train Epoch: 22 [8640/17010 (51%)] Loss: 0.000567
Train Epoch: 22 [8800/17010 (52%)] Loss: 0.000560
Train Epoch: 22 [8960/17010 (53%)] Loss: 0.000146
Train Epoch: 22 [9120/17010 (54%)] Loss: 0.003097
Train Epoch: 22 [9280/17010 (55%)] Loss: 0.000065
Train Epoch: 22 [9440/17010 (55%)] Loss: 0.001492
Train Epoch: 22 [9600/17010 (56%)] Loss: 0.000266
Train Epoch: 22 [9760/17010 (57%)] Loss: 0.001809
Train Epoch: 22 [9920/17010 (58%)] Loss: 0.000292
Train Epoch: 22 [10080/17010 (59%)] Loss: 0.075164
Train Epoch: 22 [10240/17010 (60%)] Loss: 0.000834
Train Epoch: 22 [10400/17010 (61%)] Loss: 0.000101
Train Epoch: 22 [10560/17010 (62%)] Loss: 0.000209
Train Epoch: 22 [10720/17010 (63%)] Loss: 0.000488
Train Epoch: 22 [10880/17010 (64%)] Loss: 0.001750
Train Epoch: 22 [11040/17010 (65%)] Loss: 0.000623
Train Epoch: 22 [11200/17010 (66%)] Loss: 0.000626
Train Epoch: 22 [11360/17010 (67%)] Loss: 0.002041
Train Epoch: 22 [11520/17010 (68%)] Loss: 0.014199
Train Epoch: 22 [11680/17010 (69%)] Loss: 0.002959
Train Epoch: 22 [11840/17010 (70%)] Loss: 0.011509
Train Epoch: 22 [12000/17010 (71%)] Loss: 0.000144
Train Epoch: 22 [12160/17010 (71%)] Loss: 0.001155
Train Epoch: 22 [12320/17010 (72%)] Loss: 0.000236
Train Epoch: 22 [12480/17010 (73%)] Loss: 0.000038
Train Epoch: 22 [12640/17010 (74%)] Loss: 0.002074
Train Epoch: 22 [12800/17010 (75%)] Loss: 0.002869
Train Epoch: 22 [12960/17010 (76%)] Loss: 0.000092
Train Epoch: 22 [13120/17010 (77%)] Loss: 0.005814
Train Epoch: 22 [13280/17010 (78%)] Loss: 0.004294
Train Epoch: 22 [13440/17010 (79%)] Loss: 0.000845
Train Epoch: 22 [13600/17010 (80%)] Loss: 0.000614
Train Epoch: 22 [13760/17010 (81%)] Loss: 0.007500
Train Epoch: 22 [13920/17010 (82%)] Loss: 0.000211
Train Epoch: 22 [14080/17010 (83%)] Loss: 0.000808
Train Epoch: 22 [14240/17010 (84%)] Loss: 0.000101
Train Epoch: 22 [14400/17010 (85%)] Loss: 0.000228
Train Epoch: 22 [14560/17010 (86%)] Loss: 0.000130
Train Epoch: 22 [14720/17010 (87%)] Loss: 0.000922
Train Epoch: 22 [14880/17010 (87%)] Loss: 0.009814
Train Epoch: 22 [15040/17010 (88%)] Loss: 0.000405
Train Epoch: 22 [15200/17010 (89%)] Loss: 0.000739
Train Epoch: 22 [15360/17010 (90%)] Loss: 0.000234
Train Epoch: 22 [15520/17010 (91%)] Loss: 0.001016
Train Epoch: 22 [15680/17010 (92%)] Loss: 0.003562
Train Epoch: 22 [15840/17010 (93%)] Loss: 0.006049
Train Epoch: 22 [16000/17010 (94%)] Loss: 0.000099
Train Epoch: 22 [16160/17010 (95%)] Loss: 0.000434
Train Epoch: 22 [16320/17010 (96%)] Loss: 0.000498
Train Epoch: 22 [16480/17010 (97%)] Loss: 0.002335
Train Epoch: 22 [16640/17010 (98%)] Loss: 0.000167
Train Epoch: 22 [16800/17010 (99%)] Loss: 0.000359
Train Epoch: 22 [16960/17010 (100%)] Loss: 0.000303
    epoch          : 22
    Train_loss     : 0.008489099127654524
    Train_accuracy : 0.9972979323308271
    Train_f1_score : 0.9972979426383972
    Val_loss       : 0.013849281424791115
    Val_accuracy   : 0.9958333333333333
    Val_f1_score   : 0.9958333969116211
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch22.pth ...
Train Epoch: 23 [0/17010 (0%)] Loss: 0.004723
Train Epoch: 23 [160/17010 (1%)] Loss: 0.000301
Train Epoch: 23 [320/17010 (2%)] Loss: 0.000066
Train Epoch: 23 [480/17010 (3%)] Loss: 0.000251
Train Epoch: 23 [640/17010 (4%)] Loss: 0.000446
Train Epoch: 23 [800/17010 (5%)] Loss: 0.005620
Train Epoch: 23 [960/17010 (6%)] Loss: 0.000359
Train Epoch: 23 [1120/17010 (7%)] Loss: 0.000113
Train Epoch: 23 [1280/17010 (8%)] Loss: 0.000122
Train Epoch: 23 [1440/17010 (8%)] Loss: 0.000281
Train Epoch: 23 [1600/17010 (9%)] Loss: 0.000088
Train Epoch: 23 [1760/17010 (10%)] Loss: 0.000304
Train Epoch: 23 [1920/17010 (11%)] Loss: 0.000189
Train Epoch: 23 [2080/17010 (12%)] Loss: 0.000243
Train Epoch: 23 [2240/17010 (13%)] Loss: 0.000597
Train Epoch: 23 [2400/17010 (14%)] Loss: 0.000080
Train Epoch: 23 [2560/17010 (15%)] Loss: 0.000050
Train Epoch: 23 [2720/17010 (16%)] Loss: 0.001074
Train Epoch: 23 [2880/17010 (17%)] Loss: 0.004097
Train Epoch: 23 [3040/17010 (18%)] Loss: 0.000340
Train Epoch: 23 [3200/17010 (19%)] Loss: 0.000536
Train Epoch: 23 [3360/17010 (20%)] Loss: 0.000070
Train Epoch: 23 [3520/17010 (21%)] Loss: 0.000193
Train Epoch: 23 [3680/17010 (22%)] Loss: 0.000323
Train Epoch: 23 [3840/17010 (23%)] Loss: 0.006452
Train Epoch: 23 [4000/17010 (24%)] Loss: 0.000066
Train Epoch: 23 [4160/17010 (24%)] Loss: 0.000066
Train Epoch: 23 [4320/17010 (25%)] Loss: 0.000981
Train Epoch: 23 [4480/17010 (26%)] Loss: 0.000226
Train Epoch: 23 [4640/17010 (27%)] Loss: 0.000103
Train Epoch: 23 [4800/17010 (28%)] Loss: 0.000055
Train Epoch: 23 [4960/17010 (29%)] Loss: 0.000100
Train Epoch: 23 [5120/17010 (30%)] Loss: 0.000057
Train Epoch: 23 [5280/17010 (31%)] Loss: 0.000244
Train Epoch: 23 [5440/17010 (32%)] Loss: 0.000121
Train Epoch: 23 [5600/17010 (33%)] Loss: 0.000147
Train Epoch: 23 [5760/17010 (34%)] Loss: 0.000044
Train Epoch: 23 [5920/17010 (35%)] Loss: 0.000055
Train Epoch: 23 [6080/17010 (36%)] Loss: 0.000219
Train Epoch: 23 [6240/17010 (37%)] Loss: 0.000234
Train Epoch: 23 [6400/17010 (38%)] Loss: 0.000194
Train Epoch: 23 [6560/17010 (39%)] Loss: 0.000704
Train Epoch: 23 [6720/17010 (40%)] Loss: 0.000029
Train Epoch: 23 [6880/17010 (40%)] Loss: 0.000206
Train Epoch: 23 [7040/17010 (41%)] Loss: 0.000120
Train Epoch: 23 [7200/17010 (42%)] Loss: 0.000084
Train Epoch: 23 [7360/17010 (43%)] Loss: 0.000209
Train Epoch: 23 [7520/17010 (44%)] Loss: 0.000085
Train Epoch: 23 [7680/17010 (45%)] Loss: 0.000107
Train Epoch: 23 [7840/17010 (46%)] Loss: 0.000162
Train Epoch: 23 [8000/17010 (47%)] Loss: 0.001183
Train Epoch: 23 [8160/17010 (48%)] Loss: 0.000025
Train Epoch: 23 [8320/17010 (49%)] Loss: 0.000397
Train Epoch: 23 [8480/17010 (50%)] Loss: 0.001042
Train Epoch: 23 [8640/17010 (51%)] Loss: 0.000103
Train Epoch: 23 [8800/17010 (52%)] Loss: 0.000196
Train Epoch: 23 [8960/17010 (53%)] Loss: 0.000056
Train Epoch: 23 [9120/17010 (54%)] Loss: 0.000040
Train Epoch: 23 [9280/17010 (55%)] Loss: 0.000072
Train Epoch: 23 [9440/17010 (55%)] Loss: 0.000291
Train Epoch: 23 [9600/17010 (56%)] Loss: 0.000060
Train Epoch: 23 [9760/17010 (57%)] Loss: 0.000050
Train Epoch: 23 [9920/17010 (58%)] Loss: 0.000622
Train Epoch: 23 [10080/17010 (59%)] Loss: 0.000828
Train Epoch: 23 [10240/17010 (60%)] Loss: 0.000081
Train Epoch: 23 [10400/17010 (61%)] Loss: 0.000108
Train Epoch: 23 [10560/17010 (62%)] Loss: 0.001732
Train Epoch: 23 [10720/17010 (63%)] Loss: 0.000120
Train Epoch: 23 [10880/17010 (64%)] Loss: 0.000055
Train Epoch: 23 [11040/17010 (65%)] Loss: 0.001005
Train Epoch: 23 [11200/17010 (66%)] Loss: 0.000225
Train Epoch: 23 [11360/17010 (67%)] Loss: 0.000051
Train Epoch: 23 [11520/17010 (68%)] Loss: 0.000131
Train Epoch: 23 [11680/17010 (69%)] Loss: 0.000827
Train Epoch: 23 [11840/17010 (70%)] Loss: 0.000151
Train Epoch: 23 [12000/17010 (71%)] Loss: 0.000134
Train Epoch: 23 [12160/17010 (71%)] Loss: 0.000196
Train Epoch: 23 [12320/17010 (72%)] Loss: 0.000114
Train Epoch: 23 [12480/17010 (73%)] Loss: 0.000012
Train Epoch: 23 [12640/17010 (74%)] Loss: 0.000087
Train Epoch: 23 [12800/17010 (75%)] Loss: 0.000123
Train Epoch: 23 [12960/17010 (76%)] Loss: 0.000227
Train Epoch: 23 [13120/17010 (77%)] Loss: 0.000076
Train Epoch: 23 [13280/17010 (78%)] Loss: 0.000125
Train Epoch: 23 [13440/17010 (79%)] Loss: 0.007220
Train Epoch: 23 [13600/17010 (80%)] Loss: 0.000048
Train Epoch: 23 [13760/17010 (81%)] Loss: 0.000111
Train Epoch: 23 [13920/17010 (82%)] Loss: 0.000248
Train Epoch: 23 [14080/17010 (83%)] Loss: 0.000051
Train Epoch: 23 [14240/17010 (84%)] Loss: 0.000099
Train Epoch: 23 [14400/17010 (85%)] Loss: 0.000212
Train Epoch: 23 [14560/17010 (86%)] Loss: 0.000116
Train Epoch: 23 [14720/17010 (87%)] Loss: 0.000051
Train Epoch: 23 [14880/17010 (87%)] Loss: 0.000065
Train Epoch: 23 [15040/17010 (88%)] Loss: 0.000146
Train Epoch: 23 [15200/17010 (89%)] Loss: 0.000065
Train Epoch: 23 [15360/17010 (90%)] Loss: 0.001800
Train Epoch: 23 [15520/17010 (91%)] Loss: 0.004391
Train Epoch: 23 [15680/17010 (92%)] Loss: 0.000070
Train Epoch: 23 [15840/17010 (93%)] Loss: 0.000020
Train Epoch: 23 [16000/17010 (94%)] Loss: 0.004471
Train Epoch: 23 [16160/17010 (95%)] Loss: 0.008960
Train Epoch: 23 [16320/17010 (96%)] Loss: 0.000053
Train Epoch: 23 [16480/17010 (97%)] Loss: 0.000336
Train Epoch: 23 [16640/17010 (98%)] Loss: 0.023671
Train Epoch: 23 [16800/17010 (99%)] Loss: 0.000067
Train Epoch: 23 [16960/17010 (100%)] Loss: 0.000030
    epoch          : 23
    Train_loss     : 0.0010331103218781674
    Train_accuracy : 0.9998237781954887
    Train_f1_score : 0.999823808670044
    Val_loss       : 0.023533876460805913
    Val_accuracy   : 0.9953125
    Val_f1_score   : 0.9953125715255737
Train Epoch: 24 [0/17010 (0%)] Loss: 0.004367
Train Epoch: 24 [160/17010 (1%)] Loss: 0.000203
Train Epoch: 24 [320/17010 (2%)] Loss: 0.000209
Train Epoch: 24 [480/17010 (3%)] Loss: 0.000181
Train Epoch: 24 [640/17010 (4%)] Loss: 0.000323
Train Epoch: 24 [800/17010 (5%)] Loss: 0.000033
Train Epoch: 24 [960/17010 (6%)] Loss: 0.000318
Train Epoch: 24 [1120/17010 (7%)] Loss: 0.008734
Train Epoch: 24 [1280/17010 (8%)] Loss: 0.000032
Train Epoch: 24 [1440/17010 (8%)] Loss: 0.000252
Train Epoch: 24 [1600/17010 (9%)] Loss: 0.000253
Train Epoch: 24 [1760/17010 (10%)] Loss: 0.010082
Train Epoch: 24 [1920/17010 (11%)] Loss: 0.000173
Train Epoch: 24 [2080/17010 (12%)] Loss: 0.003919
Train Epoch: 24 [2240/17010 (13%)] Loss: 0.000132
Train Epoch: 24 [2400/17010 (14%)] Loss: 0.004601
Train Epoch: 24 [2560/17010 (15%)] Loss: 0.000130
Train Epoch: 24 [2720/17010 (16%)] Loss: 0.008819
Train Epoch: 24 [2880/17010 (17%)] Loss: 0.000058
Train Epoch: 24 [3040/17010 (18%)] Loss: 0.000354
Train Epoch: 24 [3200/17010 (19%)] Loss: 0.000244
Train Epoch: 24 [3360/17010 (20%)] Loss: 0.000054
Train Epoch: 24 [3520/17010 (21%)] Loss: 0.000069
Train Epoch: 24 [3680/17010 (22%)] Loss: 0.000273
Train Epoch: 24 [3840/17010 (23%)] Loss: 0.000073
Train Epoch: 24 [4000/17010 (24%)] Loss: 0.000029
Train Epoch: 24 [4160/17010 (24%)] Loss: 0.000087
Train Epoch: 24 [4320/17010 (25%)] Loss: 0.000171
Train Epoch: 24 [4480/17010 (26%)] Loss: 0.000096
Train Epoch: 24 [4640/17010 (27%)] Loss: 0.000174
Train Epoch: 24 [4800/17010 (28%)] Loss: 0.001415
Train Epoch: 24 [4960/17010 (29%)] Loss: 0.000299
Train Epoch: 24 [5120/17010 (30%)] Loss: 0.000142
Train Epoch: 24 [5280/17010 (31%)] Loss: 0.000040
Train Epoch: 24 [5440/17010 (32%)] Loss: 0.000123
Train Epoch: 24 [5600/17010 (33%)] Loss: 0.000144
Train Epoch: 24 [5760/17010 (34%)] Loss: 0.074394
Train Epoch: 24 [5920/17010 (35%)] Loss: 0.000043
Train Epoch: 24 [6080/17010 (36%)] Loss: 0.000034
Train Epoch: 24 [6240/17010 (37%)] Loss: 0.000027
Train Epoch: 24 [6400/17010 (38%)] Loss: 0.000041
Train Epoch: 24 [6560/17010 (39%)] Loss: 0.043578
Train Epoch: 24 [6720/17010 (40%)] Loss: 0.000346
Train Epoch: 24 [6880/17010 (40%)] Loss: 0.008836
Train Epoch: 24 [7040/17010 (41%)] Loss: 0.000625
Train Epoch: 24 [7200/17010 (42%)] Loss: 0.000096
Train Epoch: 24 [7360/17010 (43%)] Loss: 0.000707
Train Epoch: 24 [7520/17010 (44%)] Loss: 0.007152
Train Epoch: 24 [7680/17010 (45%)] Loss: 0.006149
Train Epoch: 24 [7840/17010 (46%)] Loss: 0.000465
Train Epoch: 24 [8000/17010 (47%)] Loss: 0.000092
Train Epoch: 24 [8160/17010 (48%)] Loss: 0.000127
Train Epoch: 24 [8320/17010 (49%)] Loss: 0.000242
Train Epoch: 24 [8480/17010 (50%)] Loss: 0.000056
Train Epoch: 24 [8640/17010 (51%)] Loss: 0.000228
Train Epoch: 24 [8800/17010 (52%)] Loss: 0.000110
Train Epoch: 24 [8960/17010 (53%)] Loss: 0.000037
Train Epoch: 24 [9120/17010 (54%)] Loss: 0.000090
Train Epoch: 24 [9280/17010 (55%)] Loss: 0.000087
Train Epoch: 24 [9440/17010 (55%)] Loss: 0.000253
Train Epoch: 24 [9600/17010 (56%)] Loss: 0.000349
Train Epoch: 24 [9760/17010 (57%)] Loss: 0.000113
Train Epoch: 24 [9920/17010 (58%)] Loss: 0.000050
Train Epoch: 24 [10080/17010 (59%)] Loss: 0.000084
Train Epoch: 24 [10240/17010 (60%)] Loss: 0.000127
Train Epoch: 24 [10400/17010 (61%)] Loss: 0.000188
Train Epoch: 24 [10560/17010 (62%)] Loss: 0.005109
Train Epoch: 24 [10720/17010 (63%)] Loss: 0.002212
Train Epoch: 24 [10880/17010 (64%)] Loss: 0.000058
Train Epoch: 24 [11040/17010 (65%)] Loss: 0.000105
Train Epoch: 24 [11200/17010 (66%)] Loss: 0.000035
Train Epoch: 24 [11360/17010 (67%)] Loss: 0.000177
Train Epoch: 24 [11520/17010 (68%)] Loss: 0.000052
Train Epoch: 24 [11680/17010 (69%)] Loss: 0.000073
Train Epoch: 24 [11840/17010 (70%)] Loss: 0.091914
Train Epoch: 24 [12000/17010 (71%)] Loss: 0.000652
Train Epoch: 24 [12160/17010 (71%)] Loss: 0.000091
Train Epoch: 24 [12320/17010 (72%)] Loss: 0.000214
Train Epoch: 24 [12480/17010 (73%)] Loss: 0.014302
Train Epoch: 24 [12640/17010 (74%)] Loss: 0.000195
Train Epoch: 24 [12800/17010 (75%)] Loss: 0.000064
Train Epoch: 24 [12960/17010 (76%)] Loss: 0.000677
Train Epoch: 24 [13120/17010 (77%)] Loss: 0.000176
Train Epoch: 24 [13280/17010 (78%)] Loss: 0.000047
Train Epoch: 24 [13440/17010 (79%)] Loss: 0.000059
Train Epoch: 24 [13600/17010 (80%)] Loss: 0.000065
Train Epoch: 24 [13760/17010 (81%)] Loss: 0.018578
Train Epoch: 24 [13920/17010 (82%)] Loss: 0.000206
Train Epoch: 24 [14080/17010 (83%)] Loss: 0.000032
Train Epoch: 24 [14240/17010 (84%)] Loss: 0.000116
Train Epoch: 24 [14400/17010 (85%)] Loss: 0.008543
Train Epoch: 24 [14560/17010 (86%)] Loss: 0.000027
Train Epoch: 24 [14720/17010 (87%)] Loss: 0.097771
Train Epoch: 24 [14880/17010 (87%)] Loss: 0.000045
Train Epoch: 24 [15040/17010 (88%)] Loss: 0.000165
Train Epoch: 24 [15200/17010 (89%)] Loss: 0.000333
Train Epoch: 24 [15360/17010 (90%)] Loss: 0.000229
Train Epoch: 24 [15520/17010 (91%)] Loss: 0.003305
Train Epoch: 24 [15680/17010 (92%)] Loss: 0.016144
Train Epoch: 24 [15840/17010 (93%)] Loss: 0.000540
Train Epoch: 24 [16000/17010 (94%)] Loss: 0.001222
Train Epoch: 24 [16160/17010 (95%)] Loss: 0.002837
Train Epoch: 24 [16320/17010 (96%)] Loss: 0.003773
Train Epoch: 24 [16480/17010 (97%)] Loss: 0.000089
Train Epoch: 24 [16640/17010 (98%)] Loss: 0.038173
Train Epoch: 24 [16800/17010 (99%)] Loss: 0.000593
Train Epoch: 24 [16960/17010 (100%)] Loss: 0.000090
    epoch          : 24
    Train_loss     : 0.005152611359248236
    Train_accuracy : 0.9986032790309106
    Train_f1_score : 0.998603343963623
    Val_loss       : 0.04219909753343624
    Val_accuracy   : 0.9911458333333333
    Val_f1_score   : 0.99114590883255
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch24.pth ...
Train Epoch: 25 [0/17010 (0%)] Loss: 0.000109
Train Epoch: 25 [160/17010 (1%)] Loss: 0.000056
Train Epoch: 25 [320/17010 (2%)] Loss: 0.000286
Train Epoch: 25 [480/17010 (3%)] Loss: 0.000743
Train Epoch: 25 [640/17010 (4%)] Loss: 0.005510
Train Epoch: 25 [800/17010 (5%)] Loss: 0.000125
Train Epoch: 25 [960/17010 (6%)] Loss: 0.000177
Train Epoch: 25 [1120/17010 (7%)] Loss: 0.001887
Train Epoch: 25 [1280/17010 (8%)] Loss: 0.000382
Train Epoch: 25 [1440/17010 (8%)] Loss: 0.003714
Train Epoch: 25 [1600/17010 (9%)] Loss: 0.001642
Train Epoch: 25 [1760/17010 (10%)] Loss: 0.000224
Train Epoch: 25 [1920/17010 (11%)] Loss: 0.002720
Train Epoch: 25 [2080/17010 (12%)] Loss: 0.035472
Train Epoch: 25 [2240/17010 (13%)] Loss: 0.023390
Train Epoch: 25 [2400/17010 (14%)] Loss: 0.006681
Train Epoch: 25 [2560/17010 (15%)] Loss: 0.000084
Train Epoch: 25 [2720/17010 (16%)] Loss: 0.000397
Train Epoch: 25 [2880/17010 (17%)] Loss: 0.000308
Train Epoch: 25 [3040/17010 (18%)] Loss: 0.000218
Train Epoch: 25 [3200/17010 (19%)] Loss: 0.004664
Train Epoch: 25 [3360/17010 (20%)] Loss: 0.000273
Train Epoch: 25 [3520/17010 (21%)] Loss: 0.002046
Train Epoch: 25 [3680/17010 (22%)] Loss: 0.000352
Train Epoch: 25 [3840/17010 (23%)] Loss: 0.000394
Train Epoch: 25 [4000/17010 (24%)] Loss: 0.058817
Train Epoch: 25 [4160/17010 (24%)] Loss: 0.001043
Train Epoch: 25 [4320/17010 (25%)] Loss: 0.000583
Train Epoch: 25 [4480/17010 (26%)] Loss: 0.000335
Train Epoch: 25 [4640/17010 (27%)] Loss: 0.074546
Train Epoch: 25 [4800/17010 (28%)] Loss: 0.000800
Train Epoch: 25 [4960/17010 (29%)] Loss: 0.003762
Train Epoch: 25 [5120/17010 (30%)] Loss: 0.004747
Train Epoch: 25 [5280/17010 (31%)] Loss: 0.000418
Train Epoch: 25 [5440/17010 (32%)] Loss: 0.000956
Train Epoch: 25 [5600/17010 (33%)] Loss: 0.024569
Train Epoch: 25 [5760/17010 (34%)] Loss: 0.014809
Train Epoch: 25 [5920/17010 (35%)] Loss: 0.012466
Train Epoch: 25 [6080/17010 (36%)] Loss: 0.000232
Train Epoch: 25 [6240/17010 (37%)] Loss: 0.000266
Train Epoch: 25 [6400/17010 (38%)] Loss: 0.000527
Train Epoch: 25 [6560/17010 (39%)] Loss: 0.000291
Train Epoch: 25 [6720/17010 (40%)] Loss: 0.005471
Train Epoch: 25 [6880/17010 (40%)] Loss: 0.002254
Train Epoch: 25 [7040/17010 (41%)] Loss: 0.000148
Train Epoch: 25 [7200/17010 (42%)] Loss: 0.027688
Train Epoch: 25 [7360/17010 (43%)] Loss: 0.000139
Train Epoch: 25 [7520/17010 (44%)] Loss: 0.000193
Train Epoch: 25 [7680/17010 (45%)] Loss: 0.036148
Train Epoch: 25 [7840/17010 (46%)] Loss: 0.000055
Train Epoch: 25 [8000/17010 (47%)] Loss: 0.000242
Train Epoch: 25 [8160/17010 (48%)] Loss: 0.000988
Train Epoch: 25 [8320/17010 (49%)] Loss: 0.000931
Train Epoch: 25 [8480/17010 (50%)] Loss: 0.001326
Train Epoch: 25 [8640/17010 (51%)] Loss: 0.000086
Train Epoch: 25 [8800/17010 (52%)] Loss: 0.000103
Train Epoch: 25 [8960/17010 (53%)] Loss: 0.000208
Train Epoch: 25 [9120/17010 (54%)] Loss: 0.018499
Train Epoch: 25 [9280/17010 (55%)] Loss: 0.001988
Train Epoch: 25 [9440/17010 (55%)] Loss: 0.000122
Train Epoch: 25 [9600/17010 (56%)] Loss: 0.000889
Train Epoch: 25 [9760/17010 (57%)] Loss: 0.000951
Train Epoch: 25 [9920/17010 (58%)] Loss: 0.001292
Train Epoch: 25 [10080/17010 (59%)] Loss: 0.000166
Train Epoch: 25 [10240/17010 (60%)] Loss: 0.000121
Train Epoch: 25 [10400/17010 (61%)] Loss: 0.000346
Train Epoch: 25 [10560/17010 (62%)] Loss: 0.000152
Train Epoch: 25 [10720/17010 (63%)] Loss: 0.000521
Train Epoch: 25 [10880/17010 (64%)] Loss: 0.000144
Train Epoch: 25 [11040/17010 (65%)] Loss: 0.000994
Train Epoch: 25 [11200/17010 (66%)] Loss: 0.000102
Train Epoch: 25 [11360/17010 (67%)] Loss: 0.000150
Train Epoch: 25 [11520/17010 (68%)] Loss: 0.000182
Train Epoch: 25 [11680/17010 (69%)] Loss: 0.000760
Train Epoch: 25 [11840/17010 (70%)] Loss: 0.001230
Train Epoch: 25 [12000/17010 (71%)] Loss: 0.000627
Train Epoch: 25 [12160/17010 (71%)] Loss: 0.000126
Train Epoch: 25 [12320/17010 (72%)] Loss: 0.007611
Train Epoch: 25 [12480/17010 (73%)] Loss: 0.000128
Train Epoch: 25 [12640/17010 (74%)] Loss: 0.000037
Train Epoch: 25 [12800/17010 (75%)] Loss: 0.000264
Train Epoch: 25 [12960/17010 (76%)] Loss: 0.001731
Train Epoch: 25 [13120/17010 (77%)] Loss: 0.000641
Train Epoch: 25 [13280/17010 (78%)] Loss: 0.000156
Train Epoch: 25 [13440/17010 (79%)] Loss: 0.000069
Train Epoch: 25 [13600/17010 (80%)] Loss: 0.236850
Train Epoch: 25 [13760/17010 (81%)] Loss: 0.000768
Train Epoch: 25 [13920/17010 (82%)] Loss: 0.000244
Train Epoch: 25 [14080/17010 (83%)] Loss: 0.011081
Train Epoch: 25 [14240/17010 (84%)] Loss: 0.002391
Train Epoch: 25 [14400/17010 (85%)] Loss: 0.000266
Train Epoch: 25 [14560/17010 (86%)] Loss: 0.005619
Train Epoch: 25 [14720/17010 (87%)] Loss: 0.000778
Train Epoch: 25 [14880/17010 (87%)] Loss: 0.000337
Train Epoch: 25 [15040/17010 (88%)] Loss: 0.002320
Train Epoch: 25 [15200/17010 (89%)] Loss: 0.001327
Train Epoch: 25 [15360/17010 (90%)] Loss: 0.001716
Train Epoch: 25 [15520/17010 (91%)] Loss: 0.005742
Train Epoch: 25 [15680/17010 (92%)] Loss: 0.006166
Train Epoch: 25 [15840/17010 (93%)] Loss: 0.000661
Train Epoch: 25 [16000/17010 (94%)] Loss: 0.000921
Train Epoch: 25 [16160/17010 (95%)] Loss: 0.000355
Train Epoch: 25 [16320/17010 (96%)] Loss: 0.001631
Train Epoch: 25 [16480/17010 (97%)] Loss: 0.000359
Train Epoch: 25 [16640/17010 (98%)] Loss: 0.000206
Train Epoch: 25 [16800/17010 (99%)] Loss: 0.000339
Train Epoch: 25 [16960/17010 (100%)] Loss: 0.000434
    epoch          : 25
    Train_loss     : 0.008333716012886617
    Train_accuracy : 0.9972391917293233
    Train_f1_score : 0.9972392320632935
    Val_loss       : 0.044909572421261146
    Val_accuracy   : 0.9890625
    Val_f1_score   : 0.9890625476837158
Train Epoch: 26 [0/17010 (0%)] Loss: 0.015885
Train Epoch: 26 [160/17010 (1%)] Loss: 0.000664
Train Epoch: 26 [320/17010 (2%)] Loss: 0.040165
Train Epoch: 26 [480/17010 (3%)] Loss: 0.002059
Train Epoch: 26 [640/17010 (4%)] Loss: 0.001251
Train Epoch: 26 [800/17010 (5%)] Loss: 0.000584
Train Epoch: 26 [960/17010 (6%)] Loss: 0.031843
Train Epoch: 26 [1120/17010 (7%)] Loss: 0.015721
Train Epoch: 26 [1280/17010 (8%)] Loss: 0.000359
Train Epoch: 26 [1440/17010 (8%)] Loss: 0.000451
Train Epoch: 26 [1600/17010 (9%)] Loss: 0.013769
Train Epoch: 26 [1760/17010 (10%)] Loss: 0.000160
Train Epoch: 26 [1920/17010 (11%)] Loss: 0.000486
Train Epoch: 26 [2080/17010 (12%)] Loss: 0.001424
Train Epoch: 26 [2240/17010 (13%)] Loss: 0.000174
Train Epoch: 26 [2400/17010 (14%)] Loss: 0.004959
Train Epoch: 26 [2560/17010 (15%)] Loss: 0.001660
Train Epoch: 26 [2720/17010 (16%)] Loss: 0.076509
Train Epoch: 26 [2880/17010 (17%)] Loss: 0.000449
Train Epoch: 26 [3040/17010 (18%)] Loss: 0.000080
Train Epoch: 26 [3200/17010 (19%)] Loss: 0.010081
Train Epoch: 26 [3360/17010 (20%)] Loss: 0.016803
Train Epoch: 26 [3520/17010 (21%)] Loss: 0.001223
Train Epoch: 26 [3680/17010 (22%)] Loss: 0.003805
Train Epoch: 26 [3840/17010 (23%)] Loss: 0.003532
Train Epoch: 26 [4000/17010 (24%)] Loss: 0.000682
Train Epoch: 26 [4160/17010 (24%)] Loss: 0.028412
Train Epoch: 26 [4320/17010 (25%)] Loss: 0.044989
Train Epoch: 26 [4480/17010 (26%)] Loss: 0.001274
Train Epoch: 26 [4640/17010 (27%)] Loss: 0.000295
Train Epoch: 26 [4800/17010 (28%)] Loss: 0.000970
Train Epoch: 26 [4960/17010 (29%)] Loss: 0.000038
Train Epoch: 26 [5120/17010 (30%)] Loss: 0.000821
Train Epoch: 26 [5280/17010 (31%)] Loss: 0.000408
Train Epoch: 26 [5440/17010 (32%)] Loss: 0.000269
Train Epoch: 26 [5600/17010 (33%)] Loss: 0.000102
Train Epoch: 26 [5760/17010 (34%)] Loss: 0.001412
Train Epoch: 26 [5920/17010 (35%)] Loss: 0.000393
Train Epoch: 26 [6080/17010 (36%)] Loss: 0.000631
Train Epoch: 26 [6240/17010 (37%)] Loss: 0.064800
Train Epoch: 26 [6400/17010 (38%)] Loss: 0.028141
Train Epoch: 26 [6560/17010 (39%)] Loss: 0.001636
Train Epoch: 26 [6720/17010 (40%)] Loss: 0.004763
Train Epoch: 26 [6880/17010 (40%)] Loss: 0.000397
Train Epoch: 26 [7040/17010 (41%)] Loss: 0.000730
Train Epoch: 26 [7200/17010 (42%)] Loss: 0.000243
Train Epoch: 26 [7360/17010 (43%)] Loss: 0.110835
Train Epoch: 26 [7520/17010 (44%)] Loss: 0.000309
Train Epoch: 26 [7680/17010 (45%)] Loss: 0.001607
Train Epoch: 26 [7840/17010 (46%)] Loss: 0.000224
Train Epoch: 26 [8000/17010 (47%)] Loss: 0.000707
Train Epoch: 26 [8160/17010 (48%)] Loss: 0.000141
Train Epoch: 26 [8320/17010 (49%)] Loss: 0.000259
Train Epoch: 26 [8480/17010 (50%)] Loss: 0.016033
Train Epoch: 26 [8640/17010 (51%)] Loss: 0.010882
Train Epoch: 26 [8800/17010 (52%)] Loss: 0.000120
Train Epoch: 26 [8960/17010 (53%)] Loss: 0.001275
Train Epoch: 26 [9120/17010 (54%)] Loss: 0.003966
Train Epoch: 26 [9280/17010 (55%)] Loss: 0.001377
Train Epoch: 26 [9440/17010 (55%)] Loss: 0.000064
Train Epoch: 26 [9600/17010 (56%)] Loss: 0.000290
Train Epoch: 26 [9760/17010 (57%)] Loss: 0.022390
Train Epoch: 26 [9920/17010 (58%)] Loss: 0.009337
Train Epoch: 26 [10080/17010 (59%)] Loss: 0.001146
Train Epoch: 26 [10240/17010 (60%)] Loss: 0.000068
Train Epoch: 26 [10400/17010 (61%)] Loss: 0.000342
Train Epoch: 26 [10560/17010 (62%)] Loss: 0.001977
Train Epoch: 26 [10720/17010 (63%)] Loss: 0.000051
Train Epoch: 26 [10880/17010 (64%)] Loss: 0.015073
Train Epoch: 26 [11040/17010 (65%)] Loss: 0.000068
Train Epoch: 26 [11200/17010 (66%)] Loss: 0.002936
Train Epoch: 26 [11360/17010 (67%)] Loss: 0.000191
Train Epoch: 26 [11520/17010 (68%)] Loss: 0.000337
Train Epoch: 26 [11680/17010 (69%)] Loss: 0.005430
Train Epoch: 26 [11840/17010 (70%)] Loss: 0.001627
Train Epoch: 26 [12000/17010 (71%)] Loss: 0.006038
Train Epoch: 26 [12160/17010 (71%)] Loss: 0.001728
Train Epoch: 26 [12320/17010 (72%)] Loss: 0.000147
Train Epoch: 26 [12480/17010 (73%)] Loss: 0.000222
Train Epoch: 26 [12640/17010 (74%)] Loss: 0.000247
Train Epoch: 26 [12800/17010 (75%)] Loss: 0.000072
Train Epoch: 26 [12960/17010 (76%)] Loss: 0.000131
Train Epoch: 26 [13120/17010 (77%)] Loss: 0.003491
Train Epoch: 26 [13280/17010 (78%)] Loss: 0.008957
Train Epoch: 26 [13440/17010 (79%)] Loss: 0.000352
Train Epoch: 26 [13600/17010 (80%)] Loss: 0.000063
Train Epoch: 26 [13760/17010 (81%)] Loss: 0.000157
Train Epoch: 26 [13920/17010 (82%)] Loss: 0.001360
Train Epoch: 26 [14080/17010 (83%)] Loss: 0.000958
Train Epoch: 26 [14240/17010 (84%)] Loss: 0.000686
Train Epoch: 26 [14400/17010 (85%)] Loss: 0.000116
Train Epoch: 26 [14560/17010 (86%)] Loss: 0.000074
Train Epoch: 26 [14720/17010 (87%)] Loss: 0.000547
Train Epoch: 26 [14880/17010 (87%)] Loss: 0.000163
Train Epoch: 26 [15040/17010 (88%)] Loss: 0.000134
Train Epoch: 26 [15200/17010 (89%)] Loss: 0.000121
Train Epoch: 26 [15360/17010 (90%)] Loss: 0.000229
Train Epoch: 26 [15520/17010 (91%)] Loss: 0.019164
Train Epoch: 26 [15680/17010 (92%)] Loss: 0.000612
Train Epoch: 26 [15840/17010 (93%)] Loss: 0.000359
Train Epoch: 26 [16000/17010 (94%)] Loss: 0.000287
Train Epoch: 26 [16160/17010 (95%)] Loss: 0.000170
Train Epoch: 26 [16320/17010 (96%)] Loss: 0.000050
Train Epoch: 26 [16480/17010 (97%)] Loss: 0.000143
Train Epoch: 26 [16640/17010 (98%)] Loss: 0.000054
Train Epoch: 26 [16800/17010 (99%)] Loss: 0.000137
Train Epoch: 26 [16960/17010 (100%)] Loss: 0.000291
    epoch          : 26
    Train_loss     : 0.007095547557261504
    Train_accuracy : 0.9976503759398496
    Train_f1_score : 0.9976503849029541
    Val_loss       : 0.008859838857597424
    Val_accuracy   : 0.9979166666666667
    Val_f1_score   : 0.9979166984558105
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch26.pth ...
Train Epoch: 27 [0/17010 (0%)] Loss: 0.000075
Train Epoch: 27 [160/17010 (1%)] Loss: 0.000045
Train Epoch: 27 [320/17010 (2%)] Loss: 0.000181
Train Epoch: 27 [480/17010 (3%)] Loss: 0.000289
Train Epoch: 27 [640/17010 (4%)] Loss: 0.000574
Train Epoch: 27 [800/17010 (5%)] Loss: 0.000214
Train Epoch: 27 [960/17010 (6%)] Loss: 0.000235
Train Epoch: 27 [1120/17010 (7%)] Loss: 0.000111
Train Epoch: 27 [1280/17010 (8%)] Loss: 0.000793
Train Epoch: 27 [1440/17010 (8%)] Loss: 0.000043
Train Epoch: 27 [1600/17010 (9%)] Loss: 0.000082
Train Epoch: 27 [1760/17010 (10%)] Loss: 0.000062
Train Epoch: 27 [1920/17010 (11%)] Loss: 0.000020
Train Epoch: 27 [2080/17010 (12%)] Loss: 0.000037
Train Epoch: 27 [2240/17010 (13%)] Loss: 0.000141
Train Epoch: 27 [2400/17010 (14%)] Loss: 0.081612
Train Epoch: 27 [2560/17010 (15%)] Loss: 0.001163
Train Epoch: 27 [2720/17010 (16%)] Loss: 0.000229
Train Epoch: 27 [2880/17010 (17%)] Loss: 0.000203
Train Epoch: 27 [3040/17010 (18%)] Loss: 0.000077
Train Epoch: 27 [3200/17010 (19%)] Loss: 0.001487
Train Epoch: 27 [3360/17010 (20%)] Loss: 0.000130
Train Epoch: 27 [3520/17010 (21%)] Loss: 0.000162
Train Epoch: 27 [3680/17010 (22%)] Loss: 0.000581
Train Epoch: 27 [3840/17010 (23%)] Loss: 0.000247
Train Epoch: 27 [4000/17010 (24%)] Loss: 0.000268
Train Epoch: 27 [4160/17010 (24%)] Loss: 0.000078
Train Epoch: 27 [4320/17010 (25%)] Loss: 0.000045
Train Epoch: 27 [4480/17010 (26%)] Loss: 0.000123
Train Epoch: 27 [4640/17010 (27%)] Loss: 0.000036
Train Epoch: 27 [4800/17010 (28%)] Loss: 0.000052
Train Epoch: 27 [4960/17010 (29%)] Loss: 0.000184
Train Epoch: 27 [5120/17010 (30%)] Loss: 0.000100
Train Epoch: 27 [5280/17010 (31%)] Loss: 0.000072
Train Epoch: 27 [5440/17010 (32%)] Loss: 0.000083
Train Epoch: 27 [5600/17010 (33%)] Loss: 0.000790
Train Epoch: 27 [5760/17010 (34%)] Loss: 0.000619
Train Epoch: 27 [5920/17010 (35%)] Loss: 0.000061
Train Epoch: 27 [6080/17010 (36%)] Loss: 0.000106
Train Epoch: 27 [6240/17010 (37%)] Loss: 0.000344
Train Epoch: 27 [6400/17010 (38%)] Loss: 0.000386
Train Epoch: 27 [6560/17010 (39%)] Loss: 0.000137
Train Epoch: 27 [6720/17010 (40%)] Loss: 0.000082
Train Epoch: 27 [6880/17010 (40%)] Loss: 0.000129
Train Epoch: 27 [7040/17010 (41%)] Loss: 0.000647
Train Epoch: 27 [7200/17010 (42%)] Loss: 0.000074
Train Epoch: 27 [7360/17010 (43%)] Loss: 0.013729
Train Epoch: 27 [7520/17010 (44%)] Loss: 0.000319
Train Epoch: 27 [7680/17010 (45%)] Loss: 0.000087
Train Epoch: 27 [7840/17010 (46%)] Loss: 0.003128
Train Epoch: 27 [8000/17010 (47%)] Loss: 0.000088
Train Epoch: 27 [8160/17010 (48%)] Loss: 0.000904
Train Epoch: 27 [8320/17010 (49%)] Loss: 0.000057
Train Epoch: 27 [8480/17010 (50%)] Loss: 0.000099
Train Epoch: 27 [8640/17010 (51%)] Loss: 0.001146
Train Epoch: 27 [8800/17010 (52%)] Loss: 0.000232
Train Epoch: 27 [8960/17010 (53%)] Loss: 0.000062
Train Epoch: 27 [9120/17010 (54%)] Loss: 0.123851
Train Epoch: 27 [9280/17010 (55%)] Loss: 0.000120
Train Epoch: 27 [9440/17010 (55%)] Loss: 0.000279
Train Epoch: 27 [9600/17010 (56%)] Loss: 0.076333
Train Epoch: 27 [9760/17010 (57%)] Loss: 0.000022
Train Epoch: 27 [9920/17010 (58%)] Loss: 0.000228
Train Epoch: 27 [10080/17010 (59%)] Loss: 0.000070
Train Epoch: 27 [10240/17010 (60%)] Loss: 0.000324
Train Epoch: 27 [10400/17010 (61%)] Loss: 0.000069
Train Epoch: 27 [10560/17010 (62%)] Loss: 0.000988
Train Epoch: 27 [10720/17010 (63%)] Loss: 0.000154
Train Epoch: 27 [10880/17010 (64%)] Loss: 0.000633
Train Epoch: 27 [11040/17010 (65%)] Loss: 0.000095
Train Epoch: 27 [11200/17010 (66%)] Loss: 0.000047
Train Epoch: 27 [11360/17010 (67%)] Loss: 0.002903
Train Epoch: 27 [11520/17010 (68%)] Loss: 0.000278
Train Epoch: 27 [11680/17010 (69%)] Loss: 0.002051
Train Epoch: 27 [11840/17010 (70%)] Loss: 0.013437
Train Epoch: 27 [12000/17010 (71%)] Loss: 0.000041
Train Epoch: 27 [12160/17010 (71%)] Loss: 0.002901
Train Epoch: 27 [12320/17010 (72%)] Loss: 0.001149
Train Epoch: 27 [12480/17010 (73%)] Loss: 0.000149
Train Epoch: 27 [12640/17010 (74%)] Loss: 0.023850
Train Epoch: 27 [12800/17010 (75%)] Loss: 0.000144
Train Epoch: 27 [12960/17010 (76%)] Loss: 0.000124
Train Epoch: 27 [13120/17010 (77%)] Loss: 0.000333
Train Epoch: 27 [13280/17010 (78%)] Loss: 0.000291
Train Epoch: 27 [13440/17010 (79%)] Loss: 0.000306
Train Epoch: 27 [13600/17010 (80%)] Loss: 0.082851
Train Epoch: 27 [13760/17010 (81%)] Loss: 0.012554
Train Epoch: 27 [13920/17010 (82%)] Loss: 0.000084
Train Epoch: 27 [14080/17010 (83%)] Loss: 0.001701
Train Epoch: 27 [14240/17010 (84%)] Loss: 0.000513
Train Epoch: 27 [14400/17010 (85%)] Loss: 0.000504
Train Epoch: 27 [14560/17010 (86%)] Loss: 0.000693
Train Epoch: 27 [14720/17010 (87%)] Loss: 0.001765
Train Epoch: 27 [14880/17010 (87%)] Loss: 0.000293
Train Epoch: 27 [15040/17010 (88%)] Loss: 0.006336
Train Epoch: 27 [15200/17010 (89%)] Loss: 0.063951
Train Epoch: 27 [15360/17010 (90%)] Loss: 0.001745
Train Epoch: 27 [15520/17010 (91%)] Loss: 0.014369
Train Epoch: 27 [15680/17010 (92%)] Loss: 0.000382
Train Epoch: 27 [15840/17010 (93%)] Loss: 0.052268
Train Epoch: 27 [16000/17010 (94%)] Loss: 0.011376
Train Epoch: 27 [16160/17010 (95%)] Loss: 0.000112
Train Epoch: 27 [16320/17010 (96%)] Loss: 0.000588
Train Epoch: 27 [16480/17010 (97%)] Loss: 0.000194
Train Epoch: 27 [16640/17010 (98%)] Loss: 0.005033
Train Epoch: 27 [16800/17010 (99%)] Loss: 0.005318
Train Epoch: 27 [16960/17010 (100%)] Loss: 0.067513
    epoch          : 27
    Train_loss     : 0.006142715019016174
    Train_accuracy : 0.998061560150376
    Train_f1_score : 0.9980615973472595
    Val_loss       : 0.06568386142473628
    Val_accuracy   : 0.9802083333333333
    Val_f1_score   : 0.9802083969116211
Train Epoch: 28 [0/17010 (0%)] Loss: 0.019551
Train Epoch: 28 [160/17010 (1%)] Loss: 0.006912
Train Epoch: 28 [320/17010 (2%)] Loss: 0.051682
Train Epoch: 28 [480/17010 (3%)] Loss: 0.001139
Train Epoch: 28 [640/17010 (4%)] Loss: 0.000220
Train Epoch: 28 [800/17010 (5%)] Loss: 0.000677
Train Epoch: 28 [960/17010 (6%)] Loss: 0.001930
Train Epoch: 28 [1120/17010 (7%)] Loss: 0.012936
Train Epoch: 28 [1280/17010 (8%)] Loss: 0.000171
Train Epoch: 28 [1440/17010 (8%)] Loss: 0.000083
Train Epoch: 28 [1600/17010 (9%)] Loss: 0.002430
Train Epoch: 28 [1760/17010 (10%)] Loss: 0.000320
Train Epoch: 28 [1920/17010 (11%)] Loss: 0.002571
Train Epoch: 28 [2080/17010 (12%)] Loss: 0.001916
Train Epoch: 28 [2240/17010 (13%)] Loss: 0.001394
Train Epoch: 28 [2400/17010 (14%)] Loss: 0.000090
Train Epoch: 28 [2560/17010 (15%)] Loss: 0.000406
Train Epoch: 28 [2720/17010 (16%)] Loss: 0.000463
Train Epoch: 28 [2880/17010 (17%)] Loss: 0.000047
Train Epoch: 28 [3040/17010 (18%)] Loss: 0.000211
Train Epoch: 28 [3200/17010 (19%)] Loss: 0.000215
Train Epoch: 28 [3360/17010 (20%)] Loss: 0.004717
Train Epoch: 28 [3520/17010 (21%)] Loss: 0.008955
Train Epoch: 28 [3680/17010 (22%)] Loss: 0.000096
Train Epoch: 28 [3840/17010 (23%)] Loss: 0.002852
Train Epoch: 28 [4000/17010 (24%)] Loss: 0.000290
Train Epoch: 28 [4160/17010 (24%)] Loss: 0.000675
Train Epoch: 28 [4320/17010 (25%)] Loss: 0.000715
Train Epoch: 28 [4480/17010 (26%)] Loss: 0.025991
Train Epoch: 28 [4640/17010 (27%)] Loss: 0.003491
Train Epoch: 28 [4800/17010 (28%)] Loss: 0.004591
Train Epoch: 28 [4960/17010 (29%)] Loss: 0.000554
Train Epoch: 28 [5120/17010 (30%)] Loss: 0.002158
Train Epoch: 28 [5280/17010 (31%)] Loss: 0.000081
Train Epoch: 28 [5440/17010 (32%)] Loss: 0.000310
Train Epoch: 28 [5600/17010 (33%)] Loss: 0.001914
Train Epoch: 28 [5760/17010 (34%)] Loss: 0.000059
Train Epoch: 28 [5920/17010 (35%)] Loss: 0.000195
Train Epoch: 28 [6080/17010 (36%)] Loss: 0.011019
Train Epoch: 28 [6240/17010 (37%)] Loss: 0.001264
Train Epoch: 28 [6400/17010 (38%)] Loss: 0.000553
Train Epoch: 28 [6560/17010 (39%)] Loss: 0.000179
Train Epoch: 28 [6720/17010 (40%)] Loss: 0.000252
Train Epoch: 28 [6880/17010 (40%)] Loss: 0.000187
Train Epoch: 28 [7040/17010 (41%)] Loss: 0.000731
Train Epoch: 28 [7200/17010 (42%)] Loss: 0.000264
Train Epoch: 28 [7360/17010 (43%)] Loss: 0.000812
Train Epoch: 28 [7520/17010 (44%)] Loss: 0.000635
Train Epoch: 28 [7680/17010 (45%)] Loss: 0.000120
Train Epoch: 28 [7840/17010 (46%)] Loss: 0.000234
Train Epoch: 28 [8000/17010 (47%)] Loss: 0.000110
Train Epoch: 28 [8160/17010 (48%)] Loss: 0.000039
Train Epoch: 28 [8320/17010 (49%)] Loss: 0.001043
Train Epoch: 28 [8480/17010 (50%)] Loss: 0.001073
Train Epoch: 28 [8640/17010 (51%)] Loss: 0.000237
Train Epoch: 28 [8800/17010 (52%)] Loss: 0.023805
Train Epoch: 28 [8960/17010 (53%)] Loss: 0.005506
Train Epoch: 28 [9120/17010 (54%)] Loss: 0.000191
Train Epoch: 28 [9280/17010 (55%)] Loss: 0.000186
Train Epoch: 28 [9440/17010 (55%)] Loss: 0.065640
Train Epoch: 28 [9600/17010 (56%)] Loss: 0.000185
Train Epoch: 28 [9760/17010 (57%)] Loss: 0.000808
Train Epoch: 28 [9920/17010 (58%)] Loss: 0.000078
Train Epoch: 28 [10080/17010 (59%)] Loss: 0.000113
Train Epoch: 28 [10240/17010 (60%)] Loss: 0.000021
Train Epoch: 28 [10400/17010 (61%)] Loss: 0.303393
Train Epoch: 28 [10560/17010 (62%)] Loss: 0.000045
Train Epoch: 28 [10720/17010 (63%)] Loss: 0.000162
Train Epoch: 28 [10880/17010 (64%)] Loss: 0.000197
Train Epoch: 28 [11040/17010 (65%)] Loss: 0.000062
Train Epoch: 28 [11200/17010 (66%)] Loss: 0.000053
Train Epoch: 28 [11360/17010 (67%)] Loss: 0.004681
Train Epoch: 28 [11520/17010 (68%)] Loss: 0.000469
Train Epoch: 28 [11680/17010 (69%)] Loss: 0.000297
Train Epoch: 28 [11840/17010 (70%)] Loss: 0.042494
Train Epoch: 28 [12000/17010 (71%)] Loss: 0.000135
Train Epoch: 28 [12160/17010 (71%)] Loss: 0.000174
Train Epoch: 28 [12320/17010 (72%)] Loss: 0.000044
Train Epoch: 28 [12480/17010 (73%)] Loss: 0.000120
Train Epoch: 28 [12640/17010 (74%)] Loss: 0.002253
Train Epoch: 28 [12800/17010 (75%)] Loss: 0.000418
Train Epoch: 28 [12960/17010 (76%)] Loss: 0.000436
Train Epoch: 28 [13120/17010 (77%)] Loss: 0.000216
Train Epoch: 28 [13280/17010 (78%)] Loss: 0.001887
Train Epoch: 28 [13440/17010 (79%)] Loss: 0.000361
Train Epoch: 28 [13600/17010 (80%)] Loss: 0.000325
Train Epoch: 28 [13760/17010 (81%)] Loss: 0.003830
Train Epoch: 28 [13920/17010 (82%)] Loss: 0.000027
Train Epoch: 28 [14080/17010 (83%)] Loss: 0.014259
Train Epoch: 28 [14240/17010 (84%)] Loss: 0.000529
Train Epoch: 28 [14400/17010 (85%)] Loss: 0.003801
Train Epoch: 28 [14560/17010 (86%)] Loss: 0.017186
Train Epoch: 28 [14720/17010 (87%)] Loss: 0.002140
Train Epoch: 28 [14880/17010 (87%)] Loss: 0.000357
Train Epoch: 28 [15040/17010 (88%)] Loss: 0.000053
Train Epoch: 28 [15200/17010 (89%)] Loss: 0.000032
Train Epoch: 28 [15360/17010 (90%)] Loss: 0.005056
Train Epoch: 28 [15520/17010 (91%)] Loss: 0.004004
Train Epoch: 28 [15680/17010 (92%)] Loss: 0.008089
Train Epoch: 28 [15840/17010 (93%)] Loss: 0.006873
Train Epoch: 28 [16000/17010 (94%)] Loss: 0.001110
Train Epoch: 28 [16160/17010 (95%)] Loss: 0.000069
Train Epoch: 28 [16320/17010 (96%)] Loss: 0.000062
Train Epoch: 28 [16480/17010 (97%)] Loss: 0.002389
Train Epoch: 28 [16640/17010 (98%)] Loss: 0.000799
Train Epoch: 28 [16800/17010 (99%)] Loss: 0.000031
Train Epoch: 28 [16960/17010 (100%)] Loss: 0.020597
    epoch          : 28
    Train_loss     : 0.005289000605242286
    Train_accuracy : 0.9983552631578947
    Train_f1_score : 0.9983552694320679
    Val_loss       : 0.021108816448637904
    Val_accuracy   : 0.9932291666666667
    Val_f1_score   : 0.9932292103767395
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch28.pth ...
Train Epoch: 29 [0/17010 (0%)] Loss: 0.006219
Train Epoch: 29 [160/17010 (1%)] Loss: 0.000160
Train Epoch: 29 [320/17010 (2%)] Loss: 0.000118
Train Epoch: 29 [480/17010 (3%)] Loss: 0.000482
Train Epoch: 29 [640/17010 (4%)] Loss: 0.000137
Train Epoch: 29 [800/17010 (5%)] Loss: 0.000110
Train Epoch: 29 [960/17010 (6%)] Loss: 0.000412
Train Epoch: 29 [1120/17010 (7%)] Loss: 0.000264
Train Epoch: 29 [1280/17010 (8%)] Loss: 0.000158
Train Epoch: 29 [1440/17010 (8%)] Loss: 0.000350
Train Epoch: 29 [1600/17010 (9%)] Loss: 0.000179
Train Epoch: 29 [1760/17010 (10%)] Loss: 0.002831
Train Epoch: 29 [1920/17010 (11%)] Loss: 0.005796
Train Epoch: 29 [2080/17010 (12%)] Loss: 0.000080
Train Epoch: 29 [2240/17010 (13%)] Loss: 0.002522
Train Epoch: 29 [2400/17010 (14%)] Loss: 0.000283
Train Epoch: 29 [2560/17010 (15%)] Loss: 0.000125
Train Epoch: 29 [2720/17010 (16%)] Loss: 0.000103
Train Epoch: 29 [2880/17010 (17%)] Loss: 0.000496
Train Epoch: 29 [3040/17010 (18%)] Loss: 0.000140
Train Epoch: 29 [3200/17010 (19%)] Loss: 0.000197
Train Epoch: 29 [3360/17010 (20%)] Loss: 0.061500
Train Epoch: 29 [3520/17010 (21%)] Loss: 0.000079
Train Epoch: 29 [3680/17010 (22%)] Loss: 0.000185
Train Epoch: 29 [3840/17010 (23%)] Loss: 0.000941
Train Epoch: 29 [4000/17010 (24%)] Loss: 0.000330
Train Epoch: 29 [4160/17010 (24%)] Loss: 0.001936
Train Epoch: 29 [4320/17010 (25%)] Loss: 0.000083
Train Epoch: 29 [4480/17010 (26%)] Loss: 0.002046
Train Epoch: 29 [4640/17010 (27%)] Loss: 0.001531
Train Epoch: 29 [4800/17010 (28%)] Loss: 0.000309
Train Epoch: 29 [4960/17010 (29%)] Loss: 0.000192
Train Epoch: 29 [5120/17010 (30%)] Loss: 0.000221
Train Epoch: 29 [5280/17010 (31%)] Loss: 0.000067
Train Epoch: 29 [5440/17010 (32%)] Loss: 0.000365
Train Epoch: 29 [5600/17010 (33%)] Loss: 0.000053
Train Epoch: 29 [5760/17010 (34%)] Loss: 0.000321
Train Epoch: 29 [5920/17010 (35%)] Loss: 0.000062
Train Epoch: 29 [6080/17010 (36%)] Loss: 0.000102
Train Epoch: 29 [6240/17010 (37%)] Loss: 0.000184
Train Epoch: 29 [6400/17010 (38%)] Loss: 0.004243
Train Epoch: 29 [6560/17010 (39%)] Loss: 0.000029
Train Epoch: 29 [6720/17010 (40%)] Loss: 0.000121
Train Epoch: 29 [6880/17010 (40%)] Loss: 0.000090
Train Epoch: 29 [7040/17010 (41%)] Loss: 0.000254
Train Epoch: 29 [7200/17010 (42%)] Loss: 0.000325
Train Epoch: 29 [7360/17010 (43%)] Loss: 0.000063
Train Epoch: 29 [7520/17010 (44%)] Loss: 0.000182
Train Epoch: 29 [7680/17010 (45%)] Loss: 0.000159
Train Epoch: 29 [7840/17010 (46%)] Loss: 0.000034
Train Epoch: 29 [8000/17010 (47%)] Loss: 0.000143
Train Epoch: 29 [8160/17010 (48%)] Loss: 0.003295
Train Epoch: 29 [8320/17010 (49%)] Loss: 0.000030
Train Epoch: 29 [8480/17010 (50%)] Loss: 0.001818
Train Epoch: 29 [8640/17010 (51%)] Loss: 0.000031
Train Epoch: 29 [8800/17010 (52%)] Loss: 0.000203
Train Epoch: 29 [8960/17010 (53%)] Loss: 0.000057
Train Epoch: 29 [9120/17010 (54%)] Loss: 0.000076
Train Epoch: 29 [9280/17010 (55%)] Loss: 0.000287
Train Epoch: 29 [9440/17010 (55%)] Loss: 0.000051
Train Epoch: 29 [9600/17010 (56%)] Loss: 0.000180
Train Epoch: 29 [9760/17010 (57%)] Loss: 0.000130
Train Epoch: 29 [9920/17010 (58%)] Loss: 0.000062
Train Epoch: 29 [10080/17010 (59%)] Loss: 0.000122
Train Epoch: 29 [10240/17010 (60%)] Loss: 0.000071
Train Epoch: 29 [10400/17010 (61%)] Loss: 0.000301
Train Epoch: 29 [10560/17010 (62%)] Loss: 0.000242
Train Epoch: 29 [10720/17010 (63%)] Loss: 0.000041
Train Epoch: 29 [10880/17010 (64%)] Loss: 0.000250
Train Epoch: 29 [11040/17010 (65%)] Loss: 0.000053
Train Epoch: 29 [11200/17010 (66%)] Loss: 0.000076
Train Epoch: 29 [11360/17010 (67%)] Loss: 0.000095
Train Epoch: 29 [11520/17010 (68%)] Loss: 0.000276
Train Epoch: 29 [11680/17010 (69%)] Loss: 0.000043
Train Epoch: 29 [11840/17010 (70%)] Loss: 0.000043
Train Epoch: 29 [12000/17010 (71%)] Loss: 0.000051
Train Epoch: 29 [12160/17010 (71%)] Loss: 0.000045
Train Epoch: 29 [12320/17010 (72%)] Loss: 0.000146
Train Epoch: 29 [12480/17010 (73%)] Loss: 0.000381
Train Epoch: 29 [12640/17010 (74%)] Loss: 0.009586
Train Epoch: 29 [12800/17010 (75%)] Loss: 0.000193
Train Epoch: 29 [12960/17010 (76%)] Loss: 0.000519
Train Epoch: 29 [13120/17010 (77%)] Loss: 0.000076
Train Epoch: 29 [13280/17010 (78%)] Loss: 0.000112
Train Epoch: 29 [13440/17010 (79%)] Loss: 0.000231
Train Epoch: 29 [13600/17010 (80%)] Loss: 0.000119
Train Epoch: 29 [13760/17010 (81%)] Loss: 0.000068
Train Epoch: 29 [13920/17010 (82%)] Loss: 0.000219
Train Epoch: 29 [14080/17010 (83%)] Loss: 0.008491
Train Epoch: 29 [14240/17010 (84%)] Loss: 0.051264
Train Epoch: 29 [14400/17010 (85%)] Loss: 0.000181
Train Epoch: 29 [14560/17010 (86%)] Loss: 0.003097
Train Epoch: 29 [14720/17010 (87%)] Loss: 0.000426
Train Epoch: 29 [14880/17010 (87%)] Loss: 0.000092
Train Epoch: 29 [15040/17010 (88%)] Loss: 0.000647
Train Epoch: 29 [15200/17010 (89%)] Loss: 0.000437
Train Epoch: 29 [15360/17010 (90%)] Loss: 0.000075
Train Epoch: 29 [15520/17010 (91%)] Loss: 0.000317
Train Epoch: 29 [15680/17010 (92%)] Loss: 0.001392
Train Epoch: 29 [15840/17010 (93%)] Loss: 0.000082
Train Epoch: 29 [16000/17010 (94%)] Loss: 0.000051
Train Epoch: 29 [16160/17010 (95%)] Loss: 0.000027
Train Epoch: 29 [16320/17010 (96%)] Loss: 0.037943
Train Epoch: 29 [16480/17010 (97%)] Loss: 0.000242
Train Epoch: 29 [16640/17010 (98%)] Loss: 0.008233
Train Epoch: 29 [16800/17010 (99%)] Loss: 0.000098
Train Epoch: 29 [16960/17010 (100%)] Loss: 0.000427
    epoch          : 29
    Train_loss     : 0.0019302010804293055
    Train_accuracy : 0.9994125939849624
    Train_f1_score : 0.9994125962257385
    Val_loss       : 0.012956385534600183
    Val_accuracy   : 0.9958333333333333
    Val_f1_score   : 0.9958333969116211
Train Epoch: 30 [0/17010 (0%)] Loss: 0.000158
Train Epoch: 30 [160/17010 (1%)] Loss: 0.001049
Train Epoch: 30 [320/17010 (2%)] Loss: 0.000083
Train Epoch: 30 [480/17010 (3%)] Loss: 0.000228
Train Epoch: 30 [640/17010 (4%)] Loss: 0.001516
Train Epoch: 30 [800/17010 (5%)] Loss: 0.000044
Train Epoch: 30 [960/17010 (6%)] Loss: 0.000122
Train Epoch: 30 [1120/17010 (7%)] Loss: 0.000078
Train Epoch: 30 [1280/17010 (8%)] Loss: 0.000235
Train Epoch: 30 [1440/17010 (8%)] Loss: 0.021353
Train Epoch: 30 [1600/17010 (9%)] Loss: 0.000071
Train Epoch: 30 [1760/17010 (10%)] Loss: 0.000274
Train Epoch: 30 [1920/17010 (11%)] Loss: 0.000417
Train Epoch: 30 [2080/17010 (12%)] Loss: 0.000154
Train Epoch: 30 [2240/17010 (13%)] Loss: 0.000301
Train Epoch: 30 [2400/17010 (14%)] Loss: 0.000563
Train Epoch: 30 [2560/17010 (15%)] Loss: 0.000156
Train Epoch: 30 [2720/17010 (16%)] Loss: 0.000705
Train Epoch: 30 [2880/17010 (17%)] Loss: 0.000265
Train Epoch: 30 [3040/17010 (18%)] Loss: 0.000132
Train Epoch: 30 [3200/17010 (19%)] Loss: 0.000090
Train Epoch: 30 [3360/17010 (20%)] Loss: 0.000253
Train Epoch: 30 [3520/17010 (21%)] Loss: 0.000095
Train Epoch: 30 [3680/17010 (22%)] Loss: 0.000034
Train Epoch: 30 [3840/17010 (23%)] Loss: 0.055667
Train Epoch: 30 [4000/17010 (24%)] Loss: 0.000225
Train Epoch: 30 [4160/17010 (24%)] Loss: 0.000227
Train Epoch: 30 [4320/17010 (25%)] Loss: 0.001127
Train Epoch: 30 [4480/17010 (26%)] Loss: 0.001659
Train Epoch: 30 [4640/17010 (27%)] Loss: 0.002037
Train Epoch: 30 [4800/17010 (28%)] Loss: 0.001246
Train Epoch: 30 [4960/17010 (29%)] Loss: 0.000438
Train Epoch: 30 [5120/17010 (30%)] Loss: 0.120520
Train Epoch: 30 [5280/17010 (31%)] Loss: 0.000058
Train Epoch: 30 [5440/17010 (32%)] Loss: 0.000103
Train Epoch: 30 [5600/17010 (33%)] Loss: 0.007611
Train Epoch: 30 [5760/17010 (34%)] Loss: 0.112752
Train Epoch: 30 [5920/17010 (35%)] Loss: 0.000336
Train Epoch: 30 [6080/17010 (36%)] Loss: 0.000032
Train Epoch: 30 [6240/17010 (37%)] Loss: 0.000012
Train Epoch: 30 [6400/17010 (38%)] Loss: 0.000444
Train Epoch: 30 [6560/17010 (39%)] Loss: 0.000093
Train Epoch: 30 [6720/17010 (40%)] Loss: 0.002105
Train Epoch: 30 [6880/17010 (40%)] Loss: 0.000275
Train Epoch: 30 [7040/17010 (41%)] Loss: 0.000740
Train Epoch: 30 [7200/17010 (42%)] Loss: 0.000241
Train Epoch: 30 [7360/17010 (43%)] Loss: 0.001609
Train Epoch: 30 [7520/17010 (44%)] Loss: 0.017132
Train Epoch: 30 [7680/17010 (45%)] Loss: 0.023890
Train Epoch: 30 [7840/17010 (46%)] Loss: 0.000829
Train Epoch: 30 [8000/17010 (47%)] Loss: 0.000163
Train Epoch: 30 [8160/17010 (48%)] Loss: 0.000536
Train Epoch: 30 [8320/17010 (49%)] Loss: 0.001107
Train Epoch: 30 [8480/17010 (50%)] Loss: 0.000507
Train Epoch: 30 [8640/17010 (51%)] Loss: 0.008664
Train Epoch: 30 [8800/17010 (52%)] Loss: 0.018418
Train Epoch: 30 [8960/17010 (53%)] Loss: 0.002195
Train Epoch: 30 [9120/17010 (54%)] Loss: 0.002203
Train Epoch: 30 [9280/17010 (55%)] Loss: 0.035544
Train Epoch: 30 [9440/17010 (55%)] Loss: 0.002911
Train Epoch: 30 [9600/17010 (56%)] Loss: 0.000533
Train Epoch: 30 [9760/17010 (57%)] Loss: 0.000391
Train Epoch: 30 [9920/17010 (58%)] Loss: 0.000525
Train Epoch: 30 [10080/17010 (59%)] Loss: 0.000180
Train Epoch: 30 [10240/17010 (60%)] Loss: 0.004484
Train Epoch: 30 [10400/17010 (61%)] Loss: 0.005056
Train Epoch: 30 [10560/17010 (62%)] Loss: 0.016859
Train Epoch: 30 [10720/17010 (63%)] Loss: 0.000799
Train Epoch: 30 [10880/17010 (64%)] Loss: 0.000411
Train Epoch: 30 [11040/17010 (65%)] Loss: 0.000229
Train Epoch: 30 [11200/17010 (66%)] Loss: 0.002283
Train Epoch: 30 [11360/17010 (67%)] Loss: 0.000780
Train Epoch: 30 [11520/17010 (68%)] Loss: 0.000089
Train Epoch: 30 [11680/17010 (69%)] Loss: 0.000646
Train Epoch: 30 [11840/17010 (70%)] Loss: 0.006754
Train Epoch: 30 [12000/17010 (71%)] Loss: 0.001049
Train Epoch: 30 [12160/17010 (71%)] Loss: 0.000320
Train Epoch: 30 [12320/17010 (72%)] Loss: 0.000194
Train Epoch: 30 [12480/17010 (73%)] Loss: 0.001211
Train Epoch: 30 [12640/17010 (74%)] Loss: 0.012758
Train Epoch: 30 [12800/17010 (75%)] Loss: 0.000511
Train Epoch: 30 [12960/17010 (76%)] Loss: 0.002465
Train Epoch: 30 [13120/17010 (77%)] Loss: 0.142313
Train Epoch: 30 [13280/17010 (78%)] Loss: 0.007060
Train Epoch: 30 [13440/17010 (79%)] Loss: 0.005056
Train Epoch: 30 [13600/17010 (80%)] Loss: 0.025519
Train Epoch: 30 [13760/17010 (81%)] Loss: 0.000683
Train Epoch: 30 [13920/17010 (82%)] Loss: 0.061484
Train Epoch: 30 [14080/17010 (83%)] Loss: 0.046447
Train Epoch: 30 [14240/17010 (84%)] Loss: 0.018498
Train Epoch: 30 [14400/17010 (85%)] Loss: 0.005733
Train Epoch: 30 [14560/17010 (86%)] Loss: 0.001556
Train Epoch: 30 [14720/17010 (87%)] Loss: 0.005855
Train Epoch: 30 [14880/17010 (87%)] Loss: 0.024671
Train Epoch: 30 [15040/17010 (88%)] Loss: 0.020992
Train Epoch: 30 [15200/17010 (89%)] Loss: 0.000383
Train Epoch: 30 [15360/17010 (90%)] Loss: 0.213606
Train Epoch: 30 [15520/17010 (91%)] Loss: 0.000717
Train Epoch: 30 [15680/17010 (92%)] Loss: 0.007333
Train Epoch: 30 [15840/17010 (93%)] Loss: 0.000924
Train Epoch: 30 [16000/17010 (94%)] Loss: 0.000987
Train Epoch: 30 [16160/17010 (95%)] Loss: 0.003981
Train Epoch: 30 [16320/17010 (96%)] Loss: 0.003547
Train Epoch: 30 [16480/17010 (97%)] Loss: 0.000223
Train Epoch: 30 [16640/17010 (98%)] Loss: 0.000475
Train Epoch: 30 [16800/17010 (99%)] Loss: 0.001786
Train Epoch: 30 [16960/17010 (100%)] Loss: 0.001743
    epoch          : 30
    Train_loss     : 0.013224808691055452
    Train_accuracy : 0.9964755639097744
    Train_f1_score : 0.9964755773544312
    Val_loss       : 0.051120237859322515
    Val_accuracy   : 0.9864583333333333
    Val_f1_score   : 0.9864583611488342
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1029_095836/checkpoint-epoch30.pth ...
/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.234 MB uploaded (0.000 MB deduped)wandb: \ 0.234 MB of 0.234 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: Train_accuracy ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: Train_f1_score ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:     Train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   Val_accuracy ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá
wandb:   Val_f1_score ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá
wandb:       Val_loss ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: Train_accuracy 0.99648
wandb: Train_f1_score 0.99648
wandb:     Train_loss 0.01322
wandb:   Val_accuracy 0.98646
wandb:   Val_f1_score 0.98646
wandb:       Val_loss 0.05112
wandb: 
wandb: Synced devout-brook-87: https://wandb.ai/qwer55252/Boostcamp-lv1-cv1/runs/covsy863
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221029_095831-covsy863/logs
