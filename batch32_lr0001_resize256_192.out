/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Currently logged in as: qwer55252. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /opt/ml/project-T4193/wandb/run-20221030_145345-chg2ogai
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bewitched-ghost-91
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1
wandb: üöÄ View run at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1/runs/chg2ogai
Loaded pretrained weights for efficientnet-b7
EfficientNet(
  (_conv_stem): Conv2dStaticSamePadding(
    3, 64, kernel_size=(3, 3), stride=(2, 2), bias=False
    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
  )
  (_bn0): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_blocks): ModuleList(
    (0): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        64, 64, kernel_size=(3, 3), stride=[1, 1], groups=64, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        64, 16, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        16, 64, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (3): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (4): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        192, 192, kernel_size=(3, 3), stride=[2, 2], groups=192, bias=False
        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        192, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 192, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (5): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (6): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (7): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (8): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (9): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (10): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (11): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(5, 5), stride=[2, 2], groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (12): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (13): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (14): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (15): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (16): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (17): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (18): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(3, 3), stride=[2, 2], groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (19): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (20): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (21): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (22): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (23): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (24): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (25): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (26): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (27): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (28): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=[1, 1], groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (29): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (30): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (31): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (32): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (33): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (34): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (35): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (36): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (37): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (38): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=[2, 2], groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (39): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (40): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (41): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (42): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (43): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (44): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (45): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (46): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (47): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (48): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (49): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (50): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (51): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(3, 3), stride=[1, 1], groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (52): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (53): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (54): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (_conv_head): Conv2dStaticSamePadding(
    640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False
    (static_padding): Identity()
  )
  (_bn1): BatchNorm2d(2560, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)
  (_dropout): Dropout(p=0.5, inplace=False)
  (_fc): Linear(in_features=2560, out_features=18, bias=True)
  (_swish): MemoryEfficientSwish()
)
Train Epoch: 1 [0/19430 (0%)] Loss: 2.895639
Train Epoch: 1 [160/19430 (1%)] Loss: 2.843188
Train Epoch: 1 [320/19430 (2%)] Loss: 2.761547
Train Epoch: 1 [480/19430 (2%)] Loss: 2.666362
Train Epoch: 1 [640/19430 (3%)] Loss: 2.520099
Train Epoch: 1 [800/19430 (4%)] Loss: 2.420271
Train Epoch: 1 [960/19430 (5%)] Loss: 2.197664
Train Epoch: 1 [1120/19430 (6%)] Loss: 2.106901
Train Epoch: 1 [1280/19430 (7%)] Loss: 1.642504
Train Epoch: 1 [1440/19430 (7%)] Loss: 1.799132
Train Epoch: 1 [1600/19430 (8%)] Loss: 1.475334
Train Epoch: 1 [1760/19430 (9%)] Loss: 1.628740
Train Epoch: 1 [1920/19430 (10%)] Loss: 1.514994
Train Epoch: 1 [2080/19430 (11%)] Loss: 1.125674
Train Epoch: 1 [2240/19430 (12%)] Loss: 1.262482
Train Epoch: 1 [2400/19430 (12%)] Loss: 1.219638
Train Epoch: 1 [2560/19430 (13%)] Loss: 0.936244
Train Epoch: 1 [2720/19430 (14%)] Loss: 0.895449
Train Epoch: 1 [2880/19430 (15%)] Loss: 0.797761
Train Epoch: 1 [3040/19430 (16%)] Loss: 0.794341
Train Epoch: 1 [3200/19430 (16%)] Loss: 0.751065
Train Epoch: 1 [3360/19430 (17%)] Loss: 0.545939
Train Epoch: 1 [3520/19430 (18%)] Loss: 0.844909
Train Epoch: 1 [3680/19430 (19%)] Loss: 0.447361
Train Epoch: 1 [3840/19430 (20%)] Loss: 0.587295
Train Epoch: 1 [4000/19430 (21%)] Loss: 0.737036
Train Epoch: 1 [4160/19430 (21%)] Loss: 0.694707
Train Epoch: 1 [4320/19430 (22%)] Loss: 0.677197
Train Epoch: 1 [4480/19430 (23%)] Loss: 0.981821
Train Epoch: 1 [4640/19430 (24%)] Loss: 0.837903
Train Epoch: 1 [4800/19430 (25%)] Loss: 0.742807
Train Epoch: 1 [4960/19430 (26%)] Loss: 0.463782
Train Epoch: 1 [5120/19430 (26%)] Loss: 0.508478
Train Epoch: 1 [5280/19430 (27%)] Loss: 0.659052
Train Epoch: 1 [5440/19430 (28%)] Loss: 0.565025
Train Epoch: 1 [5600/19430 (29%)] Loss: 0.412945
Train Epoch: 1 [5760/19430 (30%)] Loss: 0.437374
Train Epoch: 1 [5920/19430 (30%)] Loss: 0.324724
Train Epoch: 1 [6080/19430 (31%)] Loss: 0.513693
Train Epoch: 1 [6240/19430 (32%)] Loss: 0.564644
Train Epoch: 1 [6400/19430 (33%)] Loss: 0.288184
Train Epoch: 1 [6560/19430 (34%)] Loss: 0.389379
Train Epoch: 1 [6720/19430 (35%)] Loss: 0.344599
Train Epoch: 1 [6880/19430 (35%)] Loss: 0.571070
Train Epoch: 1 [7040/19430 (36%)] Loss: 0.745270
Train Epoch: 1 [7200/19430 (37%)] Loss: 0.319348
Train Epoch: 1 [7360/19430 (38%)] Loss: 0.621648
Train Epoch: 1 [7520/19430 (39%)] Loss: 0.262150
Train Epoch: 1 [7680/19430 (40%)] Loss: 0.357869
Train Epoch: 1 [7840/19430 (40%)] Loss: 0.391170
Train Epoch: 1 [8000/19430 (41%)] Loss: 0.355309
Train Epoch: 1 [8160/19430 (42%)] Loss: 0.365055
Train Epoch: 1 [8320/19430 (43%)] Loss: 0.479726
Train Epoch: 1 [8480/19430 (44%)] Loss: 0.177751
Train Epoch: 1 [8640/19430 (44%)] Loss: 0.448316
Train Epoch: 1 [8800/19430 (45%)] Loss: 0.251014
Train Epoch: 1 [8960/19430 (46%)] Loss: 0.223147
Train Epoch: 1 [9120/19430 (47%)] Loss: 0.443316
Train Epoch: 1 [9280/19430 (48%)] Loss: 0.266924
Train Epoch: 1 [9440/19430 (49%)] Loss: 0.241960
Train Epoch: 1 [9600/19430 (49%)] Loss: 0.156089
Train Epoch: 1 [9760/19430 (50%)] Loss: 0.367027
Train Epoch: 1 [9920/19430 (51%)] Loss: 0.209991
Train Epoch: 1 [10080/19430 (52%)] Loss: 0.410137
Train Epoch: 1 [10240/19430 (53%)] Loss: 0.458568
Train Epoch: 1 [10400/19430 (54%)] Loss: 0.313579
Train Epoch: 1 [10560/19430 (54%)] Loss: 0.321232
Train Epoch: 1 [10720/19430 (55%)] Loss: 0.424298
Train Epoch: 1 [10880/19430 (56%)] Loss: 0.590958
Train Epoch: 1 [11040/19430 (57%)] Loss: 0.350246
Train Epoch: 1 [11200/19430 (58%)] Loss: 0.541294
Train Epoch: 1 [11360/19430 (58%)] Loss: 0.325789
Train Epoch: 1 [11520/19430 (59%)] Loss: 0.186950
Train Epoch: 1 [11680/19430 (60%)] Loss: 0.107376
Train Epoch: 1 [11840/19430 (61%)] Loss: 0.256595
Train Epoch: 1 [12000/19430 (62%)] Loss: 0.424346
Train Epoch: 1 [12160/19430 (63%)] Loss: 0.389036
Train Epoch: 1 [12320/19430 (63%)] Loss: 0.214107
Train Epoch: 1 [12480/19430 (64%)] Loss: 0.207245
Train Epoch: 1 [12640/19430 (65%)] Loss: 0.076460
Train Epoch: 1 [12800/19430 (66%)] Loss: 0.517383
Train Epoch: 1 [12960/19430 (67%)] Loss: 0.131999
Train Epoch: 1 [13120/19430 (68%)] Loss: 0.294811
Train Epoch: 1 [13280/19430 (68%)] Loss: 0.367336
Train Epoch: 1 [13440/19430 (69%)] Loss: 0.218592
Train Epoch: 1 [13600/19430 (70%)] Loss: 0.129152
Train Epoch: 1 [13760/19430 (71%)] Loss: 0.208212
Train Epoch: 1 [13920/19430 (72%)] Loss: 0.315288
Train Epoch: 1 [14080/19430 (72%)] Loss: 0.242152
Train Epoch: 1 [14240/19430 (73%)] Loss: 0.279112
Train Epoch: 1 [14400/19430 (74%)] Loss: 0.091014
Train Epoch: 1 [14560/19430 (75%)] Loss: 0.400648
Train Epoch: 1 [14720/19430 (76%)] Loss: 0.336718
Train Epoch: 1 [14880/19430 (77%)] Loss: 0.188703
Train Epoch: 1 [15040/19430 (77%)] Loss: 0.061137
Train Epoch: 1 [15200/19430 (78%)] Loss: 0.308092
Train Epoch: 1 [15360/19430 (79%)] Loss: 0.484861
Train Epoch: 1 [15520/19430 (80%)] Loss: 0.446310
Train Epoch: 1 [15680/19430 (81%)] Loss: 0.317327
Train Epoch: 1 [15840/19430 (82%)] Loss: 0.346093
Train Epoch: 1 [16000/19430 (82%)] Loss: 0.203633
Train Epoch: 1 [16160/19430 (83%)] Loss: 0.417023
Train Epoch: 1 [16320/19430 (84%)] Loss: 0.143544
Train Epoch: 1 [16480/19430 (85%)] Loss: 0.093425
Train Epoch: 1 [16640/19430 (86%)] Loss: 0.228080
Train Epoch: 1 [16800/19430 (86%)] Loss: 0.192625
Train Epoch: 1 [16960/19430 (87%)] Loss: 0.163956
Train Epoch: 1 [17120/19430 (88%)] Loss: 0.071663
Train Epoch: 1 [17280/19430 (89%)] Loss: 0.436210
Train Epoch: 1 [17440/19430 (90%)] Loss: 0.151516
Train Epoch: 1 [17600/19430 (91%)] Loss: 0.046804
Train Epoch: 1 [17760/19430 (91%)] Loss: 0.173266
Train Epoch: 1 [17920/19430 (92%)] Loss: 0.063042
Train Epoch: 1 [18080/19430 (93%)] Loss: 0.263902
Train Epoch: 1 [18240/19430 (94%)] Loss: 0.061343
Train Epoch: 1 [18400/19430 (95%)] Loss: 0.368054
Train Epoch: 1 [18560/19430 (96%)] Loss: 0.154270
Train Epoch: 1 [18720/19430 (96%)] Loss: 0.129277
Train Epoch: 1 [18880/19430 (97%)] Loss: 0.059189
Train Epoch: 1 [19040/19430 (98%)] Loss: 0.101910
Train Epoch: 1 [19200/19430 (99%)] Loss: 0.196176
Train Epoch: 1 [19360/19430 (100%)] Loss: 0.363339
    epoch          : 1
    Train_loss     : 0.5587541812595813
    Train_accuracy : 0.8415741502192983
    Train_f1_score : 0.8415741920471191
    Val_loss       : 0.10712491696262184
    Val_accuracy   : 0.9709164915966387
    Val_f1_score   : 0.9709165096282959
Warning: Metric 'val_loss' is not found. Model performance monitoring is disabled.
Train Epoch: 2 [0/19430 (0%)] Loss: 0.045666
Train Epoch: 2 [160/19430 (1%)] Loss: 0.061016
Train Epoch: 2 [320/19430 (2%)] Loss: 0.062635
Train Epoch: 2 [480/19430 (2%)] Loss: 0.289817
Train Epoch: 2 [640/19430 (3%)] Loss: 0.076799
Train Epoch: 2 [800/19430 (4%)] Loss: 0.322156
Train Epoch: 2 [960/19430 (5%)] Loss: 0.135795
Train Epoch: 2 [1120/19430 (6%)] Loss: 0.111263
Train Epoch: 2 [1280/19430 (7%)] Loss: 0.081995
Train Epoch: 2 [1440/19430 (7%)] Loss: 0.164040
Train Epoch: 2 [1600/19430 (8%)] Loss: 0.035925
Train Epoch: 2 [1760/19430 (9%)] Loss: 0.105462
Train Epoch: 2 [1920/19430 (10%)] Loss: 0.148807
Train Epoch: 2 [2080/19430 (11%)] Loss: 0.036498
Train Epoch: 2 [2240/19430 (12%)] Loss: 0.104464
Train Epoch: 2 [2400/19430 (12%)] Loss: 0.127780
Train Epoch: 2 [2560/19430 (13%)] Loss: 0.110973
Train Epoch: 2 [2720/19430 (14%)] Loss: 0.017765
Train Epoch: 2 [2880/19430 (15%)] Loss: 0.040967
Train Epoch: 2 [3040/19430 (16%)] Loss: 0.082858
Train Epoch: 2 [3200/19430 (16%)] Loss: 0.356187
Train Epoch: 2 [3360/19430 (17%)] Loss: 0.063491
Train Epoch: 2 [3520/19430 (18%)] Loss: 0.114642
Train Epoch: 2 [3680/19430 (19%)] Loss: 0.085577
Train Epoch: 2 [3840/19430 (20%)] Loss: 0.052517
Train Epoch: 2 [4000/19430 (21%)] Loss: 0.031145
Train Epoch: 2 [4160/19430 (21%)] Loss: 0.183561
Train Epoch: 2 [4320/19430 (22%)] Loss: 0.045099
Train Epoch: 2 [4480/19430 (23%)] Loss: 0.154170
Train Epoch: 2 [4640/19430 (24%)] Loss: 0.112252
Train Epoch: 2 [4800/19430 (25%)] Loss: 0.031182
Train Epoch: 2 [4960/19430 (26%)] Loss: 0.128514
Train Epoch: 2 [5120/19430 (26%)] Loss: 0.072354
Train Epoch: 2 [5280/19430 (27%)] Loss: 0.047168
Train Epoch: 2 [5440/19430 (28%)] Loss: 0.027616
Train Epoch: 2 [5600/19430 (29%)] Loss: 0.022557
Train Epoch: 2 [5760/19430 (30%)] Loss: 0.071877
Train Epoch: 2 [5920/19430 (30%)] Loss: 0.206078
Train Epoch: 2 [6080/19430 (31%)] Loss: 0.156510
Train Epoch: 2 [6240/19430 (32%)] Loss: 0.064938
Train Epoch: 2 [6400/19430 (33%)] Loss: 0.075728
Train Epoch: 2 [6560/19430 (34%)] Loss: 0.055779
Train Epoch: 2 [6720/19430 (35%)] Loss: 0.080479
Train Epoch: 2 [6880/19430 (35%)] Loss: 0.038496
Train Epoch: 2 [7040/19430 (36%)] Loss: 0.078055
Train Epoch: 2 [7200/19430 (37%)] Loss: 0.015048
Train Epoch: 2 [7360/19430 (38%)] Loss: 0.149065
Train Epoch: 2 [7520/19430 (39%)] Loss: 0.075844
Train Epoch: 2 [7680/19430 (40%)] Loss: 0.036463
Train Epoch: 2 [7840/19430 (40%)] Loss: 0.141927
Train Epoch: 2 [8000/19430 (41%)] Loss: 0.182063
Train Epoch: 2 [8160/19430 (42%)] Loss: 0.060912
Train Epoch: 2 [8320/19430 (43%)] Loss: 0.048187
Train Epoch: 2 [8480/19430 (44%)] Loss: 0.041270
Train Epoch: 2 [8640/19430 (44%)] Loss: 0.080792
Train Epoch: 2 [8800/19430 (45%)] Loss: 0.023258
Train Epoch: 2 [8960/19430 (46%)] Loss: 0.065474
Train Epoch: 2 [9120/19430 (47%)] Loss: 0.032805
Train Epoch: 2 [9280/19430 (48%)] Loss: 0.050253
Train Epoch: 2 [9440/19430 (49%)] Loss: 0.012856
Train Epoch: 2 [9600/19430 (49%)] Loss: 0.059324
Train Epoch: 2 [9760/19430 (50%)] Loss: 0.130247
Train Epoch: 2 [9920/19430 (51%)] Loss: 0.057674
Train Epoch: 2 [10080/19430 (52%)] Loss: 0.024182
Train Epoch: 2 [10240/19430 (53%)] Loss: 0.087777
Train Epoch: 2 [10400/19430 (54%)] Loss: 0.044015
Train Epoch: 2 [10560/19430 (54%)] Loss: 0.038410
Train Epoch: 2 [10720/19430 (55%)] Loss: 0.191636
Train Epoch: 2 [10880/19430 (56%)] Loss: 0.191027
Train Epoch: 2 [11040/19430 (57%)] Loss: 0.029602
Train Epoch: 2 [11200/19430 (58%)] Loss: 0.032380
Train Epoch: 2 [11360/19430 (58%)] Loss: 0.026528
Train Epoch: 2 [11520/19430 (59%)] Loss: 0.022615
Train Epoch: 2 [11680/19430 (60%)] Loss: 0.033270
Train Epoch: 2 [11840/19430 (61%)] Loss: 0.024858
Train Epoch: 2 [12000/19430 (62%)] Loss: 0.020870
Train Epoch: 2 [12160/19430 (63%)] Loss: 0.015898
Train Epoch: 2 [12320/19430 (63%)] Loss: 0.011225
Train Epoch: 2 [12480/19430 (64%)] Loss: 0.128658
Train Epoch: 2 [12640/19430 (65%)] Loss: 0.126690
Train Epoch: 2 [12800/19430 (66%)] Loss: 0.188472
Train Epoch: 2 [12960/19430 (67%)] Loss: 0.023084
Train Epoch: 2 [13120/19430 (68%)] Loss: 0.031651
Train Epoch: 2 [13280/19430 (68%)] Loss: 0.078183
Train Epoch: 2 [13440/19430 (69%)] Loss: 0.026268
Train Epoch: 2 [13600/19430 (70%)] Loss: 0.059879
Train Epoch: 2 [13760/19430 (71%)] Loss: 0.065371
Train Epoch: 2 [13920/19430 (72%)] Loss: 0.017467
Train Epoch: 2 [14080/19430 (72%)] Loss: 0.040391
Train Epoch: 2 [14240/19430 (73%)] Loss: 0.081740
Train Epoch: 2 [14400/19430 (74%)] Loss: 0.093529
Train Epoch: 2 [14560/19430 (75%)] Loss: 0.012525
Train Epoch: 2 [14720/19430 (76%)] Loss: 0.300592
Train Epoch: 2 [14880/19430 (77%)] Loss: 0.098015
Train Epoch: 2 [15040/19430 (77%)] Loss: 0.211388
Train Epoch: 2 [15200/19430 (78%)] Loss: 0.143566
Train Epoch: 2 [15360/19430 (79%)] Loss: 0.142357
Train Epoch: 2 [15520/19430 (80%)] Loss: 0.011590
Train Epoch: 2 [15680/19430 (81%)] Loss: 0.097424
Train Epoch: 2 [15840/19430 (82%)] Loss: 0.023391
Train Epoch: 2 [16000/19430 (82%)] Loss: 0.082150
Train Epoch: 2 [16160/19430 (83%)] Loss: 0.019726
Train Epoch: 2 [16320/19430 (84%)] Loss: 0.013840
Train Epoch: 2 [16480/19430 (85%)] Loss: 0.021793
Train Epoch: 2 [16640/19430 (86%)] Loss: 0.190037
Train Epoch: 2 [16800/19430 (86%)] Loss: 0.051442
Train Epoch: 2 [16960/19430 (87%)] Loss: 0.285576
Train Epoch: 2 [17120/19430 (88%)] Loss: 0.152463
Train Epoch: 2 [17280/19430 (89%)] Loss: 0.008761
Train Epoch: 2 [17440/19430 (90%)] Loss: 0.040674
Train Epoch: 2 [17600/19430 (91%)] Loss: 0.120468
Train Epoch: 2 [17760/19430 (91%)] Loss: 0.035642
Train Epoch: 2 [17920/19430 (92%)] Loss: 0.037187
Train Epoch: 2 [18080/19430 (93%)] Loss: 0.059549
Train Epoch: 2 [18240/19430 (94%)] Loss: 0.079626
Train Epoch: 2 [18400/19430 (95%)] Loss: 0.071660
Train Epoch: 2 [18560/19430 (96%)] Loss: 0.161602
Train Epoch: 2 [18720/19430 (96%)] Loss: 0.201498
Train Epoch: 2 [18880/19430 (97%)] Loss: 0.015421
Train Epoch: 2 [19040/19430 (98%)] Loss: 0.105220
Train Epoch: 2 [19200/19430 (99%)] Loss: 0.020047
Train Epoch: 2 [19360/19430 (100%)] Loss: 0.006376
    epoch          : 2
    Train_loss     : 0.0813920134398933
    Train_accuracy : 0.9768708881578947
    Train_f1_score : 0.9768708944320679
    Val_loss       : 0.06135566784616779
    Val_accuracy   : 0.9802389705882353
    Val_f1_score   : 0.9802389740943909
Train Epoch: 3 [0/19430 (0%)] Loss: 0.014867
Train Epoch: 3 [160/19430 (1%)] Loss: 0.024866
Train Epoch: 3 [320/19430 (2%)] Loss: 0.025143
Train Epoch: 3 [480/19430 (2%)] Loss: 0.015169
Train Epoch: 3 [640/19430 (3%)] Loss: 0.087205
Train Epoch: 3 [800/19430 (4%)] Loss: 0.043111
Train Epoch: 3 [960/19430 (5%)] Loss: 0.004379
Train Epoch: 3 [1120/19430 (6%)] Loss: 0.020918
Train Epoch: 3 [1280/19430 (7%)] Loss: 0.008795
Train Epoch: 3 [1440/19430 (7%)] Loss: 0.073593
Train Epoch: 3 [1600/19430 (8%)] Loss: 0.012895
Train Epoch: 3 [1760/19430 (9%)] Loss: 0.004788
Train Epoch: 3 [1920/19430 (10%)] Loss: 0.151539
Train Epoch: 3 [2080/19430 (11%)] Loss: 0.021768
Train Epoch: 3 [2240/19430 (12%)] Loss: 0.065844
Train Epoch: 3 [2400/19430 (12%)] Loss: 0.021170
Train Epoch: 3 [2560/19430 (13%)] Loss: 0.021120
Train Epoch: 3 [2720/19430 (14%)] Loss: 0.200568
Train Epoch: 3 [2880/19430 (15%)] Loss: 0.010408
Train Epoch: 3 [3040/19430 (16%)] Loss: 0.017793
Train Epoch: 3 [3200/19430 (16%)] Loss: 0.019941
Train Epoch: 3 [3360/19430 (17%)] Loss: 0.010411
Train Epoch: 3 [3520/19430 (18%)] Loss: 0.041019
Train Epoch: 3 [3680/19430 (19%)] Loss: 0.012735
Train Epoch: 3 [3840/19430 (20%)] Loss: 0.114627
Train Epoch: 3 [4000/19430 (21%)] Loss: 0.010158
Train Epoch: 3 [4160/19430 (21%)] Loss: 0.097220
Train Epoch: 3 [4320/19430 (22%)] Loss: 0.174175
Train Epoch: 3 [4480/19430 (23%)] Loss: 0.083852
Train Epoch: 3 [4640/19430 (24%)] Loss: 0.007109
Train Epoch: 3 [4800/19430 (25%)] Loss: 0.029476
Train Epoch: 3 [4960/19430 (26%)] Loss: 0.006827
Train Epoch: 3 [5120/19430 (26%)] Loss: 0.012335
Train Epoch: 3 [5280/19430 (27%)] Loss: 0.375785
Train Epoch: 3 [5440/19430 (28%)] Loss: 0.025277
Train Epoch: 3 [5600/19430 (29%)] Loss: 0.077406
Train Epoch: 3 [5760/19430 (30%)] Loss: 0.007812
Train Epoch: 3 [5920/19430 (30%)] Loss: 0.022072
Train Epoch: 3 [6080/19430 (31%)] Loss: 0.011620
Train Epoch: 3 [6240/19430 (32%)] Loss: 0.031784
Train Epoch: 3 [6400/19430 (33%)] Loss: 0.015907
Train Epoch: 3 [6560/19430 (34%)] Loss: 0.034187
Train Epoch: 3 [6720/19430 (35%)] Loss: 0.039793
Train Epoch: 3 [6880/19430 (35%)] Loss: 0.013306
Train Epoch: 3 [7040/19430 (36%)] Loss: 0.136517
Train Epoch: 3 [7200/19430 (37%)] Loss: 0.067407
Train Epoch: 3 [7360/19430 (38%)] Loss: 0.093985
Train Epoch: 3 [7520/19430 (39%)] Loss: 0.145540
Train Epoch: 3 [7680/19430 (40%)] Loss: 0.013594
Train Epoch: 3 [7840/19430 (40%)] Loss: 0.206549
Train Epoch: 3 [8000/19430 (41%)] Loss: 0.009605
Train Epoch: 3 [8160/19430 (42%)] Loss: 0.130460
Train Epoch: 3 [8320/19430 (43%)] Loss: 0.006345
Train Epoch: 3 [8480/19430 (44%)] Loss: 0.012399
Train Epoch: 3 [8640/19430 (44%)] Loss: 0.007987
Train Epoch: 3 [8800/19430 (45%)] Loss: 0.009222
Train Epoch: 3 [8960/19430 (46%)] Loss: 0.023562
Train Epoch: 3 [9120/19430 (47%)] Loss: 0.052130
Train Epoch: 3 [9280/19430 (48%)] Loss: 0.034962
Train Epoch: 3 [9440/19430 (49%)] Loss: 0.061414
Train Epoch: 3 [9600/19430 (49%)] Loss: 0.022558
Train Epoch: 3 [9760/19430 (50%)] Loss: 0.076118
Train Epoch: 3 [9920/19430 (51%)] Loss: 0.040450
Train Epoch: 3 [10080/19430 (52%)] Loss: 0.009781
Train Epoch: 3 [10240/19430 (53%)] Loss: 0.006348
Train Epoch: 3 [10400/19430 (54%)] Loss: 0.087337
Train Epoch: 3 [10560/19430 (54%)] Loss: 0.031085
Train Epoch: 3 [10720/19430 (55%)] Loss: 0.020491
Train Epoch: 3 [10880/19430 (56%)] Loss: 0.009271
Train Epoch: 3 [11040/19430 (57%)] Loss: 0.052154
Train Epoch: 3 [11200/19430 (58%)] Loss: 0.015736
Train Epoch: 3 [11360/19430 (58%)] Loss: 0.011317
Train Epoch: 3 [11520/19430 (59%)] Loss: 0.119900
Train Epoch: 3 [11680/19430 (60%)] Loss: 0.142951
Train Epoch: 3 [11840/19430 (61%)] Loss: 0.032619
Train Epoch: 3 [12000/19430 (62%)] Loss: 0.014385
Train Epoch: 3 [12160/19430 (63%)] Loss: 0.008704
Train Epoch: 3 [12320/19430 (63%)] Loss: 0.019825
Train Epoch: 3 [12480/19430 (64%)] Loss: 0.067326
Train Epoch: 3 [12640/19430 (65%)] Loss: 0.023075
Train Epoch: 3 [12800/19430 (66%)] Loss: 0.018516
Train Epoch: 3 [12960/19430 (67%)] Loss: 0.307043
Train Epoch: 3 [13120/19430 (68%)] Loss: 0.017420
Train Epoch: 3 [13280/19430 (68%)] Loss: 0.032293
Train Epoch: 3 [13440/19430 (69%)] Loss: 0.014759
Train Epoch: 3 [13600/19430 (70%)] Loss: 0.041416
Train Epoch: 3 [13760/19430 (71%)] Loss: 0.011570
Train Epoch: 3 [13920/19430 (72%)] Loss: 0.023102
Train Epoch: 3 [14080/19430 (72%)] Loss: 0.032915
Train Epoch: 3 [14240/19430 (73%)] Loss: 0.189265
Train Epoch: 3 [14400/19430 (74%)] Loss: 0.023828
Train Epoch: 3 [14560/19430 (75%)] Loss: 0.003968
Train Epoch: 3 [14720/19430 (76%)] Loss: 0.022822
Train Epoch: 3 [14880/19430 (77%)] Loss: 0.015059
Train Epoch: 3 [15040/19430 (77%)] Loss: 0.007360
Train Epoch: 3 [15200/19430 (78%)] Loss: 0.072207
Train Epoch: 3 [15360/19430 (79%)] Loss: 0.028593
Train Epoch: 3 [15520/19430 (80%)] Loss: 0.050593
Train Epoch: 3 [15680/19430 (81%)] Loss: 0.067483
Train Epoch: 3 [15840/19430 (82%)] Loss: 0.093142
Train Epoch: 3 [16000/19430 (82%)] Loss: 0.003403
Train Epoch: 3 [16160/19430 (83%)] Loss: 0.008314
Train Epoch: 3 [16320/19430 (84%)] Loss: 0.095639
Train Epoch: 3 [16480/19430 (85%)] Loss: 0.083993
Train Epoch: 3 [16640/19430 (86%)] Loss: 0.021387
Train Epoch: 3 [16800/19430 (86%)] Loss: 0.004496
Train Epoch: 3 [16960/19430 (87%)] Loss: 0.066432
Train Epoch: 3 [17120/19430 (88%)] Loss: 0.004014
Train Epoch: 3 [17280/19430 (89%)] Loss: 0.061494
Train Epoch: 3 [17440/19430 (90%)] Loss: 0.009128
Train Epoch: 3 [17600/19430 (91%)] Loss: 0.049296
Train Epoch: 3 [17760/19430 (91%)] Loss: 0.251584
Train Epoch: 3 [17920/19430 (92%)] Loss: 0.035223
Train Epoch: 3 [18080/19430 (93%)] Loss: 0.004217
Train Epoch: 3 [18240/19430 (94%)] Loss: 0.017151
Train Epoch: 3 [18400/19430 (95%)] Loss: 0.086214
Train Epoch: 3 [18560/19430 (96%)] Loss: 0.072141
Train Epoch: 3 [18720/19430 (96%)] Loss: 0.027248
Train Epoch: 3 [18880/19430 (97%)] Loss: 0.085962
Train Epoch: 3 [19040/19430 (98%)] Loss: 0.012722
Train Epoch: 3 [19200/19430 (99%)] Loss: 0.065071
Train Epoch: 3 [19360/19430 (100%)] Loss: 0.012098
    epoch          : 3
    Train_loss     : 0.04568445890120194
    Train_accuracy : 0.9868935032894737
    Train_f1_score : 0.9868935346603394
    Val_loss       : 0.050230543451000226
    Val_accuracy   : 0.9871323529411765
    Val_f1_score   : 0.9871323704719543
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1030_145349/checkpoint-epoch3.pth ...
Train Epoch: 4 [0/19430 (0%)] Loss: 0.010674
Train Epoch: 4 [160/19430 (1%)] Loss: 0.011939
Train Epoch: 4 [320/19430 (2%)] Loss: 0.002075
Train Epoch: 4 [480/19430 (2%)] Loss: 0.058944
Train Epoch: 4 [640/19430 (3%)] Loss: 0.039607
Train Epoch: 4 [800/19430 (4%)] Loss: 0.215044
Train Epoch: 4 [960/19430 (5%)] Loss: 0.032715
Train Epoch: 4 [1120/19430 (6%)] Loss: 0.006097
Train Epoch: 4 [1280/19430 (7%)] Loss: 0.115933
Train Epoch: 4 [1440/19430 (7%)] Loss: 0.003498
Train Epoch: 4 [1600/19430 (8%)] Loss: 0.008776
Train Epoch: 4 [1760/19430 (9%)] Loss: 0.065672
Train Epoch: 4 [1920/19430 (10%)] Loss: 0.012675
Train Epoch: 4 [2080/19430 (11%)] Loss: 0.004712
Train Epoch: 4 [2240/19430 (12%)] Loss: 0.012091
Train Epoch: 4 [2400/19430 (12%)] Loss: 0.008126
Train Epoch: 4 [2560/19430 (13%)] Loss: 0.003202
Train Epoch: 4 [2720/19430 (14%)] Loss: 0.009299
Train Epoch: 4 [2880/19430 (15%)] Loss: 0.025723
Train Epoch: 4 [3040/19430 (16%)] Loss: 0.021780
Train Epoch: 4 [3200/19430 (16%)] Loss: 0.015826
Train Epoch: 4 [3360/19430 (17%)] Loss: 0.005840
Train Epoch: 4 [3520/19430 (18%)] Loss: 0.007363
Train Epoch: 4 [3680/19430 (19%)] Loss: 0.010319
Train Epoch: 4 [3840/19430 (20%)] Loss: 0.048188
Train Epoch: 4 [4000/19430 (21%)] Loss: 0.002178
Train Epoch: 4 [4160/19430 (21%)] Loss: 0.005383
Train Epoch: 4 [4320/19430 (22%)] Loss: 0.088015
Train Epoch: 4 [4480/19430 (23%)] Loss: 0.006605
Train Epoch: 4 [4640/19430 (24%)] Loss: 0.034605
Train Epoch: 4 [4800/19430 (25%)] Loss: 0.012606
Train Epoch: 4 [4960/19430 (26%)] Loss: 0.013964
Train Epoch: 4 [5120/19430 (26%)] Loss: 0.007842
Train Epoch: 4 [5280/19430 (27%)] Loss: 0.028089
Train Epoch: 4 [5440/19430 (28%)] Loss: 0.003366
Train Epoch: 4 [5600/19430 (29%)] Loss: 0.009709
Train Epoch: 4 [5760/19430 (30%)] Loss: 0.015207
Train Epoch: 4 [5920/19430 (30%)] Loss: 0.006328
Train Epoch: 4 [6080/19430 (31%)] Loss: 0.040738
Train Epoch: 4 [6240/19430 (32%)] Loss: 0.006401
Train Epoch: 4 [6400/19430 (33%)] Loss: 0.003666
Train Epoch: 4 [6560/19430 (34%)] Loss: 0.005207
Train Epoch: 4 [6720/19430 (35%)] Loss: 0.004366
Train Epoch: 4 [6880/19430 (35%)] Loss: 0.007288
Train Epoch: 4 [7040/19430 (36%)] Loss: 0.015217
Train Epoch: 4 [7200/19430 (37%)] Loss: 0.003879
Train Epoch: 4 [7360/19430 (38%)] Loss: 0.019919
Train Epoch: 4 [7520/19430 (39%)] Loss: 0.049479
Train Epoch: 4 [7680/19430 (40%)] Loss: 0.002672
Train Epoch: 4 [7840/19430 (40%)] Loss: 0.067568
Train Epoch: 4 [8000/19430 (41%)] Loss: 0.007858
Train Epoch: 4 [8160/19430 (42%)] Loss: 0.005590
Train Epoch: 4 [8320/19430 (43%)] Loss: 0.005997
Train Epoch: 4 [8480/19430 (44%)] Loss: 0.005072
Train Epoch: 4 [8640/19430 (44%)] Loss: 0.005136
Train Epoch: 4 [8800/19430 (45%)] Loss: 0.021917
Train Epoch: 4 [8960/19430 (46%)] Loss: 0.244136
Train Epoch: 4 [9120/19430 (47%)] Loss: 0.049753
Train Epoch: 4 [9280/19430 (48%)] Loss: 0.042253
Train Epoch: 4 [9440/19430 (49%)] Loss: 0.002674
Train Epoch: 4 [9600/19430 (49%)] Loss: 0.004276
Train Epoch: 4 [9760/19430 (50%)] Loss: 0.017993
Train Epoch: 4 [9920/19430 (51%)] Loss: 0.018996
Train Epoch: 4 [10080/19430 (52%)] Loss: 0.021557
Train Epoch: 4 [10240/19430 (53%)] Loss: 0.199175
Train Epoch: 4 [10400/19430 (54%)] Loss: 0.019686
Train Epoch: 4 [10560/19430 (54%)] Loss: 0.014508
Train Epoch: 4 [10720/19430 (55%)] Loss: 0.012108
Train Epoch: 4 [10880/19430 (56%)] Loss: 0.003349
Train Epoch: 4 [11040/19430 (57%)] Loss: 0.002459
Train Epoch: 4 [11200/19430 (58%)] Loss: 0.008506
Train Epoch: 4 [11360/19430 (58%)] Loss: 0.045643
Train Epoch: 4 [11520/19430 (59%)] Loss: 0.020116
Train Epoch: 4 [11680/19430 (60%)] Loss: 0.009953
Train Epoch: 4 [11840/19430 (61%)] Loss: 0.003113
Train Epoch: 4 [12000/19430 (62%)] Loss: 0.005123
Train Epoch: 4 [12160/19430 (63%)] Loss: 0.003222
Train Epoch: 4 [12320/19430 (63%)] Loss: 0.002746
Train Epoch: 4 [12480/19430 (64%)] Loss: 0.040926
Train Epoch: 4 [12640/19430 (65%)] Loss: 0.004638
Train Epoch: 4 [12800/19430 (66%)] Loss: 0.002827
Train Epoch: 4 [12960/19430 (67%)] Loss: 0.025231
Train Epoch: 4 [13120/19430 (68%)] Loss: 0.049828
Train Epoch: 4 [13280/19430 (68%)] Loss: 0.016012
Train Epoch: 4 [13440/19430 (69%)] Loss: 0.004401
Train Epoch: 4 [13600/19430 (70%)] Loss: 0.002122
Train Epoch: 4 [13760/19430 (71%)] Loss: 0.043982
Train Epoch: 4 [13920/19430 (72%)] Loss: 0.062871
Train Epoch: 4 [14080/19430 (72%)] Loss: 0.007457
Train Epoch: 4 [14240/19430 (73%)] Loss: 0.044848
Train Epoch: 4 [14400/19430 (74%)] Loss: 0.002247
Train Epoch: 4 [14560/19430 (75%)] Loss: 0.003067
Train Epoch: 4 [14720/19430 (76%)] Loss: 0.133070
Train Epoch: 4 [14880/19430 (77%)] Loss: 0.009298
Train Epoch: 4 [15040/19430 (77%)] Loss: 0.012845
Train Epoch: 4 [15200/19430 (78%)] Loss: 0.027452
Train Epoch: 4 [15360/19430 (79%)] Loss: 0.007484
Train Epoch: 4 [15520/19430 (80%)] Loss: 0.048241
Train Epoch: 4 [15680/19430 (81%)] Loss: 0.005914
Train Epoch: 4 [15840/19430 (82%)] Loss: 0.049165
Train Epoch: 4 [16000/19430 (82%)] Loss: 0.069481
Train Epoch: 4 [16160/19430 (83%)] Loss: 0.002071
Train Epoch: 4 [16320/19430 (84%)] Loss: 0.001663
Train Epoch: 4 [16480/19430 (85%)] Loss: 0.002337
Train Epoch: 4 [16640/19430 (86%)] Loss: 0.002431
Train Epoch: 4 [16800/19430 (86%)] Loss: 0.095774
Train Epoch: 4 [16960/19430 (87%)] Loss: 0.002135
Train Epoch: 4 [17120/19430 (88%)] Loss: 0.005063
Train Epoch: 4 [17280/19430 (89%)] Loss: 0.012052
Train Epoch: 4 [17440/19430 (90%)] Loss: 0.031831
Train Epoch: 4 [17600/19430 (91%)] Loss: 0.008587
Train Epoch: 4 [17760/19430 (91%)] Loss: 0.005395
Train Epoch: 4 [17920/19430 (92%)] Loss: 0.001893
Train Epoch: 4 [18080/19430 (93%)] Loss: 0.001492
Train Epoch: 4 [18240/19430 (94%)] Loss: 0.005501
Train Epoch: 4 [18400/19430 (95%)] Loss: 0.014976
Train Epoch: 4 [18560/19430 (96%)] Loss: 0.015477
Train Epoch: 4 [18720/19430 (96%)] Loss: 0.003890
Train Epoch: 4 [18880/19430 (97%)] Loss: 0.004677
Train Epoch: 4 [19040/19430 (98%)] Loss: 0.002302
Train Epoch: 4 [19200/19430 (99%)] Loss: 0.005289
Train Epoch: 4 [19360/19430 (100%)] Loss: 0.008485
    epoch          : 4
    Train_loss     : 0.022497627269043925
    Train_accuracy : 0.9939692982456141
    Train_f1_score : 0.9939692616462708
    Val_loss       : 0.02925996072265073
    Val_accuracy   : 0.9903492647058824
    Val_f1_score   : 0.990349292755127
Train Epoch: 5 [0/19430 (0%)] Loss: 0.014178
Train Epoch: 5 [160/19430 (1%)] Loss: 0.030865
Train Epoch: 5 [320/19430 (2%)] Loss: 0.050683
Train Epoch: 5 [480/19430 (2%)] Loss: 0.049585
Train Epoch: 5 [640/19430 (3%)] Loss: 0.050560
Train Epoch: 5 [800/19430 (4%)] Loss: 0.011447
Train Epoch: 5 [960/19430 (5%)] Loss: 0.015683
Train Epoch: 5 [1120/19430 (6%)] Loss: 0.009234
Train Epoch: 5 [1280/19430 (7%)] Loss: 0.004229
Train Epoch: 5 [1440/19430 (7%)] Loss: 0.007279
Train Epoch: 5 [1600/19430 (8%)] Loss: 0.006959
Train Epoch: 5 [1760/19430 (9%)] Loss: 0.004881
Train Epoch: 5 [1920/19430 (10%)] Loss: 0.003661
Train Epoch: 5 [2080/19430 (11%)] Loss: 0.080937
Train Epoch: 5 [2240/19430 (12%)] Loss: 0.004587
Train Epoch: 5 [2400/19430 (12%)] Loss: 0.012927
Train Epoch: 5 [2560/19430 (13%)] Loss: 0.003029
Train Epoch: 5 [2720/19430 (14%)] Loss: 0.008592
Train Epoch: 5 [2880/19430 (15%)] Loss: 0.014223
Train Epoch: 5 [3040/19430 (16%)] Loss: 0.010526
Train Epoch: 5 [3200/19430 (16%)] Loss: 0.005499
Train Epoch: 5 [3360/19430 (17%)] Loss: 0.131878
Train Epoch: 5 [3520/19430 (18%)] Loss: 0.049722
Train Epoch: 5 [3680/19430 (19%)] Loss: 0.018530
Train Epoch: 5 [3840/19430 (20%)] Loss: 0.016418
Train Epoch: 5 [4000/19430 (21%)] Loss: 0.010632
Train Epoch: 5 [4160/19430 (21%)] Loss: 0.014486
Train Epoch: 5 [4320/19430 (22%)] Loss: 0.019948
Train Epoch: 5 [4480/19430 (23%)] Loss: 0.001770
Train Epoch: 5 [4640/19430 (24%)] Loss: 0.005585
Train Epoch: 5 [4800/19430 (25%)] Loss: 0.001448
Train Epoch: 5 [4960/19430 (26%)] Loss: 0.002383
Train Epoch: 5 [5120/19430 (26%)] Loss: 0.007209
Train Epoch: 5 [5280/19430 (27%)] Loss: 0.003185
Train Epoch: 5 [5440/19430 (28%)] Loss: 0.001815
Train Epoch: 5 [5600/19430 (29%)] Loss: 0.007381
Train Epoch: 5 [5760/19430 (30%)] Loss: 0.009229
Train Epoch: 5 [5920/19430 (30%)] Loss: 0.001355
Train Epoch: 5 [6080/19430 (31%)] Loss: 0.006264
Train Epoch: 5 [6240/19430 (32%)] Loss: 0.115600
Train Epoch: 5 [6400/19430 (33%)] Loss: 0.001819
Train Epoch: 5 [6560/19430 (34%)] Loss: 0.028948
Train Epoch: 5 [6720/19430 (35%)] Loss: 0.016740
Train Epoch: 5 [6880/19430 (35%)] Loss: 0.015305
Train Epoch: 5 [7040/19430 (36%)] Loss: 0.015641
Train Epoch: 5 [7200/19430 (37%)] Loss: 0.030420
Train Epoch: 5 [7360/19430 (38%)] Loss: 0.062387
Train Epoch: 5 [7520/19430 (39%)] Loss: 0.003132
Train Epoch: 5 [7680/19430 (40%)] Loss: 0.006668
Train Epoch: 5 [7840/19430 (40%)] Loss: 0.021555
Train Epoch: 5 [8000/19430 (41%)] Loss: 0.003231
Train Epoch: 5 [8160/19430 (42%)] Loss: 0.003951
Train Epoch: 5 [8320/19430 (43%)] Loss: 0.001777
Train Epoch: 5 [8480/19430 (44%)] Loss: 0.030686
Train Epoch: 5 [8640/19430 (44%)] Loss: 0.015957
Train Epoch: 5 [8800/19430 (45%)] Loss: 0.021856
Train Epoch: 5 [8960/19430 (46%)] Loss: 0.006840
Train Epoch: 5 [9120/19430 (47%)] Loss: 0.006814
Train Epoch: 5 [9280/19430 (48%)] Loss: 0.002179
Train Epoch: 5 [9440/19430 (49%)] Loss: 0.229899
Train Epoch: 5 [9600/19430 (49%)] Loss: 0.000970
Train Epoch: 5 [9760/19430 (50%)] Loss: 0.004737
Train Epoch: 5 [9920/19430 (51%)] Loss: 0.069839
Train Epoch: 5 [10080/19430 (52%)] Loss: 0.004538
Train Epoch: 5 [10240/19430 (53%)] Loss: 0.017487
Train Epoch: 5 [10400/19430 (54%)] Loss: 0.001843
Train Epoch: 5 [10560/19430 (54%)] Loss: 0.002469
Train Epoch: 5 [10720/19430 (55%)] Loss: 0.005783
Train Epoch: 5 [10880/19430 (56%)] Loss: 0.004083
Train Epoch: 5 [11040/19430 (57%)] Loss: 0.004464
Train Epoch: 5 [11200/19430 (58%)] Loss: 0.116091
Train Epoch: 5 [11360/19430 (58%)] Loss: 0.030723
Train Epoch: 5 [11520/19430 (59%)] Loss: 0.002367
Train Epoch: 5 [11680/19430 (60%)] Loss: 0.004514
Train Epoch: 5 [11840/19430 (61%)] Loss: 0.012878
Train Epoch: 5 [12000/19430 (62%)] Loss: 0.027255
Train Epoch: 5 [12160/19430 (63%)] Loss: 0.001691
Train Epoch: 5 [12320/19430 (63%)] Loss: 0.004212
Train Epoch: 5 [12480/19430 (64%)] Loss: 0.004642
Train Epoch: 5 [12640/19430 (65%)] Loss: 0.005141
Train Epoch: 5 [12800/19430 (66%)] Loss: 0.002982
Train Epoch: 5 [12960/19430 (67%)] Loss: 0.006416
Train Epoch: 5 [13120/19430 (68%)] Loss: 0.002466
Train Epoch: 5 [13280/19430 (68%)] Loss: 0.015299
Train Epoch: 5 [13440/19430 (69%)] Loss: 0.012267
Train Epoch: 5 [13600/19430 (70%)] Loss: 0.005043
Train Epoch: 5 [13760/19430 (71%)] Loss: 0.006365
Train Epoch: 5 [13920/19430 (72%)] Loss: 0.003952
Train Epoch: 5 [14080/19430 (72%)] Loss: 0.001601
Train Epoch: 5 [14240/19430 (73%)] Loss: 0.000750
Train Epoch: 5 [14400/19430 (74%)] Loss: 0.095182
Train Epoch: 5 [14560/19430 (75%)] Loss: 0.001546
Train Epoch: 5 [14720/19430 (76%)] Loss: 0.011728
Train Epoch: 5 [14880/19430 (77%)] Loss: 0.026850
Train Epoch: 5 [15040/19430 (77%)] Loss: 0.118189
Train Epoch: 5 [15200/19430 (78%)] Loss: 0.015140
Train Epoch: 5 [15360/19430 (79%)] Loss: 0.009785
Train Epoch: 5 [15520/19430 (80%)] Loss: 0.003231
Train Epoch: 5 [15680/19430 (81%)] Loss: 0.011155
Train Epoch: 5 [15840/19430 (82%)] Loss: 0.003817
Train Epoch: 5 [16000/19430 (82%)] Loss: 0.003542
Train Epoch: 5 [16160/19430 (83%)] Loss: 0.013144
Train Epoch: 5 [16320/19430 (84%)] Loss: 0.004459
Train Epoch: 5 [16480/19430 (85%)] Loss: 0.013260
Train Epoch: 5 [16640/19430 (86%)] Loss: 0.004146
Train Epoch: 5 [16800/19430 (86%)] Loss: 0.004177
Train Epoch: 5 [16960/19430 (87%)] Loss: 0.006564
Train Epoch: 5 [17120/19430 (88%)] Loss: 0.018325
Train Epoch: 5 [17280/19430 (89%)] Loss: 0.006404
Train Epoch: 5 [17440/19430 (90%)] Loss: 0.003204
Train Epoch: 5 [17600/19430 (91%)] Loss: 0.007219
Train Epoch: 5 [17760/19430 (91%)] Loss: 0.005284
Train Epoch: 5 [17920/19430 (92%)] Loss: 0.002546
Train Epoch: 5 [18080/19430 (93%)] Loss: 0.035323
Train Epoch: 5 [18240/19430 (94%)] Loss: 0.001626
Train Epoch: 5 [18400/19430 (95%)] Loss: 0.008880
Train Epoch: 5 [18560/19430 (96%)] Loss: 0.003709
Train Epoch: 5 [18720/19430 (96%)] Loss: 0.028794
Train Epoch: 5 [18880/19430 (97%)] Loss: 0.002993
Train Epoch: 5 [19040/19430 (98%)] Loss: 0.002223
Train Epoch: 5 [19200/19430 (99%)] Loss: 0.006809
Train Epoch: 5 [19360/19430 (100%)] Loss: 0.001905
    epoch          : 5
    Train_loss     : 0.02321565890375304
    Train_accuracy : 0.9934553179824562
    Train_f1_score : 0.9934552907943726
    Val_loss       : 0.025769896192821974
    Val_accuracy   : 0.9929753151260504
    Val_f1_score   : 0.9929753541946411
Train Epoch: 6 [0/19430 (0%)] Loss: 0.008585
Train Epoch: 6 [160/19430 (1%)] Loss: 0.029579
Train Epoch: 6 [320/19430 (2%)] Loss: 0.029969
Train Epoch: 6 [480/19430 (2%)] Loss: 0.015075
Train Epoch: 6 [640/19430 (3%)] Loss: 0.012853
Train Epoch: 6 [800/19430 (4%)] Loss: 0.002713
Train Epoch: 6 [960/19430 (5%)] Loss: 0.002198
Train Epoch: 6 [1120/19430 (6%)] Loss: 0.001699
Train Epoch: 6 [1280/19430 (7%)] Loss: 0.005005
Train Epoch: 6 [1440/19430 (7%)] Loss: 0.003945
Train Epoch: 6 [1600/19430 (8%)] Loss: 0.004630
Train Epoch: 6 [1760/19430 (9%)] Loss: 0.009478
Train Epoch: 6 [1920/19430 (10%)] Loss: 0.001053
Train Epoch: 6 [2080/19430 (11%)] Loss: 0.017275
Train Epoch: 6 [2240/19430 (12%)] Loss: 0.120390
Train Epoch: 6 [2400/19430 (12%)] Loss: 0.002129
Train Epoch: 6 [2560/19430 (13%)] Loss: 0.002895
Train Epoch: 6 [2720/19430 (14%)] Loss: 0.195163
Train Epoch: 6 [2880/19430 (15%)] Loss: 0.011968
Train Epoch: 6 [3040/19430 (16%)] Loss: 0.042161
Train Epoch: 6 [3200/19430 (16%)] Loss: 0.002917
Train Epoch: 6 [3360/19430 (17%)] Loss: 0.004079
Train Epoch: 6 [3520/19430 (18%)] Loss: 0.011815
Train Epoch: 6 [3680/19430 (19%)] Loss: 0.004515
Train Epoch: 6 [3840/19430 (20%)] Loss: 0.014605
Train Epoch: 6 [4000/19430 (21%)] Loss: 0.004847
Train Epoch: 6 [4160/19430 (21%)] Loss: 0.011279
Train Epoch: 6 [4320/19430 (22%)] Loss: 0.004335
Train Epoch: 6 [4480/19430 (23%)] Loss: 0.002891
Train Epoch: 6 [4640/19430 (24%)] Loss: 0.008248
Train Epoch: 6 [4800/19430 (25%)] Loss: 0.050829
Train Epoch: 6 [4960/19430 (26%)] Loss: 0.006103
Train Epoch: 6 [5120/19430 (26%)] Loss: 0.003736
Train Epoch: 6 [5280/19430 (27%)] Loss: 0.004661
Train Epoch: 6 [5440/19430 (28%)] Loss: 0.017237
Train Epoch: 6 [5600/19430 (29%)] Loss: 0.024661
Train Epoch: 6 [5760/19430 (30%)] Loss: 0.006402
Train Epoch: 6 [5920/19430 (30%)] Loss: 0.005144
Train Epoch: 6 [6080/19430 (31%)] Loss: 0.002540
Train Epoch: 6 [6240/19430 (32%)] Loss: 0.001245
Train Epoch: 6 [6400/19430 (33%)] Loss: 0.003287
Train Epoch: 6 [6560/19430 (34%)] Loss: 0.004098
Train Epoch: 6 [6720/19430 (35%)] Loss: 0.002784
Train Epoch: 6 [6880/19430 (35%)] Loss: 0.012303
Train Epoch: 6 [7040/19430 (36%)] Loss: 0.031733
Train Epoch: 6 [7200/19430 (37%)] Loss: 0.012280
Train Epoch: 6 [7360/19430 (38%)] Loss: 0.019002
Train Epoch: 6 [7520/19430 (39%)] Loss: 0.003332
Train Epoch: 6 [7680/19430 (40%)] Loss: 0.019614
Train Epoch: 6 [7840/19430 (40%)] Loss: 0.003616
Train Epoch: 6 [8000/19430 (41%)] Loss: 0.022170
Train Epoch: 6 [8160/19430 (42%)] Loss: 0.002412
Train Epoch: 6 [8320/19430 (43%)] Loss: 0.002920
Train Epoch: 6 [8480/19430 (44%)] Loss: 0.005139
Train Epoch: 6 [8640/19430 (44%)] Loss: 0.003101
Train Epoch: 6 [8800/19430 (45%)] Loss: 0.003396
Train Epoch: 6 [8960/19430 (46%)] Loss: 0.026896
Train Epoch: 6 [9120/19430 (47%)] Loss: 0.006101
Train Epoch: 6 [9280/19430 (48%)] Loss: 0.002898
Train Epoch: 6 [9440/19430 (49%)] Loss: 0.015173
Train Epoch: 6 [9600/19430 (49%)] Loss: 0.010456
Train Epoch: 6 [9760/19430 (50%)] Loss: 0.001718
Train Epoch: 6 [9920/19430 (51%)] Loss: 0.000678
Train Epoch: 6 [10080/19430 (52%)] Loss: 0.020137
Train Epoch: 6 [10240/19430 (53%)] Loss: 0.003286
Train Epoch: 6 [10400/19430 (54%)] Loss: 0.007106
Train Epoch: 6 [10560/19430 (54%)] Loss: 0.004837
Train Epoch: 6 [10720/19430 (55%)] Loss: 0.003205
Train Epoch: 6 [10880/19430 (56%)] Loss: 0.005424
Train Epoch: 6 [11040/19430 (57%)] Loss: 0.033116
Train Epoch: 6 [11200/19430 (58%)] Loss: 0.005782
Train Epoch: 6 [11360/19430 (58%)] Loss: 0.003715
Train Epoch: 6 [11520/19430 (59%)] Loss: 0.005471
Train Epoch: 6 [11680/19430 (60%)] Loss: 0.006285
Train Epoch: 6 [11840/19430 (61%)] Loss: 0.003209
Train Epoch: 6 [12000/19430 (62%)] Loss: 0.013082
Train Epoch: 6 [12160/19430 (63%)] Loss: 0.008912
Train Epoch: 6 [12320/19430 (63%)] Loss: 0.002663
Train Epoch: 6 [12480/19430 (64%)] Loss: 0.007362
Train Epoch: 6 [12640/19430 (65%)] Loss: 0.043652
Train Epoch: 6 [12800/19430 (66%)] Loss: 0.001524
Train Epoch: 6 [12960/19430 (67%)] Loss: 0.000951
Train Epoch: 6 [13120/19430 (68%)] Loss: 0.011159
Train Epoch: 6 [13280/19430 (68%)] Loss: 0.012646
Train Epoch: 6 [13440/19430 (69%)] Loss: 0.009048
Train Epoch: 6 [13600/19430 (70%)] Loss: 0.002545
Train Epoch: 6 [13760/19430 (71%)] Loss: 0.002636
Train Epoch: 6 [13920/19430 (72%)] Loss: 0.013839
Train Epoch: 6 [14080/19430 (72%)] Loss: 0.008604
Train Epoch: 6 [14240/19430 (73%)] Loss: 0.001885
Train Epoch: 6 [14400/19430 (74%)] Loss: 0.001763
Train Epoch: 6 [14560/19430 (75%)] Loss: 0.034580
Train Epoch: 6 [14720/19430 (76%)] Loss: 0.005348
Train Epoch: 6 [14880/19430 (77%)] Loss: 0.006916
Train Epoch: 6 [15040/19430 (77%)] Loss: 0.045962
Train Epoch: 6 [15200/19430 (78%)] Loss: 0.088954
Train Epoch: 6 [15360/19430 (79%)] Loss: 0.001999
Train Epoch: 6 [15520/19430 (80%)] Loss: 0.015681
Train Epoch: 6 [15680/19430 (81%)] Loss: 0.002211
Train Epoch: 6 [15840/19430 (82%)] Loss: 0.033517
Train Epoch: 6 [16000/19430 (82%)] Loss: 0.002800
Train Epoch: 6 [16160/19430 (83%)] Loss: 0.001643
Train Epoch: 6 [16320/19430 (84%)] Loss: 0.003592
Train Epoch: 6 [16480/19430 (85%)] Loss: 0.043557
Train Epoch: 6 [16640/19430 (86%)] Loss: 0.002606
Train Epoch: 6 [16800/19430 (86%)] Loss: 0.016901
Train Epoch: 6 [16960/19430 (87%)] Loss: 0.014554
Train Epoch: 6 [17120/19430 (88%)] Loss: 0.003733
Train Epoch: 6 [17280/19430 (89%)] Loss: 0.001835
Train Epoch: 6 [17440/19430 (90%)] Loss: 0.003247
Train Epoch: 6 [17600/19430 (91%)] Loss: 0.007150
Train Epoch: 6 [17760/19430 (91%)] Loss: 0.001757
Train Epoch: 6 [17920/19430 (92%)] Loss: 0.003982
Train Epoch: 6 [18080/19430 (93%)] Loss: 0.001297
Train Epoch: 6 [18240/19430 (94%)] Loss: 0.003983
Train Epoch: 6 [18400/19430 (95%)] Loss: 0.001244
Train Epoch: 6 [18560/19430 (96%)] Loss: 0.006731
Train Epoch: 6 [18720/19430 (96%)] Loss: 0.001529
Train Epoch: 6 [18880/19430 (97%)] Loss: 0.003847
Train Epoch: 6 [19040/19430 (98%)] Loss: 0.005004
Train Epoch: 6 [19200/19430 (99%)] Loss: 0.090091
Train Epoch: 6 [19360/19430 (100%)] Loss: 0.001859
    epoch          : 6
    Train_loss     : 0.017427070622177684
    Train_accuracy : 0.9954255756578947
    Train_f1_score : 0.9954255819320679
    Val_loss       : 0.030444318532210995
    Val_accuracy   : 0.9935661764705882
    Val_f1_score   : 0.9935661554336548
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1030_145349/checkpoint-epoch6.pth ...
Train Epoch: 7 [0/19430 (0%)] Loss: 0.002876
Train Epoch: 7 [160/19430 (1%)] Loss: 0.000703
Train Epoch: 7 [320/19430 (2%)] Loss: 0.001288
Train Epoch: 7 [480/19430 (2%)] Loss: 0.001287
Train Epoch: 7 [640/19430 (3%)] Loss: 0.001503
Train Epoch: 7 [800/19430 (4%)] Loss: 0.018118
Train Epoch: 7 [960/19430 (5%)] Loss: 0.001902
Train Epoch: 7 [1120/19430 (6%)] Loss: 0.008286
Train Epoch: 7 [1280/19430 (7%)] Loss: 0.011027
Train Epoch: 7 [1440/19430 (7%)] Loss: 0.003042
Train Epoch: 7 [1600/19430 (8%)] Loss: 0.001434
Train Epoch: 7 [1760/19430 (9%)] Loss: 0.014802
Train Epoch: 7 [1920/19430 (10%)] Loss: 0.003805
Train Epoch: 7 [2080/19430 (11%)] Loss: 0.001081
Train Epoch: 7 [2240/19430 (12%)] Loss: 0.001257
Train Epoch: 7 [2400/19430 (12%)] Loss: 0.016292
Train Epoch: 7 [2560/19430 (13%)] Loss: 0.006110
Train Epoch: 7 [2720/19430 (14%)] Loss: 0.002167
Train Epoch: 7 [2880/19430 (15%)] Loss: 0.005433
Train Epoch: 7 [3040/19430 (16%)] Loss: 0.003157
Train Epoch: 7 [3200/19430 (16%)] Loss: 0.003962
Train Epoch: 7 [3360/19430 (17%)] Loss: 0.007057
Train Epoch: 7 [3520/19430 (18%)] Loss: 0.000878
Train Epoch: 7 [3680/19430 (19%)] Loss: 0.020978
Train Epoch: 7 [3840/19430 (20%)] Loss: 0.000890
Train Epoch: 7 [4000/19430 (21%)] Loss: 0.000711
Train Epoch: 7 [4160/19430 (21%)] Loss: 0.001029
Train Epoch: 7 [4320/19430 (22%)] Loss: 0.000864
Train Epoch: 7 [4480/19430 (23%)] Loss: 0.002504
Train Epoch: 7 [4640/19430 (24%)] Loss: 0.000842
Train Epoch: 7 [4800/19430 (25%)] Loss: 0.006769
Train Epoch: 7 [4960/19430 (26%)] Loss: 0.000729
Train Epoch: 7 [5120/19430 (26%)] Loss: 0.002392
Train Epoch: 7 [5280/19430 (27%)] Loss: 0.003732
Train Epoch: 7 [5440/19430 (28%)] Loss: 0.001326
Train Epoch: 7 [5600/19430 (29%)] Loss: 0.001469
Train Epoch: 7 [5760/19430 (30%)] Loss: 0.001151
Train Epoch: 7 [5920/19430 (30%)] Loss: 0.007292
Train Epoch: 7 [6080/19430 (31%)] Loss: 0.001706
Train Epoch: 7 [6240/19430 (32%)] Loss: 0.005864
Train Epoch: 7 [6400/19430 (33%)] Loss: 0.001965
Train Epoch: 7 [6560/19430 (34%)] Loss: 0.001164
Train Epoch: 7 [6720/19430 (35%)] Loss: 0.001375
Train Epoch: 7 [6880/19430 (35%)] Loss: 0.009032
Train Epoch: 7 [7040/19430 (36%)] Loss: 0.001060
Train Epoch: 7 [7200/19430 (37%)] Loss: 0.001424
Train Epoch: 7 [7360/19430 (38%)] Loss: 0.097826
Train Epoch: 7 [7520/19430 (39%)] Loss: 0.001442
Train Epoch: 7 [7680/19430 (40%)] Loss: 0.002622
Train Epoch: 7 [7840/19430 (40%)] Loss: 0.037539
Train Epoch: 7 [8000/19430 (41%)] Loss: 0.002890
Train Epoch: 7 [8160/19430 (42%)] Loss: 0.002056
Train Epoch: 7 [8320/19430 (43%)] Loss: 0.000594
Train Epoch: 7 [8480/19430 (44%)] Loss: 0.002672
Train Epoch: 7 [8640/19430 (44%)] Loss: 0.005979
Train Epoch: 7 [8800/19430 (45%)] Loss: 0.000632
Train Epoch: 7 [8960/19430 (46%)] Loss: 0.070269
Train Epoch: 7 [9120/19430 (47%)] Loss: 0.001994
Train Epoch: 7 [9280/19430 (48%)] Loss: 0.003145
Train Epoch: 7 [9440/19430 (49%)] Loss: 0.006374
Train Epoch: 7 [9600/19430 (49%)] Loss: 0.007519
Train Epoch: 7 [9760/19430 (50%)] Loss: 0.001243
Train Epoch: 7 [9920/19430 (51%)] Loss: 0.001759
Train Epoch: 7 [10080/19430 (52%)] Loss: 0.001085
Train Epoch: 7 [10240/19430 (53%)] Loss: 0.001970
Train Epoch: 7 [10400/19430 (54%)] Loss: 0.001551
Train Epoch: 7 [10560/19430 (54%)] Loss: 0.027520
Train Epoch: 7 [10720/19430 (55%)] Loss: 0.011029
Train Epoch: 7 [10880/19430 (56%)] Loss: 0.004876
Train Epoch: 7 [11040/19430 (57%)] Loss: 0.000714
Train Epoch: 7 [11200/19430 (58%)] Loss: 0.002931
Train Epoch: 7 [11360/19430 (58%)] Loss: 0.000561
Train Epoch: 7 [11520/19430 (59%)] Loss: 0.020133
Train Epoch: 7 [11680/19430 (60%)] Loss: 0.018978
Train Epoch: 7 [11840/19430 (61%)] Loss: 0.003947
Train Epoch: 7 [12000/19430 (62%)] Loss: 0.000479
Train Epoch: 7 [12160/19430 (63%)] Loss: 0.000884
Train Epoch: 7 [12320/19430 (63%)] Loss: 0.003913
Train Epoch: 7 [12480/19430 (64%)] Loss: 0.001776
Train Epoch: 7 [12640/19430 (65%)] Loss: 0.002385
Train Epoch: 7 [12800/19430 (66%)] Loss: 0.001590
Train Epoch: 7 [12960/19430 (67%)] Loss: 0.004348
Train Epoch: 7 [13120/19430 (68%)] Loss: 0.002160
Train Epoch: 7 [13280/19430 (68%)] Loss: 0.004253
Train Epoch: 7 [13440/19430 (69%)] Loss: 0.001070
Train Epoch: 7 [13600/19430 (70%)] Loss: 0.017796
Train Epoch: 7 [13760/19430 (71%)] Loss: 0.000799
Train Epoch: 7 [13920/19430 (72%)] Loss: 0.001061
Train Epoch: 7 [14080/19430 (72%)] Loss: 0.005878
Train Epoch: 7 [14240/19430 (73%)] Loss: 0.001155
Train Epoch: 7 [14400/19430 (74%)] Loss: 0.001174
Train Epoch: 7 [14560/19430 (75%)] Loss: 0.000434
Train Epoch: 7 [14720/19430 (76%)] Loss: 0.001766
Train Epoch: 7 [14880/19430 (77%)] Loss: 0.005058
Train Epoch: 7 [15040/19430 (77%)] Loss: 0.002759
Train Epoch: 7 [15200/19430 (78%)] Loss: 0.000538
Train Epoch: 7 [15360/19430 (79%)] Loss: 0.012330
Train Epoch: 7 [15520/19430 (80%)] Loss: 0.003079
Train Epoch: 7 [15680/19430 (81%)] Loss: 0.002732
Train Epoch: 7 [15840/19430 (82%)] Loss: 0.002261
Train Epoch: 7 [16000/19430 (82%)] Loss: 0.007617
Train Epoch: 7 [16160/19430 (83%)] Loss: 0.002084
Train Epoch: 7 [16320/19430 (84%)] Loss: 0.000866
Train Epoch: 7 [16480/19430 (85%)] Loss: 0.033147
Train Epoch: 7 [16640/19430 (86%)] Loss: 0.001420
Train Epoch: 7 [16800/19430 (86%)] Loss: 0.000891
Train Epoch: 7 [16960/19430 (87%)] Loss: 0.001671
Train Epoch: 7 [17120/19430 (88%)] Loss: 0.004082
Train Epoch: 7 [17280/19430 (89%)] Loss: 0.000791
Train Epoch: 7 [17440/19430 (90%)] Loss: 0.001562
Train Epoch: 7 [17600/19430 (91%)] Loss: 0.000727
Train Epoch: 7 [17760/19430 (91%)] Loss: 0.000547
Train Epoch: 7 [17920/19430 (92%)] Loss: 0.001177
Train Epoch: 7 [18080/19430 (93%)] Loss: 0.005115
Train Epoch: 7 [18240/19430 (94%)] Loss: 0.000841
Train Epoch: 7 [18400/19430 (95%)] Loss: 0.004800
Train Epoch: 7 [18560/19430 (96%)] Loss: 0.000878
Train Epoch: 7 [18720/19430 (96%)] Loss: 0.000716
Train Epoch: 7 [18880/19430 (97%)] Loss: 0.162786
Train Epoch: 7 [19040/19430 (98%)] Loss: 0.003063
Train Epoch: 7 [19200/19430 (99%)] Loss: 0.002804
Train Epoch: 7 [19360/19430 (100%)] Loss: 0.000960
    epoch          : 7
    Train_loss     : 0.009940849161227417
    Train_accuracy : 0.9972245065789473
    Train_f1_score : 0.9972245097160339
    Val_loss       : 0.01676460998286904
    Val_accuracy   : 0.9963235294117647
    Val_f1_score   : 0.9963235259056091
Train Epoch: 8 [0/19430 (0%)] Loss: 0.001966
Train Epoch: 8 [160/19430 (1%)] Loss: 0.244384
Train Epoch: 8 [320/19430 (2%)] Loss: 0.073122
Train Epoch: 8 [480/19430 (2%)] Loss: 0.008862
Train Epoch: 8 [640/19430 (3%)] Loss: 0.007931
Train Epoch: 8 [800/19430 (4%)] Loss: 0.061325
Train Epoch: 8 [960/19430 (5%)] Loss: 0.004081
Train Epoch: 8 [1120/19430 (6%)] Loss: 0.022544
Train Epoch: 8 [1280/19430 (7%)] Loss: 0.009741
Train Epoch: 8 [1440/19430 (7%)] Loss: 0.003368
Train Epoch: 8 [1600/19430 (8%)] Loss: 0.036119
Train Epoch: 8 [1760/19430 (9%)] Loss: 0.005079
Train Epoch: 8 [1920/19430 (10%)] Loss: 0.006695
Train Epoch: 8 [2080/19430 (11%)] Loss: 0.002658
Train Epoch: 8 [2240/19430 (12%)] Loss: 0.000827
Train Epoch: 8 [2400/19430 (12%)] Loss: 0.003245
Train Epoch: 8 [2560/19430 (13%)] Loss: 0.002353
Train Epoch: 8 [2720/19430 (14%)] Loss: 0.003379
Train Epoch: 8 [2880/19430 (15%)] Loss: 0.004063
Train Epoch: 8 [3040/19430 (16%)] Loss: 0.007942
Train Epoch: 8 [3200/19430 (16%)] Loss: 0.001447
Train Epoch: 8 [3360/19430 (17%)] Loss: 0.001268
Train Epoch: 8 [3520/19430 (18%)] Loss: 0.008182
Train Epoch: 8 [3680/19430 (19%)] Loss: 0.055310
Train Epoch: 8 [3840/19430 (20%)] Loss: 0.009306
Train Epoch: 8 [4000/19430 (21%)] Loss: 0.003704
Train Epoch: 8 [4160/19430 (21%)] Loss: 0.000523
Train Epoch: 8 [4320/19430 (22%)] Loss: 0.003864
Train Epoch: 8 [4480/19430 (23%)] Loss: 0.004158
Train Epoch: 8 [4640/19430 (24%)] Loss: 0.000629
Train Epoch: 8 [4800/19430 (25%)] Loss: 0.000591
Train Epoch: 8 [4960/19430 (26%)] Loss: 0.000953
Train Epoch: 8 [5120/19430 (26%)] Loss: 0.002537
Train Epoch: 8 [5280/19430 (27%)] Loss: 0.034596
Train Epoch: 8 [5440/19430 (28%)] Loss: 0.001361
Train Epoch: 8 [5600/19430 (29%)] Loss: 0.005362
Train Epoch: 8 [5760/19430 (30%)] Loss: 0.004897
Train Epoch: 8 [5920/19430 (30%)] Loss: 0.002797
Train Epoch: 8 [6080/19430 (31%)] Loss: 0.001333
Train Epoch: 8 [6240/19430 (32%)] Loss: 0.002685
Train Epoch: 8 [6400/19430 (33%)] Loss: 0.003732
Train Epoch: 8 [6560/19430 (34%)] Loss: 0.003261
Train Epoch: 8 [6720/19430 (35%)] Loss: 0.001283
Train Epoch: 8 [6880/19430 (35%)] Loss: 0.001286
Train Epoch: 8 [7040/19430 (36%)] Loss: 0.001251
Train Epoch: 8 [7200/19430 (37%)] Loss: 0.001286
Train Epoch: 8 [7360/19430 (38%)] Loss: 0.027091
Train Epoch: 8 [7520/19430 (39%)] Loss: 0.004688
Train Epoch: 8 [7680/19430 (40%)] Loss: 0.086889
Train Epoch: 8 [7840/19430 (40%)] Loss: 0.083903
Train Epoch: 8 [8000/19430 (41%)] Loss: 0.009093
Train Epoch: 8 [8160/19430 (42%)] Loss: 0.004556
Train Epoch: 8 [8320/19430 (43%)] Loss: 0.002621
Train Epoch: 8 [8480/19430 (44%)] Loss: 0.002203
Train Epoch: 8 [8640/19430 (44%)] Loss: 0.007632
Train Epoch: 8 [8800/19430 (45%)] Loss: 0.004930
Train Epoch: 8 [8960/19430 (46%)] Loss: 0.002056
Train Epoch: 8 [9120/19430 (47%)] Loss: 0.001381
Train Epoch: 8 [9280/19430 (48%)] Loss: 0.002716
Train Epoch: 8 [9440/19430 (49%)] Loss: 0.002924
Train Epoch: 8 [9600/19430 (49%)] Loss: 0.002174
Train Epoch: 8 [9760/19430 (50%)] Loss: 0.003966
Train Epoch: 8 [9920/19430 (51%)] Loss: 0.106685
Train Epoch: 8 [10080/19430 (52%)] Loss: 0.002393
Train Epoch: 8 [10240/19430 (53%)] Loss: 0.002950
Train Epoch: 8 [10400/19430 (54%)] Loss: 0.014458
Train Epoch: 8 [10560/19430 (54%)] Loss: 0.001196
Train Epoch: 8 [10720/19430 (55%)] Loss: 0.058404
Train Epoch: 8 [10880/19430 (56%)] Loss: 0.007449
Train Epoch: 8 [11040/19430 (57%)] Loss: 0.003259
Train Epoch: 8 [11200/19430 (58%)] Loss: 0.003248
Train Epoch: 8 [11360/19430 (58%)] Loss: 0.005270
Train Epoch: 8 [11520/19430 (59%)] Loss: 0.006732
Train Epoch: 8 [11680/19430 (60%)] Loss: 0.009371
Train Epoch: 8 [11840/19430 (61%)] Loss: 0.169973
Train Epoch: 8 [12000/19430 (62%)] Loss: 0.012429
Train Epoch: 8 [12160/19430 (63%)] Loss: 0.000673
Train Epoch: 8 [12320/19430 (63%)] Loss: 0.000806
Train Epoch: 8 [12480/19430 (64%)] Loss: 0.013959
Train Epoch: 8 [12640/19430 (65%)] Loss: 0.005775
Train Epoch: 8 [12800/19430 (66%)] Loss: 0.001247
Train Epoch: 8 [12960/19430 (67%)] Loss: 0.354981
Train Epoch: 8 [13120/19430 (68%)] Loss: 0.002251
Train Epoch: 8 [13280/19430 (68%)] Loss: 0.001113
Train Epoch: 8 [13440/19430 (69%)] Loss: 0.001115
Train Epoch: 8 [13600/19430 (70%)] Loss: 0.003813
Train Epoch: 8 [13760/19430 (71%)] Loss: 0.004301
Train Epoch: 8 [13920/19430 (72%)] Loss: 0.001789
Train Epoch: 8 [14080/19430 (72%)] Loss: 0.002268
Train Epoch: 8 [14240/19430 (73%)] Loss: 0.109876
Train Epoch: 8 [14400/19430 (74%)] Loss: 0.000980
Train Epoch: 8 [14560/19430 (75%)] Loss: 0.003699
Train Epoch: 8 [14720/19430 (76%)] Loss: 0.000889
Train Epoch: 8 [14880/19430 (77%)] Loss: 0.004276
Train Epoch: 8 [15040/19430 (77%)] Loss: 0.004451
Train Epoch: 8 [15200/19430 (78%)] Loss: 0.001260
Train Epoch: 8 [15360/19430 (79%)] Loss: 0.001044
Train Epoch: 8 [15520/19430 (80%)] Loss: 0.002085
Train Epoch: 8 [15680/19430 (81%)] Loss: 0.002934
Train Epoch: 8 [15840/19430 (82%)] Loss: 0.007835
Train Epoch: 8 [16000/19430 (82%)] Loss: 0.001006
Train Epoch: 8 [16160/19430 (83%)] Loss: 0.001744
Train Epoch: 8 [16320/19430 (84%)] Loss: 0.001150
Train Epoch: 8 [16480/19430 (85%)] Loss: 0.003655
Train Epoch: 8 [16640/19430 (86%)] Loss: 0.001767
Train Epoch: 8 [16800/19430 (86%)] Loss: 0.006111
Train Epoch: 8 [16960/19430 (87%)] Loss: 0.000940
Train Epoch: 8 [17120/19430 (88%)] Loss: 0.005507
Train Epoch: 8 [17280/19430 (89%)] Loss: 0.000925
Train Epoch: 8 [17440/19430 (90%)] Loss: 0.026671
Train Epoch: 8 [17600/19430 (91%)] Loss: 0.000587
Train Epoch: 8 [17760/19430 (91%)] Loss: 0.000807
Train Epoch: 8 [17920/19430 (92%)] Loss: 0.002908
Train Epoch: 8 [18080/19430 (93%)] Loss: 0.005076
Train Epoch: 8 [18240/19430 (94%)] Loss: 0.001110
Train Epoch: 8 [18400/19430 (95%)] Loss: 0.051171
Train Epoch: 8 [18560/19430 (96%)] Loss: 0.006466
Train Epoch: 8 [18720/19430 (96%)] Loss: 0.006448
Train Epoch: 8 [18880/19430 (97%)] Loss: 0.057061
Train Epoch: 8 [19040/19430 (98%)] Loss: 0.005724
Train Epoch: 8 [19200/19430 (99%)] Loss: 0.004450
Train Epoch: 8 [19360/19430 (100%)] Loss: 0.000903
    epoch          : 8
    Train_loss     : 0.018562133242037242
    Train_accuracy : 0.9951685855263158
    Train_f1_score : 0.9951685667037964
    Val_loss       : 0.04175958586668404
    Val_accuracy   : 0.9847032563025211
    Val_f1_score   : 0.9847033023834229
Train Epoch: 9 [0/19430 (0%)] Loss: 0.001308
Train Epoch: 9 [160/19430 (1%)] Loss: 0.016317
Train Epoch: 9 [320/19430 (2%)] Loss: 0.001265
Train Epoch: 9 [480/19430 (2%)] Loss: 0.003751
Train Epoch: 9 [640/19430 (3%)] Loss: 0.042876
Train Epoch: 9 [800/19430 (4%)] Loss: 0.000959
Train Epoch: 9 [960/19430 (5%)] Loss: 0.034187
Train Epoch: 9 [1120/19430 (6%)] Loss: 0.001333
Train Epoch: 9 [1280/19430 (7%)] Loss: 0.017520
Train Epoch: 9 [1440/19430 (7%)] Loss: 0.004498
Train Epoch: 9 [1600/19430 (8%)] Loss: 0.001973
Train Epoch: 9 [1760/19430 (9%)] Loss: 0.010000
Train Epoch: 9 [1920/19430 (10%)] Loss: 0.008353
Train Epoch: 9 [2080/19430 (11%)] Loss: 0.022610
Train Epoch: 9 [2240/19430 (12%)] Loss: 0.001936
Train Epoch: 9 [2400/19430 (12%)] Loss: 0.002053
Train Epoch: 9 [2560/19430 (13%)] Loss: 0.005237
Train Epoch: 9 [2720/19430 (14%)] Loss: 0.002098
Train Epoch: 9 [2880/19430 (15%)] Loss: 0.010565
Train Epoch: 9 [3040/19430 (16%)] Loss: 0.002102
Train Epoch: 9 [3200/19430 (16%)] Loss: 0.006380
Train Epoch: 9 [3360/19430 (17%)] Loss: 0.001744
Train Epoch: 9 [3520/19430 (18%)] Loss: 0.014943
Train Epoch: 9 [3680/19430 (19%)] Loss: 0.000765
Train Epoch: 9 [3840/19430 (20%)] Loss: 0.002925
Train Epoch: 9 [4000/19430 (21%)] Loss: 0.004544
Train Epoch: 9 [4160/19430 (21%)] Loss: 0.001882
Train Epoch: 9 [4320/19430 (22%)] Loss: 0.005527
Train Epoch: 9 [4480/19430 (23%)] Loss: 0.000959
Train Epoch: 9 [4640/19430 (24%)] Loss: 0.010125
Train Epoch: 9 [4800/19430 (25%)] Loss: 0.004022
Train Epoch: 9 [4960/19430 (26%)] Loss: 0.001584
Train Epoch: 9 [5120/19430 (26%)] Loss: 0.001104
Train Epoch: 9 [5280/19430 (27%)] Loss: 0.005381
Train Epoch: 9 [5440/19430 (28%)] Loss: 0.024210
Train Epoch: 9 [5600/19430 (29%)] Loss: 0.003815
Train Epoch: 9 [5760/19430 (30%)] Loss: 0.002487
Train Epoch: 9 [5920/19430 (30%)] Loss: 0.002102
Train Epoch: 9 [6080/19430 (31%)] Loss: 0.000760
Train Epoch: 9 [6240/19430 (32%)] Loss: 0.004837
Train Epoch: 9 [6400/19430 (33%)] Loss: 0.007957
Train Epoch: 9 [6560/19430 (34%)] Loss: 0.006187
Train Epoch: 9 [6720/19430 (35%)] Loss: 0.000850
Train Epoch: 9 [6880/19430 (35%)] Loss: 0.001691
Train Epoch: 9 [7040/19430 (36%)] Loss: 0.045066
Train Epoch: 9 [7200/19430 (37%)] Loss: 0.001229
Train Epoch: 9 [7360/19430 (38%)] Loss: 0.000780
Train Epoch: 9 [7520/19430 (39%)] Loss: 0.012861
Train Epoch: 9 [7680/19430 (40%)] Loss: 0.005601
Train Epoch: 9 [7840/19430 (40%)] Loss: 0.001009
Train Epoch: 9 [8000/19430 (41%)] Loss: 0.000636
Train Epoch: 9 [8160/19430 (42%)] Loss: 0.003503
Train Epoch: 9 [8320/19430 (43%)] Loss: 0.002775
Train Epoch: 9 [8480/19430 (44%)] Loss: 0.000915
Train Epoch: 9 [8640/19430 (44%)] Loss: 0.001729
Train Epoch: 9 [8800/19430 (45%)] Loss: 0.001054
Train Epoch: 9 [8960/19430 (46%)] Loss: 0.001606
Train Epoch: 9 [9120/19430 (47%)] Loss: 0.002150
Train Epoch: 9 [9280/19430 (48%)] Loss: 0.000915
Train Epoch: 9 [9440/19430 (49%)] Loss: 0.003531
Train Epoch: 9 [9600/19430 (49%)] Loss: 0.002039
Train Epoch: 9 [9760/19430 (50%)] Loss: 0.000899
Train Epoch: 9 [9920/19430 (51%)] Loss: 0.002918
Train Epoch: 9 [10080/19430 (52%)] Loss: 0.001751
Train Epoch: 9 [10240/19430 (53%)] Loss: 0.004393
Train Epoch: 9 [10400/19430 (54%)] Loss: 0.025052
Train Epoch: 9 [10560/19430 (54%)] Loss: 0.000537
Train Epoch: 9 [10720/19430 (55%)] Loss: 0.001512
Train Epoch: 9 [10880/19430 (56%)] Loss: 0.025257
Train Epoch: 9 [11040/19430 (57%)] Loss: 0.000725
Train Epoch: 9 [11200/19430 (58%)] Loss: 0.005068
Train Epoch: 9 [11360/19430 (58%)] Loss: 0.000668
Train Epoch: 9 [11520/19430 (59%)] Loss: 0.006012
Train Epoch: 9 [11680/19430 (60%)] Loss: 0.001378
Train Epoch: 9 [11840/19430 (61%)] Loss: 0.000849
Train Epoch: 9 [12000/19430 (62%)] Loss: 0.000916
Train Epoch: 9 [12160/19430 (63%)] Loss: 0.002656
Train Epoch: 9 [12320/19430 (63%)] Loss: 0.001320
Train Epoch: 9 [12480/19430 (64%)] Loss: 0.001920
Train Epoch: 9 [12640/19430 (65%)] Loss: 0.000999
Train Epoch: 9 [12800/19430 (66%)] Loss: 0.000345
Train Epoch: 9 [12960/19430 (67%)] Loss: 0.000377
Train Epoch: 9 [13120/19430 (68%)] Loss: 0.004108
Train Epoch: 9 [13280/19430 (68%)] Loss: 0.000793
Train Epoch: 9 [13440/19430 (69%)] Loss: 0.001991
Train Epoch: 9 [13600/19430 (70%)] Loss: 0.002547
Train Epoch: 9 [13760/19430 (71%)] Loss: 0.000760
Train Epoch: 9 [13920/19430 (72%)] Loss: 0.000724
Train Epoch: 9 [14080/19430 (72%)] Loss: 0.005681
Train Epoch: 9 [14240/19430 (73%)] Loss: 0.000522
Train Epoch: 9 [14400/19430 (74%)] Loss: 0.003295
Train Epoch: 9 [14560/19430 (75%)] Loss: 0.000680
Train Epoch: 9 [14720/19430 (76%)] Loss: 0.000915
Train Epoch: 9 [14880/19430 (77%)] Loss: 0.003656
Train Epoch: 9 [15040/19430 (77%)] Loss: 0.001659
Train Epoch: 9 [15200/19430 (78%)] Loss: 0.001014
Train Epoch: 9 [15360/19430 (79%)] Loss: 0.000505
Train Epoch: 9 [15520/19430 (80%)] Loss: 0.002696
Train Epoch: 9 [15680/19430 (81%)] Loss: 0.002186
Train Epoch: 9 [15840/19430 (82%)] Loss: 0.001167
Train Epoch: 9 [16000/19430 (82%)] Loss: 0.000559
Train Epoch: 9 [16160/19430 (83%)] Loss: 0.000543
Train Epoch: 9 [16320/19430 (84%)] Loss: 0.002315
Train Epoch: 9 [16480/19430 (85%)] Loss: 0.000577
Train Epoch: 9 [16640/19430 (86%)] Loss: 0.000572
Train Epoch: 9 [16800/19430 (86%)] Loss: 0.000832
Train Epoch: 9 [16960/19430 (87%)] Loss: 0.000731
Train Epoch: 9 [17120/19430 (88%)] Loss: 0.004244
Train Epoch: 9 [17280/19430 (89%)] Loss: 0.000740
Train Epoch: 9 [17440/19430 (90%)] Loss: 0.009555
Train Epoch: 9 [17600/19430 (91%)] Loss: 0.001743
Train Epoch: 9 [17760/19430 (91%)] Loss: 0.001734
Train Epoch: 9 [17920/19430 (92%)] Loss: 0.001866
Train Epoch: 9 [18080/19430 (93%)] Loss: 0.000448
Train Epoch: 9 [18240/19430 (94%)] Loss: 0.001414
Train Epoch: 9 [18400/19430 (95%)] Loss: 0.001426
Train Epoch: 9 [18560/19430 (96%)] Loss: 0.001244
Train Epoch: 9 [18720/19430 (96%)] Loss: 0.003496
Train Epoch: 9 [18880/19430 (97%)] Loss: 0.001004
Train Epoch: 9 [19040/19430 (98%)] Loss: 0.000389
Train Epoch: 9 [19200/19430 (99%)] Loss: 0.002244
Train Epoch: 9 [19360/19430 (100%)] Loss: 0.008732
    epoch          : 9
    Train_loss     : 0.00651916343500297
    Train_accuracy : 0.9988178453947368
    Train_f1_score : 0.9988178610801697
    Val_loss       : 0.037983935624652077
    Val_accuracy   : 0.9898897058823529
    Val_f1_score   : 0.9898896813392639
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1030_145349/checkpoint-epoch9.pth ...
Train Epoch: 10 [0/19430 (0%)] Loss: 0.000651
Train Epoch: 10 [160/19430 (1%)] Loss: 0.000895
Train Epoch: 10 [320/19430 (2%)] Loss: 0.001092
Train Epoch: 10 [480/19430 (2%)] Loss: 0.005003
Train Epoch: 10 [640/19430 (3%)] Loss: 0.011731
Train Epoch: 10 [800/19430 (4%)] Loss: 0.076466
Train Epoch: 10 [960/19430 (5%)] Loss: 0.008393
Train Epoch: 10 [1120/19430 (6%)] Loss: 0.002098
Train Epoch: 10 [1280/19430 (7%)] Loss: 0.003044
Train Epoch: 10 [1440/19430 (7%)] Loss: 0.025844
Train Epoch: 10 [1600/19430 (8%)] Loss: 0.001144
Train Epoch: 10 [1760/19430 (9%)] Loss: 0.000503
Train Epoch: 10 [1920/19430 (10%)] Loss: 0.001001
Train Epoch: 10 [2080/19430 (11%)] Loss: 0.030900
Train Epoch: 10 [2240/19430 (12%)] Loss: 0.001084
Train Epoch: 10 [2400/19430 (12%)] Loss: 0.002282
Train Epoch: 10 [2560/19430 (13%)] Loss: 0.001576
Train Epoch: 10 [2720/19430 (14%)] Loss: 0.001764
Train Epoch: 10 [2880/19430 (15%)] Loss: 0.001257
Train Epoch: 10 [3040/19430 (16%)] Loss: 0.000285
Train Epoch: 10 [3200/19430 (16%)] Loss: 0.000464
Train Epoch: 10 [3360/19430 (17%)] Loss: 0.002478
Train Epoch: 10 [3520/19430 (18%)] Loss: 0.000749
Train Epoch: 10 [3680/19430 (19%)] Loss: 0.001880
Train Epoch: 10 [3840/19430 (20%)] Loss: 0.001108
Train Epoch: 10 [4000/19430 (21%)] Loss: 0.001380
Train Epoch: 10 [4160/19430 (21%)] Loss: 0.000949
Train Epoch: 10 [4320/19430 (22%)] Loss: 0.003135
Train Epoch: 10 [4480/19430 (23%)] Loss: 0.000817
Train Epoch: 10 [4640/19430 (24%)] Loss: 0.000732
Train Epoch: 10 [4800/19430 (25%)] Loss: 0.008341
Train Epoch: 10 [4960/19430 (26%)] Loss: 0.000649
Train Epoch: 10 [5120/19430 (26%)] Loss: 0.000639
Train Epoch: 10 [5280/19430 (27%)] Loss: 0.068262
Train Epoch: 10 [5440/19430 (28%)] Loss: 0.001039
Train Epoch: 10 [5600/19430 (29%)] Loss: 0.000604
Train Epoch: 10 [5760/19430 (30%)] Loss: 0.000383
Train Epoch: 10 [5920/19430 (30%)] Loss: 0.000535
Train Epoch: 10 [6080/19430 (31%)] Loss: 0.003194
Train Epoch: 10 [6240/19430 (32%)] Loss: 0.000466
Train Epoch: 10 [6400/19430 (33%)] Loss: 0.000889
Train Epoch: 10 [6560/19430 (34%)] Loss: 0.000773
Train Epoch: 10 [6720/19430 (35%)] Loss: 0.000356
Train Epoch: 10 [6880/19430 (35%)] Loss: 0.000394
Train Epoch: 10 [7040/19430 (36%)] Loss: 0.008580
Train Epoch: 10 [7200/19430 (37%)] Loss: 0.000813
Train Epoch: 10 [7360/19430 (38%)] Loss: 0.001472
Train Epoch: 10 [7520/19430 (39%)] Loss: 0.000500
Train Epoch: 10 [7680/19430 (40%)] Loss: 0.000530
Train Epoch: 10 [7840/19430 (40%)] Loss: 0.010431
Train Epoch: 10 [8000/19430 (41%)] Loss: 0.001336
Train Epoch: 10 [8160/19430 (42%)] Loss: 0.000586
Train Epoch: 10 [8320/19430 (43%)] Loss: 0.006940
Train Epoch: 10 [8480/19430 (44%)] Loss: 0.000219
Train Epoch: 10 [8640/19430 (44%)] Loss: 0.019374
Train Epoch: 10 [8800/19430 (45%)] Loss: 0.000878
Train Epoch: 10 [8960/19430 (46%)] Loss: 0.002867
Train Epoch: 10 [9120/19430 (47%)] Loss: 0.007435
Train Epoch: 10 [9280/19430 (48%)] Loss: 0.000587
Train Epoch: 10 [9440/19430 (49%)] Loss: 0.001016
Train Epoch: 10 [9600/19430 (49%)] Loss: 0.001258
Train Epoch: 10 [9760/19430 (50%)] Loss: 0.000312
Train Epoch: 10 [9920/19430 (51%)] Loss: 0.000681
Train Epoch: 10 [10080/19430 (52%)] Loss: 0.000430
Train Epoch: 10 [10240/19430 (53%)] Loss: 0.000614
Train Epoch: 10 [10400/19430 (54%)] Loss: 0.000731
Train Epoch: 10 [10560/19430 (54%)] Loss: 0.000787
Train Epoch: 10 [10720/19430 (55%)] Loss: 0.000387
Train Epoch: 10 [10880/19430 (56%)] Loss: 0.001524
Train Epoch: 10 [11040/19430 (57%)] Loss: 0.000412
Train Epoch: 10 [11200/19430 (58%)] Loss: 0.000202
Train Epoch: 10 [11360/19430 (58%)] Loss: 0.001975
Train Epoch: 10 [11520/19430 (59%)] Loss: 0.001513
Train Epoch: 10 [11680/19430 (60%)] Loss: 0.009219
Train Epoch: 10 [11840/19430 (61%)] Loss: 0.002339
Train Epoch: 10 [12000/19430 (62%)] Loss: 0.000773
Train Epoch: 10 [12160/19430 (63%)] Loss: 0.000658
Train Epoch: 10 [12320/19430 (63%)] Loss: 0.000243
Train Epoch: 10 [12480/19430 (64%)] Loss: 0.002171
Train Epoch: 10 [12640/19430 (65%)] Loss: 0.000804
Train Epoch: 10 [12800/19430 (66%)] Loss: 0.001560
Train Epoch: 10 [12960/19430 (67%)] Loss: 0.000211
Train Epoch: 10 [13120/19430 (68%)] Loss: 0.000356
Train Epoch: 10 [13280/19430 (68%)] Loss: 0.001927
Train Epoch: 10 [13440/19430 (69%)] Loss: 0.000361
Train Epoch: 10 [13600/19430 (70%)] Loss: 0.003909
Train Epoch: 10 [13760/19430 (71%)] Loss: 0.000442
Train Epoch: 10 [13920/19430 (72%)] Loss: 0.001227
Train Epoch: 10 [14080/19430 (72%)] Loss: 0.000294
Train Epoch: 10 [14240/19430 (73%)] Loss: 0.013974
Train Epoch: 10 [14400/19430 (74%)] Loss: 0.000664
Train Epoch: 10 [14560/19430 (75%)] Loss: 0.000241
Train Epoch: 10 [14720/19430 (76%)] Loss: 0.000254
Train Epoch: 10 [14880/19430 (77%)] Loss: 0.001319
Train Epoch: 10 [15040/19430 (77%)] Loss: 0.000229
Train Epoch: 10 [15200/19430 (78%)] Loss: 0.000518
Train Epoch: 10 [15360/19430 (79%)] Loss: 0.000434
Train Epoch: 10 [15520/19430 (80%)] Loss: 0.000351
Train Epoch: 10 [15680/19430 (81%)] Loss: 0.001473
Train Epoch: 10 [15840/19430 (82%)] Loss: 0.000193
Train Epoch: 10 [16000/19430 (82%)] Loss: 0.000278
Train Epoch: 10 [16160/19430 (83%)] Loss: 0.001340
Train Epoch: 10 [16320/19430 (84%)] Loss: 0.008501
Train Epoch: 10 [16480/19430 (85%)] Loss: 0.002710
Train Epoch: 10 [16640/19430 (86%)] Loss: 0.000890
Train Epoch: 10 [16800/19430 (86%)] Loss: 0.016701
Train Epoch: 10 [16960/19430 (87%)] Loss: 0.000785
Train Epoch: 10 [17120/19430 (88%)] Loss: 0.000350
Train Epoch: 10 [17280/19430 (89%)] Loss: 0.000459
Train Epoch: 10 [17440/19430 (90%)] Loss: 0.000542
Train Epoch: 10 [17600/19430 (91%)] Loss: 0.001947
Train Epoch: 10 [17760/19430 (91%)] Loss: 0.001052
Train Epoch: 10 [17920/19430 (92%)] Loss: 0.000644
Train Epoch: 10 [18080/19430 (93%)] Loss: 0.001664
Train Epoch: 10 [18240/19430 (94%)] Loss: 0.001566
Train Epoch: 10 [18400/19430 (95%)] Loss: 0.001350
Train Epoch: 10 [18560/19430 (96%)] Loss: 0.000673
Train Epoch: 10 [18720/19430 (96%)] Loss: 0.000649
Train Epoch: 10 [18880/19430 (97%)] Loss: 0.001190
Train Epoch: 10 [19040/19430 (98%)] Loss: 0.008937
Train Epoch: 10 [19200/19430 (99%)] Loss: 0.000354
Train Epoch: 10 [19360/19430 (100%)] Loss: 0.001603
    epoch          : 10
    Train_loss     : 0.00386429354138703
    Train_accuracy : 0.9990748355263158
    Train_f1_score : 0.9990748167037964
    Val_loss       : 0.02011732850897876
    Val_accuracy   : 0.9935661764705882
    Val_f1_score   : 0.9935661554336548
Train Epoch: 11 [0/19430 (0%)] Loss: 0.042304
Train Epoch: 11 [160/19430 (1%)] Loss: 0.007930
Train Epoch: 11 [320/19430 (2%)] Loss: 0.001020
Train Epoch: 11 [480/19430 (2%)] Loss: 0.000346
Train Epoch: 11 [640/19430 (3%)] Loss: 0.000155
Train Epoch: 11 [800/19430 (4%)] Loss: 0.000308
Train Epoch: 11 [960/19430 (5%)] Loss: 0.002104
Train Epoch: 11 [1120/19430 (6%)] Loss: 0.023406
Train Epoch: 11 [1280/19430 (7%)] Loss: 0.000417
Train Epoch: 11 [1440/19430 (7%)] Loss: 0.001730
Train Epoch: 11 [1600/19430 (8%)] Loss: 0.000461
Train Epoch: 11 [1760/19430 (9%)] Loss: 0.007389
Train Epoch: 11 [1920/19430 (10%)] Loss: 0.001086
Train Epoch: 11 [2080/19430 (11%)] Loss: 0.000454
Train Epoch: 11 [2240/19430 (12%)] Loss: 0.000421
Train Epoch: 11 [2400/19430 (12%)] Loss: 0.001872
Train Epoch: 11 [2560/19430 (13%)] Loss: 0.005148
Train Epoch: 11 [2720/19430 (14%)] Loss: 0.000649
Train Epoch: 11 [2880/19430 (15%)] Loss: 0.000779
Train Epoch: 11 [3040/19430 (16%)] Loss: 0.000914
Train Epoch: 11 [3200/19430 (16%)] Loss: 0.001244
Train Epoch: 11 [3360/19430 (17%)] Loss: 0.001131
Train Epoch: 11 [3520/19430 (18%)] Loss: 0.000321
Train Epoch: 11 [3680/19430 (19%)] Loss: 0.004106
Train Epoch: 11 [3840/19430 (20%)] Loss: 0.001464
Train Epoch: 11 [4000/19430 (21%)] Loss: 0.000519
Train Epoch: 11 [4160/19430 (21%)] Loss: 0.003803
Train Epoch: 11 [4320/19430 (22%)] Loss: 0.000745
Train Epoch: 11 [4480/19430 (23%)] Loss: 0.000980
Train Epoch: 11 [4640/19430 (24%)] Loss: 0.000875
Train Epoch: 11 [4800/19430 (25%)] Loss: 0.001157
Train Epoch: 11 [4960/19430 (26%)] Loss: 0.001445
Train Epoch: 11 [5120/19430 (26%)] Loss: 0.000497
Train Epoch: 11 [5280/19430 (27%)] Loss: 0.005430
Train Epoch: 11 [5440/19430 (28%)] Loss: 0.008458
Train Epoch: 11 [5600/19430 (29%)] Loss: 0.000559
Train Epoch: 11 [5760/19430 (30%)] Loss: 0.002745
Train Epoch: 11 [5920/19430 (30%)] Loss: 0.000512
Train Epoch: 11 [6080/19430 (31%)] Loss: 0.000562
Train Epoch: 11 [6240/19430 (32%)] Loss: 0.015631
Train Epoch: 11 [6400/19430 (33%)] Loss: 0.000424
Train Epoch: 11 [6560/19430 (34%)] Loss: 0.044359
Train Epoch: 11 [6720/19430 (35%)] Loss: 0.002623
Train Epoch: 11 [6880/19430 (35%)] Loss: 0.000857
Train Epoch: 11 [7040/19430 (36%)] Loss: 0.005045
Train Epoch: 11 [7200/19430 (37%)] Loss: 0.001027
Train Epoch: 11 [7360/19430 (38%)] Loss: 0.002255
Train Epoch: 11 [7520/19430 (39%)] Loss: 0.000574
Train Epoch: 11 [7680/19430 (40%)] Loss: 0.005368
Train Epoch: 11 [7840/19430 (40%)] Loss: 0.000152
Train Epoch: 11 [8000/19430 (41%)] Loss: 0.000612
Train Epoch: 11 [8160/19430 (42%)] Loss: 0.001983
Train Epoch: 11 [8320/19430 (43%)] Loss: 0.003293
Train Epoch: 11 [8480/19430 (44%)] Loss: 0.027333
Train Epoch: 11 [8640/19430 (44%)] Loss: 0.000702
Train Epoch: 11 [8800/19430 (45%)] Loss: 0.000468
Train Epoch: 11 [8960/19430 (46%)] Loss: 0.000952
Train Epoch: 11 [9120/19430 (47%)] Loss: 0.000595
Train Epoch: 11 [9280/19430 (48%)] Loss: 0.000543
Train Epoch: 11 [9440/19430 (49%)] Loss: 0.000197
Train Epoch: 11 [9600/19430 (49%)] Loss: 0.000240
Train Epoch: 11 [9760/19430 (50%)] Loss: 0.000691
Train Epoch: 11 [9920/19430 (51%)] Loss: 0.103478
Train Epoch: 11 [10080/19430 (52%)] Loss: 0.000266
Train Epoch: 11 [10240/19430 (53%)] Loss: 0.000426
Train Epoch: 11 [10400/19430 (54%)] Loss: 0.007201
Train Epoch: 11 [10560/19430 (54%)] Loss: 0.001835
Train Epoch: 11 [10720/19430 (55%)] Loss: 0.000836
Train Epoch: 11 [10880/19430 (56%)] Loss: 0.000924
Train Epoch: 11 [11040/19430 (57%)] Loss: 0.000219
Train Epoch: 11 [11200/19430 (58%)] Loss: 0.000412
Train Epoch: 11 [11360/19430 (58%)] Loss: 0.000638
Train Epoch: 11 [11520/19430 (59%)] Loss: 0.020745
Train Epoch: 11 [11680/19430 (60%)] Loss: 0.000761
Train Epoch: 11 [11840/19430 (61%)] Loss: 0.000694
Train Epoch: 11 [12000/19430 (62%)] Loss: 0.000412
Train Epoch: 11 [12160/19430 (63%)] Loss: 0.002295
Train Epoch: 11 [12320/19430 (63%)] Loss: 0.007927
Train Epoch: 11 [12480/19430 (64%)] Loss: 0.000610
Train Epoch: 11 [12640/19430 (65%)] Loss: 0.000304
Train Epoch: 11 [12800/19430 (66%)] Loss: 0.000612
Train Epoch: 11 [12960/19430 (67%)] Loss: 0.003033
Train Epoch: 11 [13120/19430 (68%)] Loss: 0.004215
Train Epoch: 11 [13280/19430 (68%)] Loss: 0.000328
Train Epoch: 11 [13440/19430 (69%)] Loss: 0.001005
Train Epoch: 11 [13600/19430 (70%)] Loss: 0.000497
Train Epoch: 11 [13760/19430 (71%)] Loss: 0.000404
Train Epoch: 11 [13920/19430 (72%)] Loss: 0.017074
Train Epoch: 11 [14080/19430 (72%)] Loss: 0.000820
Train Epoch: 11 [14240/19430 (73%)] Loss: 0.000342
Train Epoch: 11 [14400/19430 (74%)] Loss: 0.000259
Train Epoch: 11 [14560/19430 (75%)] Loss: 0.000621
Train Epoch: 11 [14720/19430 (76%)] Loss: 0.000269
Train Epoch: 11 [14880/19430 (77%)] Loss: 0.000534
Train Epoch: 11 [15040/19430 (77%)] Loss: 0.003285
Train Epoch: 11 [15200/19430 (78%)] Loss: 0.000391
Train Epoch: 11 [15360/19430 (79%)] Loss: 0.002069
Train Epoch: 11 [15520/19430 (80%)] Loss: 0.000638
Train Epoch: 11 [15680/19430 (81%)] Loss: 0.002560
Train Epoch: 11 [15840/19430 (82%)] Loss: 0.001000
Train Epoch: 11 [16000/19430 (82%)] Loss: 0.000585
Train Epoch: 11 [16160/19430 (83%)] Loss: 0.000868
Train Epoch: 11 [16320/19430 (84%)] Loss: 0.000398
Train Epoch: 11 [16480/19430 (85%)] Loss: 0.000417
Train Epoch: 11 [16640/19430 (86%)] Loss: 0.000339
Train Epoch: 11 [16800/19430 (86%)] Loss: 0.000505
Train Epoch: 11 [16960/19430 (87%)] Loss: 0.002301
Train Epoch: 11 [17120/19430 (88%)] Loss: 0.000272
Train Epoch: 11 [17280/19430 (89%)] Loss: 0.000482
Train Epoch: 11 [17440/19430 (90%)] Loss: 0.000428
Train Epoch: 11 [17600/19430 (91%)] Loss: 0.001667
Train Epoch: 11 [17760/19430 (91%)] Loss: 0.000585
Train Epoch: 11 [17920/19430 (92%)] Loss: 0.000256
Train Epoch: 11 [18080/19430 (93%)] Loss: 0.000416
Train Epoch: 11 [18240/19430 (94%)] Loss: 0.000147
Train Epoch: 11 [18400/19430 (95%)] Loss: 0.001207
Train Epoch: 11 [18560/19430 (96%)] Loss: 0.000418
Train Epoch: 11 [18720/19430 (96%)] Loss: 0.000406
Train Epoch: 11 [18880/19430 (97%)] Loss: 0.000704
Train Epoch: 11 [19040/19430 (98%)] Loss: 0.000420
Train Epoch: 11 [19200/19430 (99%)] Loss: 0.000228
Train Epoch: 11 [19360/19430 (100%)] Loss: 0.001031
    epoch          : 11
    Train_loss     : 0.005417508134302663
    Train_accuracy : 0.9982010690789473
    Train_f1_score : 0.9982010722160339
    Val_loss       : 0.03174304448882193
    Val_accuracy   : 0.9926470588235294
    Val_f1_score   : 0.9926470518112183
Train Epoch: 12 [0/19430 (0%)] Loss: 0.000350
Train Epoch: 12 [160/19430 (1%)] Loss: 0.080658
Train Epoch: 12 [320/19430 (2%)] Loss: 0.001403
Train Epoch: 12 [480/19430 (2%)] Loss: 0.001417
Train Epoch: 12 [640/19430 (3%)] Loss: 0.001153
Train Epoch: 12 [800/19430 (4%)] Loss: 0.018413
Train Epoch: 12 [960/19430 (5%)] Loss: 0.018846
Train Epoch: 12 [1120/19430 (6%)] Loss: 0.003492
Train Epoch: 12 [1280/19430 (7%)] Loss: 0.000927
Train Epoch: 12 [1440/19430 (7%)] Loss: 0.060586
Train Epoch: 12 [1600/19430 (8%)] Loss: 0.002013
Train Epoch: 12 [1760/19430 (9%)] Loss: 0.000812
Train Epoch: 12 [1920/19430 (10%)] Loss: 0.003468
Train Epoch: 12 [2080/19430 (11%)] Loss: 0.001753
Train Epoch: 12 [2240/19430 (12%)] Loss: 0.002808
Train Epoch: 12 [2400/19430 (12%)] Loss: 0.171655
Train Epoch: 12 [2560/19430 (13%)] Loss: 0.006613
Train Epoch: 12 [2720/19430 (14%)] Loss: 0.004719
Train Epoch: 12 [2880/19430 (15%)] Loss: 0.025589
Train Epoch: 12 [3040/19430 (16%)] Loss: 0.001163
Train Epoch: 12 [3200/19430 (16%)] Loss: 0.001533
Train Epoch: 12 [3360/19430 (17%)] Loss: 0.002005
Train Epoch: 12 [3520/19430 (18%)] Loss: 0.001545
Train Epoch: 12 [3680/19430 (19%)] Loss: 0.003706
Train Epoch: 12 [3840/19430 (20%)] Loss: 0.002930
Train Epoch: 12 [4000/19430 (21%)] Loss: 0.002504
Train Epoch: 12 [4160/19430 (21%)] Loss: 0.004356
Train Epoch: 12 [4320/19430 (22%)] Loss: 0.004692
Train Epoch: 12 [4480/19430 (23%)] Loss: 0.001453
Train Epoch: 12 [4640/19430 (24%)] Loss: 0.000922
Train Epoch: 12 [4800/19430 (25%)] Loss: 0.000637
Train Epoch: 12 [4960/19430 (26%)] Loss: 0.004001
Train Epoch: 12 [5120/19430 (26%)] Loss: 0.002537
Train Epoch: 12 [5280/19430 (27%)] Loss: 0.008952
Train Epoch: 12 [5440/19430 (28%)] Loss: 0.000515
Train Epoch: 12 [5600/19430 (29%)] Loss: 0.001602
Train Epoch: 12 [5760/19430 (30%)] Loss: 0.001591
Train Epoch: 12 [5920/19430 (30%)] Loss: 0.001756
Train Epoch: 12 [6080/19430 (31%)] Loss: 0.003404
Train Epoch: 12 [6240/19430 (32%)] Loss: 0.002589
Train Epoch: 12 [6400/19430 (33%)] Loss: 0.001200
Train Epoch: 12 [6560/19430 (34%)] Loss: 0.000557
Train Epoch: 12 [6720/19430 (35%)] Loss: 0.013645
Train Epoch: 12 [6880/19430 (35%)] Loss: 0.001568
Train Epoch: 12 [7040/19430 (36%)] Loss: 0.001293
Train Epoch: 12 [7200/19430 (37%)] Loss: 0.000993
Train Epoch: 12 [7360/19430 (38%)] Loss: 0.008934
Train Epoch: 12 [7520/19430 (39%)] Loss: 0.003726
Train Epoch: 12 [7680/19430 (40%)] Loss: 0.003744
Train Epoch: 12 [7840/19430 (40%)] Loss: 0.004040
Train Epoch: 12 [8000/19430 (41%)] Loss: 0.001557
Train Epoch: 12 [8160/19430 (42%)] Loss: 0.000749
Train Epoch: 12 [8320/19430 (43%)] Loss: 0.001416
Train Epoch: 12 [8480/19430 (44%)] Loss: 0.004382
Train Epoch: 12 [8640/19430 (44%)] Loss: 0.000774
Train Epoch: 12 [8800/19430 (45%)] Loss: 0.007197
Train Epoch: 12 [8960/19430 (46%)] Loss: 0.000871
Train Epoch: 12 [9120/19430 (47%)] Loss: 0.000794
Train Epoch: 12 [9280/19430 (48%)] Loss: 0.008627
Train Epoch: 12 [9440/19430 (49%)] Loss: 0.001148
Train Epoch: 12 [9600/19430 (49%)] Loss: 0.000466
Train Epoch: 12 [9760/19430 (50%)] Loss: 0.004775
Train Epoch: 12 [9920/19430 (51%)] Loss: 0.001142
Train Epoch: 12 [10080/19430 (52%)] Loss: 0.004843
Train Epoch: 12 [10240/19430 (53%)] Loss: 0.004781
Train Epoch: 12 [10400/19430 (54%)] Loss: 0.000794
Train Epoch: 12 [10560/19430 (54%)] Loss: 0.000751
Train Epoch: 12 [10720/19430 (55%)] Loss: 0.003307
Train Epoch: 12 [10880/19430 (56%)] Loss: 0.001424
Train Epoch: 12 [11040/19430 (57%)] Loss: 0.000887
Train Epoch: 12 [11200/19430 (58%)] Loss: 0.036957
Train Epoch: 12 [11360/19430 (58%)] Loss: 0.001098
Train Epoch: 12 [11520/19430 (59%)] Loss: 0.001499
Train Epoch: 12 [11680/19430 (60%)] Loss: 0.001005
Train Epoch: 12 [11840/19430 (61%)] Loss: 0.001167
Train Epoch: 12 [12000/19430 (62%)] Loss: 0.001756
Train Epoch: 12 [12160/19430 (63%)] Loss: 0.001131
Train Epoch: 12 [12320/19430 (63%)] Loss: 0.000414
Train Epoch: 12 [12480/19430 (64%)] Loss: 0.000500
Train Epoch: 12 [12640/19430 (65%)] Loss: 0.002902
Train Epoch: 12 [12800/19430 (66%)] Loss: 0.001143
Train Epoch: 12 [12960/19430 (67%)] Loss: 0.002241
Train Epoch: 12 [13120/19430 (68%)] Loss: 0.001639
Train Epoch: 12 [13280/19430 (68%)] Loss: 0.000870
Train Epoch: 12 [13440/19430 (69%)] Loss: 0.000581
Train Epoch: 12 [13600/19430 (70%)] Loss: 0.001065
Train Epoch: 12 [13760/19430 (71%)] Loss: 0.002537
Train Epoch: 12 [13920/19430 (72%)] Loss: 0.001513
Train Epoch: 12 [14080/19430 (72%)] Loss: 0.176043
Train Epoch: 12 [14240/19430 (73%)] Loss: 0.002889
Train Epoch: 12 [14400/19430 (74%)] Loss: 0.000809
Train Epoch: 12 [14560/19430 (75%)] Loss: 0.001993
Train Epoch: 12 [14720/19430 (76%)] Loss: 0.003244
Train Epoch: 12 [14880/19430 (77%)] Loss: 0.000822
Train Epoch: 12 [15040/19430 (77%)] Loss: 0.001523
Train Epoch: 12 [15200/19430 (78%)] Loss: 0.001478
Train Epoch: 12 [15360/19430 (79%)] Loss: 0.001213
Train Epoch: 12 [15520/19430 (80%)] Loss: 0.006109
Train Epoch: 12 [15680/19430 (81%)] Loss: 0.000429
Train Epoch: 12 [15840/19430 (82%)] Loss: 0.001465
Train Epoch: 12 [16000/19430 (82%)] Loss: 0.001500
Train Epoch: 12 [16160/19430 (83%)] Loss: 0.050027
Train Epoch: 12 [16320/19430 (84%)] Loss: 0.000437
Train Epoch: 12 [16480/19430 (85%)] Loss: 0.001947
Train Epoch: 12 [16640/19430 (86%)] Loss: 0.001115
Train Epoch: 12 [16800/19430 (86%)] Loss: 0.009876
Train Epoch: 12 [16960/19430 (87%)] Loss: 0.003838
Train Epoch: 12 [17120/19430 (88%)] Loss: 0.002212
Train Epoch: 12 [17280/19430 (89%)] Loss: 0.000424
Train Epoch: 12 [17440/19430 (90%)] Loss: 0.001101
Train Epoch: 12 [17600/19430 (91%)] Loss: 0.004707
Train Epoch: 12 [17760/19430 (91%)] Loss: 0.001044
Train Epoch: 12 [17920/19430 (92%)] Loss: 0.000239
Train Epoch: 12 [18080/19430 (93%)] Loss: 0.001774
Train Epoch: 12 [18240/19430 (94%)] Loss: 0.000826
Train Epoch: 12 [18400/19430 (95%)] Loss: 0.006739
Train Epoch: 12 [18560/19430 (96%)] Loss: 0.001106
Train Epoch: 12 [18720/19430 (96%)] Loss: 0.001195
Train Epoch: 12 [18880/19430 (97%)] Loss: 0.001131
Train Epoch: 12 [19040/19430 (98%)] Loss: 0.001528
Train Epoch: 12 [19200/19430 (99%)] Loss: 0.000558
Train Epoch: 12 [19360/19430 (100%)] Loss: 0.001018
    epoch          : 12
    Train_loss     : 0.00829502374007892
    Train_accuracy : 0.9978926809210527
    Train_f1_score : 0.9978926777839661
    Val_loss       : 0.018023110157146756
    Val_accuracy   : 0.9967830882352942
    Val_f1_score   : 0.9967830777168274
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1030_145349/checkpoint-epoch12.pth ...
Train Epoch: 13 [0/19430 (0%)] Loss: 0.000880
Train Epoch: 13 [160/19430 (1%)] Loss: 0.005032
Train Epoch: 13 [320/19430 (2%)] Loss: 0.003424
Train Epoch: 13 [480/19430 (2%)] Loss: 0.000384
Train Epoch: 13 [640/19430 (3%)] Loss: 0.000283
Train Epoch: 13 [800/19430 (4%)] Loss: 0.001161
Train Epoch: 13 [960/19430 (5%)] Loss: 0.137095
Train Epoch: 13 [1120/19430 (6%)] Loss: 0.004594
Train Epoch: 13 [1280/19430 (7%)] Loss: 0.001722
Train Epoch: 13 [1440/19430 (7%)] Loss: 0.001873
Train Epoch: 13 [1600/19430 (8%)] Loss: 0.001480
Train Epoch: 13 [1760/19430 (9%)] Loss: 0.001172
Train Epoch: 13 [1920/19430 (10%)] Loss: 0.035483
Train Epoch: 13 [2080/19430 (11%)] Loss: 0.001403
Train Epoch: 13 [2240/19430 (12%)] Loss: 0.000963
Train Epoch: 13 [2400/19430 (12%)] Loss: 0.004143
Train Epoch: 13 [2560/19430 (13%)] Loss: 0.000481
Train Epoch: 13 [2720/19430 (14%)] Loss: 0.001273
Train Epoch: 13 [2880/19430 (15%)] Loss: 0.083791
Train Epoch: 13 [3040/19430 (16%)] Loss: 0.009017
Train Epoch: 13 [3200/19430 (16%)] Loss: 0.000219
Train Epoch: 13 [3360/19430 (17%)] Loss: 0.001139
Train Epoch: 13 [3520/19430 (18%)] Loss: 0.000846
Train Epoch: 13 [3680/19430 (19%)] Loss: 0.002858
Train Epoch: 13 [3840/19430 (20%)] Loss: 0.019730
Train Epoch: 13 [4000/19430 (21%)] Loss: 0.000677
Train Epoch: 13 [4160/19430 (21%)] Loss: 0.019724
Train Epoch: 13 [4320/19430 (22%)] Loss: 0.006491
Train Epoch: 13 [4480/19430 (23%)] Loss: 0.008718
Train Epoch: 13 [4640/19430 (24%)] Loss: 0.001941
Train Epoch: 13 [4800/19430 (25%)] Loss: 0.001971
Train Epoch: 13 [4960/19430 (26%)] Loss: 0.093641
Train Epoch: 13 [5120/19430 (26%)] Loss: 0.032147
Train Epoch: 13 [5280/19430 (27%)] Loss: 0.001628
Train Epoch: 13 [5440/19430 (28%)] Loss: 0.000519
Train Epoch: 13 [5600/19430 (29%)] Loss: 0.001664
Train Epoch: 13 [5760/19430 (30%)] Loss: 0.000740
Train Epoch: 13 [5920/19430 (30%)] Loss: 0.001329
Train Epoch: 13 [6080/19430 (31%)] Loss: 0.006914
Train Epoch: 13 [6240/19430 (32%)] Loss: 0.003867
Train Epoch: 13 [6400/19430 (33%)] Loss: 0.000416
Train Epoch: 13 [6560/19430 (34%)] Loss: 0.065561
Train Epoch: 13 [6720/19430 (35%)] Loss: 0.000330
Train Epoch: 13 [6880/19430 (35%)] Loss: 0.000514
Train Epoch: 13 [7040/19430 (36%)] Loss: 0.000481
Train Epoch: 13 [7200/19430 (37%)] Loss: 0.002234
Train Epoch: 13 [7360/19430 (38%)] Loss: 0.004111
Train Epoch: 13 [7520/19430 (39%)] Loss: 0.002138
Train Epoch: 13 [7680/19430 (40%)] Loss: 0.002034
Train Epoch: 13 [7840/19430 (40%)] Loss: 0.000698
Train Epoch: 13 [8000/19430 (41%)] Loss: 0.000346
Train Epoch: 13 [8160/19430 (42%)] Loss: 0.001533
Train Epoch: 13 [8320/19430 (43%)] Loss: 0.001726
Train Epoch: 13 [8480/19430 (44%)] Loss: 0.000709
Train Epoch: 13 [8640/19430 (44%)] Loss: 0.000555
Train Epoch: 13 [8800/19430 (45%)] Loss: 0.012848
Train Epoch: 13 [8960/19430 (46%)] Loss: 0.001237
Train Epoch: 13 [9120/19430 (47%)] Loss: 0.000256
Train Epoch: 13 [9280/19430 (48%)] Loss: 0.009031
Train Epoch: 13 [9440/19430 (49%)] Loss: 0.003382
Train Epoch: 13 [9600/19430 (49%)] Loss: 0.002147
Train Epoch: 13 [9760/19430 (50%)] Loss: 0.003158
Train Epoch: 13 [9920/19430 (51%)] Loss: 0.000888
Train Epoch: 13 [10080/19430 (52%)] Loss: 0.002094
Train Epoch: 13 [10240/19430 (53%)] Loss: 0.002502
Train Epoch: 13 [10400/19430 (54%)] Loss: 0.002566
Train Epoch: 13 [10560/19430 (54%)] Loss: 0.032426
Train Epoch: 13 [10720/19430 (55%)] Loss: 0.001191
Train Epoch: 13 [10880/19430 (56%)] Loss: 0.001010
Train Epoch: 13 [11040/19430 (57%)] Loss: 0.000344
Train Epoch: 13 [11200/19430 (58%)] Loss: 0.000368
Train Epoch: 13 [11360/19430 (58%)] Loss: 0.000755
Train Epoch: 13 [11520/19430 (59%)] Loss: 0.000307
Train Epoch: 13 [11680/19430 (60%)] Loss: 0.001301
Train Epoch: 13 [11840/19430 (61%)] Loss: 0.001514
Train Epoch: 13 [12000/19430 (62%)] Loss: 0.000327
Train Epoch: 13 [12160/19430 (63%)] Loss: 0.002436
Train Epoch: 13 [12320/19430 (63%)] Loss: 0.002214
Train Epoch: 13 [12480/19430 (64%)] Loss: 0.028346
Train Epoch: 13 [12640/19430 (65%)] Loss: 0.000582
Train Epoch: 13 [12800/19430 (66%)] Loss: 0.002557
Train Epoch: 13 [12960/19430 (67%)] Loss: 0.000616
Train Epoch: 13 [13120/19430 (68%)] Loss: 0.000832
Train Epoch: 13 [13280/19430 (68%)] Loss: 0.000623
Train Epoch: 13 [13440/19430 (69%)] Loss: 0.000470
Train Epoch: 13 [13600/19430 (70%)] Loss: 0.002174
Train Epoch: 13 [13760/19430 (71%)] Loss: 0.000523
Train Epoch: 13 [13920/19430 (72%)] Loss: 0.000512
Train Epoch: 13 [14080/19430 (72%)] Loss: 0.000371
Train Epoch: 13 [14240/19430 (73%)] Loss: 0.000795
Train Epoch: 13 [14400/19430 (74%)] Loss: 0.000391
Train Epoch: 13 [14560/19430 (75%)] Loss: 0.000518
Train Epoch: 13 [14720/19430 (76%)] Loss: 0.000719
Train Epoch: 13 [14880/19430 (77%)] Loss: 0.000375
Train Epoch: 13 [15040/19430 (77%)] Loss: 0.000323
Train Epoch: 13 [15200/19430 (78%)] Loss: 0.000582
Train Epoch: 13 [15360/19430 (79%)] Loss: 0.002010
Train Epoch: 13 [15520/19430 (80%)] Loss: 0.001632
Train Epoch: 13 [15680/19430 (81%)] Loss: 0.003230
Train Epoch: 13 [15840/19430 (82%)] Loss: 0.000768
Train Epoch: 13 [16000/19430 (82%)] Loss: 0.000649
Train Epoch: 13 [16160/19430 (83%)] Loss: 0.000288
Train Epoch: 13 [16320/19430 (84%)] Loss: 0.000977
Train Epoch: 13 [16480/19430 (85%)] Loss: 0.000299
Train Epoch: 13 [16640/19430 (86%)] Loss: 0.000345
Train Epoch: 13 [16800/19430 (86%)] Loss: 0.000812
Train Epoch: 13 [16960/19430 (87%)] Loss: 0.000259
Train Epoch: 13 [17120/19430 (88%)] Loss: 0.000624
Train Epoch: 13 [17280/19430 (89%)] Loss: 0.002568
Train Epoch: 13 [17440/19430 (90%)] Loss: 0.001312
Train Epoch: 13 [17600/19430 (91%)] Loss: 0.000538
Train Epoch: 13 [17760/19430 (91%)] Loss: 0.003023
Train Epoch: 13 [17920/19430 (92%)] Loss: 0.000399
Train Epoch: 13 [18080/19430 (93%)] Loss: 0.001962
Train Epoch: 13 [18240/19430 (94%)] Loss: 0.000117
Train Epoch: 13 [18400/19430 (95%)] Loss: 0.000443
Train Epoch: 13 [18560/19430 (96%)] Loss: 0.000265
Train Epoch: 13 [18720/19430 (96%)] Loss: 0.000520
Train Epoch: 13 [18880/19430 (97%)] Loss: 0.000331
Train Epoch: 13 [19040/19430 (98%)] Loss: 0.022601
Train Epoch: 13 [19200/19430 (99%)] Loss: 0.000584
Train Epoch: 13 [19360/19430 (100%)] Loss: 0.000224
    epoch          : 13
    Train_loss     : 0.006784945663448046
    Train_accuracy : 0.998303865131579
    Train_f1_score : 0.9983038902282715
    Val_loss       : 0.014389836980733569
    Val_accuracy   : 0.9967830882352942
    Val_f1_score   : 0.9967830777168274
Train Epoch: 14 [0/19430 (0%)] Loss: 0.001793
Train Epoch: 14 [160/19430 (1%)] Loss: 0.000598
Train Epoch: 14 [320/19430 (2%)] Loss: 0.000469
Train Epoch: 14 [480/19430 (2%)] Loss: 0.000385
Train Epoch: 14 [640/19430 (3%)] Loss: 0.001072
Train Epoch: 14 [800/19430 (4%)] Loss: 0.033445
Train Epoch: 14 [960/19430 (5%)] Loss: 0.000199
Train Epoch: 14 [1120/19430 (6%)] Loss: 0.000334
Train Epoch: 14 [1280/19430 (7%)] Loss: 0.001201
Train Epoch: 14 [1440/19430 (7%)] Loss: 0.000989
Train Epoch: 14 [1600/19430 (8%)] Loss: 0.001555
Train Epoch: 14 [1760/19430 (9%)] Loss: 0.000346
Train Epoch: 14 [1920/19430 (10%)] Loss: 0.000519
Train Epoch: 14 [2080/19430 (11%)] Loss: 0.000471
Train Epoch: 14 [2240/19430 (12%)] Loss: 0.000509
Train Epoch: 14 [2400/19430 (12%)] Loss: 0.000410
Train Epoch: 14 [2560/19430 (13%)] Loss: 0.010425
Train Epoch: 14 [2720/19430 (14%)] Loss: 0.000166
Train Epoch: 14 [2880/19430 (15%)] Loss: 0.000286
Train Epoch: 14 [3040/19430 (16%)] Loss: 0.000986
Train Epoch: 14 [3200/19430 (16%)] Loss: 0.000184
Train Epoch: 14 [3360/19430 (17%)] Loss: 0.000870
Train Epoch: 14 [3520/19430 (18%)] Loss: 0.003598
Train Epoch: 14 [3680/19430 (19%)] Loss: 0.000521
Train Epoch: 14 [3840/19430 (20%)] Loss: 0.000494
Train Epoch: 14 [4000/19430 (21%)] Loss: 0.001023
Train Epoch: 14 [4160/19430 (21%)] Loss: 0.000512
Train Epoch: 14 [4320/19430 (22%)] Loss: 0.000290
Train Epoch: 14 [4480/19430 (23%)] Loss: 0.001130
Train Epoch: 14 [4640/19430 (24%)] Loss: 0.001021
Train Epoch: 14 [4800/19430 (25%)] Loss: 0.024330
Train Epoch: 14 [4960/19430 (26%)] Loss: 0.000365
Train Epoch: 14 [5120/19430 (26%)] Loss: 0.001354
Train Epoch: 14 [5280/19430 (27%)] Loss: 0.000508
Train Epoch: 14 [5440/19430 (28%)] Loss: 0.000274
Train Epoch: 14 [5600/19430 (29%)] Loss: 0.000292
Train Epoch: 14 [5760/19430 (30%)] Loss: 0.000742
Train Epoch: 14 [5920/19430 (30%)] Loss: 0.000840
Train Epoch: 14 [6080/19430 (31%)] Loss: 0.000604
Train Epoch: 14 [6240/19430 (32%)] Loss: 0.000714
Train Epoch: 14 [6400/19430 (33%)] Loss: 0.000408
Train Epoch: 14 [6560/19430 (34%)] Loss: 0.024929
Train Epoch: 14 [6720/19430 (35%)] Loss: 0.000608
Train Epoch: 14 [6880/19430 (35%)] Loss: 0.002685
Train Epoch: 14 [7040/19430 (36%)] Loss: 0.001383
Train Epoch: 14 [7200/19430 (37%)] Loss: 0.001964
Train Epoch: 14 [7360/19430 (38%)] Loss: 0.007833
Train Epoch: 14 [7520/19430 (39%)] Loss: 0.007876
Train Epoch: 14 [7680/19430 (40%)] Loss: 0.001502
Train Epoch: 14 [7840/19430 (40%)] Loss: 0.001298
Train Epoch: 14 [8000/19430 (41%)] Loss: 0.019237
Train Epoch: 14 [8160/19430 (42%)] Loss: 0.002241
Train Epoch: 14 [8320/19430 (43%)] Loss: 0.000664
Train Epoch: 14 [8480/19430 (44%)] Loss: 0.000315
Train Epoch: 14 [8640/19430 (44%)] Loss: 0.004329
Train Epoch: 14 [8800/19430 (45%)] Loss: 0.000326
Train Epoch: 14 [8960/19430 (46%)] Loss: 0.003671
Train Epoch: 14 [9120/19430 (47%)] Loss: 0.001146
Train Epoch: 14 [9280/19430 (48%)] Loss: 0.000830
Train Epoch: 14 [9440/19430 (49%)] Loss: 0.000574
Train Epoch: 14 [9600/19430 (49%)] Loss: 0.004711
Train Epoch: 14 [9760/19430 (50%)] Loss: 0.002647
Train Epoch: 14 [9920/19430 (51%)] Loss: 0.000950
Train Epoch: 14 [10080/19430 (52%)] Loss: 0.004417
Train Epoch: 14 [10240/19430 (53%)] Loss: 0.009492
Train Epoch: 14 [10400/19430 (54%)] Loss: 0.001925
Train Epoch: 14 [10560/19430 (54%)] Loss: 0.000712
Train Epoch: 14 [10720/19430 (55%)] Loss: 0.002484
Train Epoch: 14 [10880/19430 (56%)] Loss: 0.002742
Train Epoch: 14 [11040/19430 (57%)] Loss: 0.005362
Train Epoch: 14 [11200/19430 (58%)] Loss: 0.005186
Train Epoch: 14 [11360/19430 (58%)] Loss: 0.000322
Train Epoch: 14 [11520/19430 (59%)] Loss: 0.000618
Train Epoch: 14 [11680/19430 (60%)] Loss: 0.001018
Train Epoch: 14 [11840/19430 (61%)] Loss: 0.001969
Train Epoch: 14 [12000/19430 (62%)] Loss: 0.025417
Train Epoch: 14 [12160/19430 (63%)] Loss: 0.000814
Train Epoch: 14 [12320/19430 (63%)] Loss: 0.001155
Train Epoch: 14 [12480/19430 (64%)] Loss: 0.001228
Train Epoch: 14 [12640/19430 (65%)] Loss: 0.000243
Train Epoch: 14 [12800/19430 (66%)] Loss: 0.046983
Train Epoch: 14 [12960/19430 (67%)] Loss: 0.000377
Train Epoch: 14 [13120/19430 (68%)] Loss: 0.000803
Train Epoch: 14 [13280/19430 (68%)] Loss: 0.001033
Train Epoch: 14 [13440/19430 (69%)] Loss: 0.000596
Train Epoch: 14 [13600/19430 (70%)] Loss: 0.006212
Train Epoch: 14 [13760/19430 (71%)] Loss: 0.000350
Train Epoch: 14 [13920/19430 (72%)] Loss: 0.002731
Train Epoch: 14 [14080/19430 (72%)] Loss: 0.000373
Train Epoch: 14 [14240/19430 (73%)] Loss: 0.000639
Train Epoch: 14 [14400/19430 (74%)] Loss: 0.000417
Train Epoch: 14 [14560/19430 (75%)] Loss: 0.000738
Train Epoch: 14 [14720/19430 (76%)] Loss: 0.000620
Train Epoch: 14 [14880/19430 (77%)] Loss: 0.000506
Train Epoch: 14 [15040/19430 (77%)] Loss: 0.001684
Train Epoch: 14 [15200/19430 (78%)] Loss: 0.000601
Train Epoch: 14 [15360/19430 (79%)] Loss: 0.021681
Train Epoch: 14 [15520/19430 (80%)] Loss: 0.000607
Train Epoch: 14 [15680/19430 (81%)] Loss: 0.001353
Train Epoch: 14 [15840/19430 (82%)] Loss: 0.000658
Train Epoch: 14 [16000/19430 (82%)] Loss: 0.000270
Train Epoch: 14 [16160/19430 (83%)] Loss: 0.000678
Train Epoch: 14 [16320/19430 (84%)] Loss: 0.000401
Train Epoch: 14 [16480/19430 (85%)] Loss: 0.011644
Train Epoch: 14 [16640/19430 (86%)] Loss: 0.000291
Train Epoch: 14 [16800/19430 (86%)] Loss: 0.000350
Train Epoch: 14 [16960/19430 (87%)] Loss: 0.000929
Train Epoch: 14 [17120/19430 (88%)] Loss: 0.000353
Train Epoch: 14 [17280/19430 (89%)] Loss: 0.000290
Train Epoch: 14 [17440/19430 (90%)] Loss: 0.000278
Train Epoch: 14 [17600/19430 (91%)] Loss: 0.003500
Train Epoch: 14 [17760/19430 (91%)] Loss: 0.000755
Train Epoch: 14 [17920/19430 (92%)] Loss: 0.032806
Train Epoch: 14 [18080/19430 (93%)] Loss: 0.001467
Train Epoch: 14 [18240/19430 (94%)] Loss: 0.000526
Train Epoch: 14 [18400/19430 (95%)] Loss: 0.001684
Train Epoch: 14 [18560/19430 (96%)] Loss: 0.000577
Train Epoch: 14 [18720/19430 (96%)] Loss: 0.002462
Train Epoch: 14 [18880/19430 (97%)] Loss: 0.000481
Train Epoch: 14 [19040/19430 (98%)] Loss: 0.001195
Train Epoch: 14 [19200/19430 (99%)] Loss: 0.001694
Train Epoch: 14 [19360/19430 (100%)] Loss: 0.000972
    epoch          : 14
    Train_loss     : 0.0049186542812309966
    Train_accuracy : 0.9987150493421053
    Train_f1_score : 0.9987150430679321
    Val_loss       : 0.02184819514717246
    Val_accuracy   : 0.9949448529411765
    Val_f1_score   : 0.9949448704719543
Train Epoch: 15 [0/19430 (0%)] Loss: 0.046870
Train Epoch: 15 [160/19430 (1%)] Loss: 0.003424
Train Epoch: 15 [320/19430 (2%)] Loss: 0.000947
Train Epoch: 15 [480/19430 (2%)] Loss: 0.000739
Train Epoch: 15 [640/19430 (3%)] Loss: 0.003632
Train Epoch: 15 [800/19430 (4%)] Loss: 0.001112
Train Epoch: 15 [960/19430 (5%)] Loss: 0.004393
Train Epoch: 15 [1120/19430 (6%)] Loss: 0.000980
Train Epoch: 15 [1280/19430 (7%)] Loss: 0.003886
Train Epoch: 15 [1440/19430 (7%)] Loss: 0.181155
Train Epoch: 15 [1600/19430 (8%)] Loss: 0.000489
Train Epoch: 15 [1760/19430 (9%)] Loss: 0.001044
Train Epoch: 15 [1920/19430 (10%)] Loss: 0.001200
Train Epoch: 15 [2080/19430 (11%)] Loss: 0.000918
Train Epoch: 15 [2240/19430 (12%)] Loss: 0.001692
Train Epoch: 15 [2400/19430 (12%)] Loss: 0.000433
Train Epoch: 15 [2560/19430 (13%)] Loss: 0.000441
Train Epoch: 15 [2720/19430 (14%)] Loss: 0.000653
Train Epoch: 15 [2880/19430 (15%)] Loss: 0.000297
Train Epoch: 15 [3040/19430 (16%)] Loss: 0.001509
Train Epoch: 15 [3200/19430 (16%)] Loss: 0.000593
Train Epoch: 15 [3360/19430 (17%)] Loss: 0.001050
Train Epoch: 15 [3520/19430 (18%)] Loss: 0.001560
Train Epoch: 15 [3680/19430 (19%)] Loss: 0.001628
Train Epoch: 15 [3840/19430 (20%)] Loss: 0.000830
Train Epoch: 15 [4000/19430 (21%)] Loss: 0.001008
Train Epoch: 15 [4160/19430 (21%)] Loss: 0.002268
Train Epoch: 15 [4320/19430 (22%)] Loss: 0.002305
Train Epoch: 15 [4480/19430 (23%)] Loss: 0.000747
Train Epoch: 15 [4640/19430 (24%)] Loss: 0.000454
Train Epoch: 15 [4800/19430 (25%)] Loss: 0.000645
Train Epoch: 15 [4960/19430 (26%)] Loss: 0.003601
Train Epoch: 15 [5120/19430 (26%)] Loss: 0.004424
Train Epoch: 15 [5280/19430 (27%)] Loss: 0.000377
Train Epoch: 15 [5440/19430 (28%)] Loss: 0.003636
Train Epoch: 15 [5600/19430 (29%)] Loss: 0.000700
Train Epoch: 15 [5760/19430 (30%)] Loss: 0.000393
Train Epoch: 15 [5920/19430 (30%)] Loss: 0.000610
Train Epoch: 15 [6080/19430 (31%)] Loss: 0.000522
Train Epoch: 15 [6240/19430 (32%)] Loss: 0.001069
Train Epoch: 15 [6400/19430 (33%)] Loss: 0.001286
Train Epoch: 15 [6560/19430 (34%)] Loss: 0.000586
Train Epoch: 15 [6720/19430 (35%)] Loss: 0.000998
Train Epoch: 15 [6880/19430 (35%)] Loss: 0.000436
Train Epoch: 15 [7040/19430 (36%)] Loss: 0.079721
Train Epoch: 15 [7200/19430 (37%)] Loss: 0.001425
Train Epoch: 15 [7360/19430 (38%)] Loss: 0.000977
Train Epoch: 15 [7520/19430 (39%)] Loss: 0.000345
Train Epoch: 15 [7680/19430 (40%)] Loss: 0.000340
Train Epoch: 15 [7840/19430 (40%)] Loss: 0.000528
Train Epoch: 15 [8000/19430 (41%)] Loss: 0.000387
Train Epoch: 15 [8160/19430 (42%)] Loss: 0.000546
Train Epoch: 15 [8320/19430 (43%)] Loss: 0.000585
Train Epoch: 15 [8480/19430 (44%)] Loss: 0.338422
Train Epoch: 15 [8640/19430 (44%)] Loss: 0.000580
Train Epoch: 15 [8800/19430 (45%)] Loss: 0.000408
Train Epoch: 15 [8960/19430 (46%)] Loss: 0.003655
Train Epoch: 15 [9120/19430 (47%)] Loss: 0.003645
Train Epoch: 15 [9280/19430 (48%)] Loss: 0.001773
Train Epoch: 15 [9440/19430 (49%)] Loss: 0.002437
Train Epoch: 15 [9600/19430 (49%)] Loss: 0.000859
Train Epoch: 15 [9760/19430 (50%)] Loss: 0.002881
Train Epoch: 15 [9920/19430 (51%)] Loss: 0.001518
Train Epoch: 15 [10080/19430 (52%)] Loss: 0.002091
Train Epoch: 15 [10240/19430 (53%)] Loss: 0.002182
Train Epoch: 15 [10400/19430 (54%)] Loss: 0.000245
Train Epoch: 15 [10560/19430 (54%)] Loss: 0.000358
Train Epoch: 15 [10720/19430 (55%)] Loss: 0.002736
Train Epoch: 15 [10880/19430 (56%)] Loss: 0.001017
Train Epoch: 15 [11040/19430 (57%)] Loss: 0.000824
Train Epoch: 15 [11200/19430 (58%)] Loss: 0.000950
Train Epoch: 15 [11360/19430 (58%)] Loss: 0.000407
Train Epoch: 15 [11520/19430 (59%)] Loss: 0.000996
Train Epoch: 15 [11680/19430 (60%)] Loss: 0.016007
Train Epoch: 15 [11840/19430 (61%)] Loss: 0.003095
Train Epoch: 15 [12000/19430 (62%)] Loss: 0.001194
Train Epoch: 15 [12160/19430 (63%)] Loss: 0.002334
Train Epoch: 15 [12320/19430 (63%)] Loss: 0.001485
Train Epoch: 15 [12480/19430 (64%)] Loss: 0.014007
Train Epoch: 15 [12640/19430 (65%)] Loss: 0.001780
Train Epoch: 15 [12800/19430 (66%)] Loss: 0.002645
Train Epoch: 15 [12960/19430 (67%)] Loss: 0.001107
Train Epoch: 15 [13120/19430 (68%)] Loss: 0.000977
Train Epoch: 15 [13280/19430 (68%)] Loss: 0.000424
Train Epoch: 15 [13440/19430 (69%)] Loss: 0.005826
Train Epoch: 15 [13600/19430 (70%)] Loss: 0.017823
Train Epoch: 15 [13760/19430 (71%)] Loss: 0.000830
Train Epoch: 15 [13920/19430 (72%)] Loss: 0.011580
Train Epoch: 15 [14080/19430 (72%)] Loss: 0.000331
Train Epoch: 15 [14240/19430 (73%)] Loss: 0.008871
Train Epoch: 15 [14400/19430 (74%)] Loss: 0.001093
Train Epoch: 15 [14560/19430 (75%)] Loss: 0.000639
Train Epoch: 15 [14720/19430 (76%)] Loss: 0.001252
Train Epoch: 15 [14880/19430 (77%)] Loss: 0.001540
Train Epoch: 15 [15040/19430 (77%)] Loss: 0.001802
Train Epoch: 15 [15200/19430 (78%)] Loss: 0.000447
Train Epoch: 15 [15360/19430 (79%)] Loss: 0.001640
Train Epoch: 15 [15520/19430 (80%)] Loss: 0.000408
Train Epoch: 15 [15680/19430 (81%)] Loss: 0.010229
Train Epoch: 15 [15840/19430 (82%)] Loss: 0.002112
Train Epoch: 15 [16000/19430 (82%)] Loss: 0.002506
Train Epoch: 15 [16160/19430 (83%)] Loss: 0.010280
Train Epoch: 15 [16320/19430 (84%)] Loss: 0.000864
Train Epoch: 15 [16480/19430 (85%)] Loss: 0.000782
Train Epoch: 15 [16640/19430 (86%)] Loss: 0.000207
Train Epoch: 15 [16800/19430 (86%)] Loss: 0.000401
Train Epoch: 15 [16960/19430 (87%)] Loss: 0.003319
Train Epoch: 15 [17120/19430 (88%)] Loss: 0.000485
Train Epoch: 15 [17280/19430 (89%)] Loss: 0.001662
Train Epoch: 15 [17440/19430 (90%)] Loss: 0.000679
Train Epoch: 15 [17600/19430 (91%)] Loss: 0.000871
Train Epoch: 15 [17760/19430 (91%)] Loss: 0.000524
Train Epoch: 15 [17920/19430 (92%)] Loss: 0.001019
Train Epoch: 15 [18080/19430 (93%)] Loss: 0.002115
Train Epoch: 15 [18240/19430 (94%)] Loss: 0.001416
Train Epoch: 15 [18400/19430 (95%)] Loss: 0.000476
Train Epoch: 15 [18560/19430 (96%)] Loss: 0.001368
Train Epoch: 15 [18720/19430 (96%)] Loss: 0.000696
Train Epoch: 15 [18880/19430 (97%)] Loss: 0.001481
Train Epoch: 15 [19040/19430 (98%)] Loss: 0.001030
Train Epoch: 15 [19200/19430 (99%)] Loss: 0.006014
Train Epoch: 15 [19360/19430 (100%)] Loss: 0.001441
    epoch          : 15
    Train_loss     : 0.007561678065653005
    Train_accuracy : 0.9979269462719299
    Train_f1_score : 0.9979268908500671
    Val_loss       : 0.016647851519177064
    Val_accuracy   : 0.9954044117647058
    Val_f1_score   : 0.9954044222831726
Saving checkpoint: saved/models/efficientnet-b7-pretrained/1030_145349/checkpoint-epoch15.pth ...
Train Epoch: 16 [0/19430 (0%)] Loss: 0.007027
Train Epoch: 16 [160/19430 (1%)] Loss: 0.001337
Train Epoch: 16 [320/19430 (2%)] Loss: 0.002966
Train Epoch: 16 [480/19430 (2%)] Loss: 0.009489
Train Epoch: 16 [640/19430 (3%)] Loss: 0.000775
Train Epoch: 16 [800/19430 (4%)] Loss: 0.002020
Train Epoch: 16 [960/19430 (5%)] Loss: 0.000858
Train Epoch: 16 [1120/19430 (6%)] Loss: 0.000590
Train Epoch: 16 [1280/19430 (7%)] Loss: 0.001946
Train Epoch: 16 [1440/19430 (7%)] Loss: 0.102913
Train Epoch: 16 [1600/19430 (8%)] Loss: 0.017621
Train Epoch: 16 [1760/19430 (9%)] Loss: 0.000236
Train Epoch: 16 [1920/19430 (10%)] Loss: 0.001483
Train Epoch: 16 [2080/19430 (11%)] Loss: 0.000818
Train Epoch: 16 [2240/19430 (12%)] Loss: 0.003843
Train Epoch: 16 [2400/19430 (12%)] Loss: 0.002830
Train Epoch: 16 [2560/19430 (13%)] Loss: 0.029600
Train Epoch: 16 [2720/19430 (14%)] Loss: 0.000469
Train Epoch: 16 [2880/19430 (15%)] Loss: 0.001200
Train Epoch: 16 [3040/19430 (16%)] Loss: 0.007617
Train Epoch: 16 [3200/19430 (16%)] Loss: 0.000426
Train Epoch: 16 [3360/19430 (17%)] Loss: 0.000443
Train Epoch: 16 [3520/19430 (18%)] Loss: 0.000457
Train Epoch: 16 [3680/19430 (19%)] Loss: 0.017987
Train Epoch: 16 [3840/19430 (20%)] Loss: 0.003656
Train Epoch: 16 [4000/19430 (21%)] Loss: 0.000571
Train Epoch: 16 [4160/19430 (21%)] Loss: 0.001152
Train Epoch: 16 [4320/19430 (22%)] Loss: 0.003354
Train Epoch: 16 [4480/19430 (23%)] Loss: 0.073437
Train Epoch: 16 [4640/19430 (24%)] Loss: 0.002462
Train Epoch: 16 [4800/19430 (25%)] Loss: 0.002657
Train Epoch: 16 [4960/19430 (26%)] Loss: 0.000391
Train Epoch: 16 [5120/19430 (26%)] Loss: 0.000500
Train Epoch: 16 [5280/19430 (27%)] Loss: 0.002752
Train Epoch: 16 [5440/19430 (28%)] Loss: 0.002042
Train Epoch: 16 [5600/19430 (29%)] Loss: 0.002374
Train Epoch: 16 [5760/19430 (30%)] Loss: 0.004496
Train Epoch: 16 [5920/19430 (30%)] Loss: 0.005194
Train Epoch: 16 [6080/19430 (31%)] Loss: 0.002458
Train Epoch: 16 [6240/19430 (32%)] Loss: 0.000698
Train Epoch: 16 [6400/19430 (33%)] Loss: 0.060071
Train Epoch: 16 [6560/19430 (34%)] Loss: 0.000219
Train Epoch: 16 [6720/19430 (35%)] Loss: 0.000498
Train Epoch: 16 [6880/19430 (35%)] Loss: 0.000491
Train Epoch: 16 [7040/19430 (36%)] Loss: 0.000310
Train Epoch: 16 [7200/19430 (37%)] Loss: 0.001546
Train Epoch: 16 [7360/19430 (38%)] Loss: 0.000163
Train Epoch: 16 [7520/19430 (39%)] Loss: 0.000694
Train Epoch: 16 [7680/19430 (40%)] Loss: 0.000753
Train Epoch: 16 [7840/19430 (40%)] Loss: 0.000256
Train Epoch: 16 [8000/19430 (41%)] Loss: 0.000592
Train Epoch: 16 [8160/19430 (42%)] Loss: 0.000462
Train Epoch: 16 [8320/19430 (43%)] Loss: 0.001939
Train Epoch: 16 [8480/19430 (44%)] Loss: 0.000378
Train Epoch: 16 [8640/19430 (44%)] Loss: 0.001182
Train Epoch: 16 [8800/19430 (45%)] Loss: 0.000583
Train Epoch: 16 [8960/19430 (46%)] Loss: 0.000383
Train Epoch: 16 [9120/19430 (47%)] Loss: 0.000470
Train Epoch: 16 [9280/19430 (48%)] Loss: 0.000149
Train Epoch: 16 [9440/19430 (49%)] Loss: 0.000387
Train Epoch: 16 [9600/19430 (49%)] Loss: 0.000914
Train Epoch: 16 [9760/19430 (50%)] Loss: 0.001568
Train Epoch: 16 [9920/19430 (51%)] Loss: 0.000902
Train Epoch: 16 [10080/19430 (52%)] Loss: 0.007445
Train Epoch: 16 [10240/19430 (53%)] Loss: 0.014058
Train Epoch: 16 [10400/19430 (54%)] Loss: 0.000309
Train Epoch: 16 [10560/19430 (54%)] Loss: 0.000182
Train Epoch: 16 [10720/19430 (55%)] Loss: 0.051967
Train Epoch: 16 [10880/19430 (56%)] Loss: 0.000385
Train Epoch: 16 [11040/19430 (57%)] Loss: 0.000282
Train Epoch: 16 [11200/19430 (58%)] Loss: 0.000746
Train Epoch: 16 [11360/19430 (58%)] Loss: 0.000898
Train Epoch: 16 [11520/19430 (59%)] Loss: 0.000170
Train Epoch: 16 [11680/19430 (60%)] Loss: 0.000332
Train Epoch: 16 [11840/19430 (61%)] Loss: 0.002049
Train Epoch: 16 [12000/19430 (62%)] Loss: 0.000434
Train Epoch: 16 [12160/19430 (63%)] Loss: 0.000500
Train Epoch: 16 [12320/19430 (63%)] Loss: 0.000226
Train Epoch: 16 [12480/19430 (64%)] Loss: 0.128899
Train Epoch: 16 [12640/19430 (65%)] Loss: 0.000305
Train Epoch: 16 [12800/19430 (66%)] Loss: 0.000519
Train Epoch: 16 [12960/19430 (67%)] Loss: 0.000694
Train Epoch: 16 [13120/19430 (68%)] Loss: 0.002644
Train Epoch: 16 [13280/19430 (68%)] Loss: 0.000593
Train Epoch: 16 [13440/19430 (69%)] Loss: 0.000239
Train Epoch: 16 [13600/19430 (70%)] Loss: 0.000908
Train Epoch: 16 [13760/19430 (71%)] Loss: 0.000433
Train Epoch: 16 [13920/19430 (72%)] Loss: 0.000207
Train Epoch: 16 [14080/19430 (72%)] Loss: 0.000681
Train Epoch: 16 [14240/19430 (73%)] Loss: 0.000636
Train Epoch: 16 [14400/19430 (74%)] Loss: 0.000223
Train Epoch: 16 [14560/19430 (75%)] Loss: 0.000912
Train Epoch: 16 [14720/19430 (76%)] Loss: 0.000584
Train Epoch: 16 [14880/19430 (77%)] Loss: 0.000251
Train Epoch: 16 [15040/19430 (77%)] Loss: 0.017397
Train Epoch: 16 [15200/19430 (78%)] Loss: 0.000767
Train Epoch: 16 [15360/19430 (79%)] Loss: 0.000622
Train Epoch: 16 [15520/19430 (80%)] Loss: 0.002203
Train Epoch: 16 [15680/19430 (81%)] Loss: 0.000453
Train Epoch: 16 [15840/19430 (82%)] Loss: 0.000213
Train Epoch: 16 [16000/19430 (82%)] Loss: 0.000341
Train Epoch: 16 [16160/19430 (83%)] Loss: 0.000938
Train Epoch: 16 [16320/19430 (84%)] Loss: 0.002084
Train Epoch: 16 [16480/19430 (85%)] Loss: 0.000174
Train Epoch: 16 [16640/19430 (86%)] Loss: 0.000320
Train Epoch: 16 [16800/19430 (86%)] Loss: 0.120485
Train Epoch: 16 [16960/19430 (87%)] Loss: 0.006378
Train Epoch: 16 [17120/19430 (88%)] Loss: 0.000193
Train Epoch: 16 [17280/19430 (89%)] Loss: 0.000605
Train Epoch: 16 [17440/19430 (90%)] Loss: 0.001248
Train Epoch: 16 [17600/19430 (91%)] Loss: 0.002414
Train Epoch: 16 [17760/19430 (91%)] Loss: 0.000386
Train Epoch: 16 [17920/19430 (92%)] Loss: 0.000336
Train Epoch: 16 [18080/19430 (93%)] Loss: 0.000455
Train Epoch: 16 [18240/19430 (94%)] Loss: 0.001312
Train Epoch: 16 [18400/19430 (95%)] Loss: 0.000514
Train Epoch: 16 [18560/19430 (96%)] Loss: 0.001324
Train Epoch: 16 [18720/19430 (96%)] Loss: 0.001728
Train Epoch: 16 [18880/19430 (97%)] Loss: 0.000753
Train Epoch: 16 [19040/19430 (98%)] Loss: 0.002448
Train Epoch: 16 [19200/19430 (99%)] Loss: 0.029068
Train Epoch: 16 [19360/19430 (100%)] Loss: 0.026612
    epoch          : 16
    Train_loss     : 0.004266177686005042
    Train_accuracy : 0.9990234375
    Train_f1_score : 0.9990234375
    Val_loss       : 0.012227751434586346
    Val_accuracy   : 0.9981617647058824
    Val_f1_score   : 0.998161792755127
Train Epoch: 17 [0/19430 (0%)] Loss: 0.002648
Train Epoch: 17 [160/19430 (1%)] Loss: 0.001000
Train Epoch: 17 [320/19430 (2%)] Loss: 0.000390
Train Epoch: 17 [480/19430 (2%)] Loss: 0.001254
Train Epoch: 17 [640/19430 (3%)] Loss: 0.001380
Train Epoch: 17 [800/19430 (4%)] Loss: 0.000540
Train Epoch: 17 [960/19430 (5%)] Loss: 0.001296
Train Epoch: 17 [1120/19430 (6%)] Loss: 0.001483
Train Epoch: 17 [1280/19430 (7%)] Loss: 0.001173
Train Epoch: 17 [1440/19430 (7%)] Loss: 0.000532
Train Epoch: 17 [1600/19430 (8%)] Loss: 0.000723
Train Epoch: 17 [1760/19430 (9%)] Loss: 0.000286
Train Epoch: 17 [1920/19430 (10%)] Loss: 0.000725
Train Epoch: 17 [2080/19430 (11%)] Loss: 0.000358
Train Epoch: 17 [2240/19430 (12%)] Loss: 0.001123
Train Epoch: 17 [2400/19430 (12%)] Loss: 0.001056
Train Epoch: 17 [2560/19430 (13%)] Loss: 0.000357
Train Epoch: 17 [2720/19430 (14%)] Loss: 0.002032
Train Epoch: 17 [2880/19430 (15%)] Loss: 0.000189
Train Epoch: 17 [3040/19430 (16%)] Loss: 0.000285
Train Epoch: 17 [3200/19430 (16%)] Loss: 0.003564
Train Epoch: 17 [3360/19430 (17%)] Loss: 0.000229
Train Epoch: 17 [3520/19430 (18%)] Loss: 0.002669
Train Epoch: 17 [3680/19430 (19%)] Loss: 0.000413
Train Epoch: 17 [3840/19430 (20%)] Loss: 0.000245
Train Epoch: 17 [4000/19430 (21%)] Loss: 0.034711
Train Epoch: 17 [4160/19430 (21%)] Loss: 0.000632
Train Epoch: 17 [4320/19430 (22%)] Loss: 0.004609
Train Epoch: 17 [4480/19430 (23%)] Loss: 0.000676
Train Epoch: 17 [4640/19430 (24%)] Loss: 0.000449
Train Epoch: 17 [4800/19430 (25%)] Loss: 0.000461
Train Epoch: 17 [4960/19430 (26%)] Loss: 0.000301
Train Epoch: 17 [5120/19430 (26%)] Loss: 0.000863
Train Epoch: 17 [5280/19430 (27%)] Loss: 0.001220
Train Epoch: 17 [5440/19430 (28%)] Loss: 0.003777
Train Epoch: 17 [5600/19430 (29%)] Loss: 0.000378
Train Epoch: 17 [5760/19430 (30%)] Loss: 0.000502
Train Epoch: 17 [5920/19430 (30%)] Loss: 0.000319
Train Epoch: 17 [6080/19430 (31%)] Loss: 0.000235
Train Epoch: 17 [6240/19430 (32%)] Loss: 0.000139
Train Epoch: 17 [6400/19430 (33%)] Loss: 0.000920
Train Epoch: 17 [6560/19430 (34%)] Loss: 0.001137
Train Epoch: 17 [6720/19430 (35%)] Loss: 0.000270
Train Epoch: 17 [6880/19430 (35%)] Loss: 0.001383
Train Epoch: 17 [7040/19430 (36%)] Loss: 0.016892
Train Epoch: 17 [7200/19430 (37%)] Loss: 0.000633
Train Epoch: 17 [7360/19430 (38%)] Loss: 0.000275
Train Epoch: 17 [7520/19430 (39%)] Loss: 0.000362
Train Epoch: 17 [7680/19430 (40%)] Loss: 0.000347
Train Epoch: 17 [7840/19430 (40%)] Loss: 0.000296
Train Epoch: 17 [8000/19430 (41%)] Loss: 0.001245
Train Epoch: 17 [8160/19430 (42%)] Loss: 0.000203
Train Epoch: 17 [8320/19430 (43%)] Loss: 0.003425
Train Epoch: 17 [8480/19430 (44%)] Loss: 0.000242
Train Epoch: 17 [8640/19430 (44%)] Loss: 0.000223
Train Epoch: 17 [8800/19430 (45%)] Loss: 0.000325
Train Epoch: 17 [8960/19430 (46%)] Loss: 0.000308
Train Epoch: 17 [9120/19430 (47%)] Loss: 0.000574
Train Epoch: 17 [9280/19430 (48%)] Loss: 0.008263
Train Epoch: 17 [9440/19430 (49%)] Loss: 0.000202
Train Epoch: 17 [9600/19430 (49%)] Loss: 0.000666
Train Epoch: 17 [9760/19430 (50%)] Loss: 0.000545
Train Epoch: 17 [9920/19430 (51%)] Loss: 0.000391
Train Epoch: 17 [10080/19430 (52%)] Loss: 0.001081
Train Epoch: 17 [10240/19430 (53%)] Loss: 0.008546
Train Epoch: 17 [10400/19430 (54%)] Loss: 0.000619
Train Epoch: 17 [10560/19430 (54%)] Loss: 0.000550
Train Epoch: 17 [10720/19430 (55%)] Loss: 0.000637
Train Epoch: 17 [10880/19430 (56%)] Loss: 0.000328
Train Epoch: 17 [11040/19430 (57%)] Loss: 0.003406
Train Epoch: 17 [11200/19430 (58%)] Loss: 0.000498
Train Epoch: 17 [11360/19430 (58%)] Loss: 0.000384
Train Epoch: 17 [11520/19430 (59%)] Loss: 0.000167
Train Epoch: 17 [11680/19430 (60%)] Loss: 0.000228
Train Epoch: 17 [11840/19430 (61%)] Loss: 0.001443
Train Epoch: 17 [12000/19430 (62%)] Loss: 0.000933
Train Epoch: 17 [12160/19430 (63%)] Loss: 0.001462
Train Epoch: 17 [12320/19430 (63%)] Loss: 0.000276
Train Epoch: 17 [12480/19430 (64%)] Loss: 0.003010
Train Epoch: 17 [12640/19430 (65%)] Loss: 0.000724
Train Epoch: 17 [12800/19430 (66%)] Loss: 0.001656
Train Epoch: 17 [12960/19430 (67%)] Loss: 0.005689
Train Epoch: 17 [13120/19430 (68%)] Loss: 0.000309
Train Epoch: 17 [13280/19430 (68%)] Loss: 0.004224
Train Epoch: 17 [13440/19430 (69%)] Loss: 0.010877
Train Epoch: 17 [13600/19430 (70%)] Loss: 0.000167
Train Epoch: 17 [13760/19430 (71%)] Loss: 0.000463
Train Epoch: 17 [13920/19430 (72%)] Loss: 0.000400
Train Epoch: 17 [14080/19430 (72%)] Loss: 0.000462
Train Epoch: 17 [14240/19430 (73%)] Loss: 0.000335
Train Epoch: 17 [14400/19430 (74%)] Loss: 0.000997
Train Epoch: 17 [14560/19430 (75%)] Loss: 0.000336
Train Epoch: 17 [14720/19430 (76%)] Loss: 0.000193
Train Epoch: 17 [14880/19430 (77%)] Loss: 0.000807
Train Epoch: 17 [15040/19430 (77%)] Loss: 0.000075
Train Epoch: 17 [15200/19430 (78%)] Loss: 0.001080
Train Epoch: 17 [15360/19430 (79%)] Loss: 0.000311
Train Epoch: 17 [15520/19430 (80%)] Loss: 0.000216
Train Epoch: 17 [15680/19430 (81%)] Loss: 0.000142
Train Epoch: 17 [15840/19430 (82%)] Loss: 0.000532
Train Epoch: 17 [16000/19430 (82%)] Loss: 0.005107
Train Epoch: 17 [16160/19430 (83%)] Loss: 0.000119
Train Epoch: 17 [16320/19430 (84%)] Loss: 0.000107
Train Epoch: 17 [16480/19430 (85%)] Loss: 0.000591
Train Epoch: 17 [16640/19430 (86%)] Loss: 0.001592
Train Epoch: 17 [16800/19430 (86%)] Loss: 0.006200
Train Epoch: 17 [16960/19430 (87%)] Loss: 0.000210
Train Epoch: 17 [17120/19430 (88%)] Loss: 0.000349
Train Epoch: 17 [17280/19430 (89%)] Loss: 0.000899
Train Epoch: 17 [17440/19430 (90%)] Loss: 0.006247
Train Epoch: 17 [17600/19430 (91%)] Loss: 0.001768
Train Epoch: 17 [17760/19430 (91%)] Loss: 0.000308
Train Epoch: 17 [17920/19430 (92%)] Loss: 0.000123
Train Epoch: 17 [18080/19430 (93%)] Loss: 0.000225
Train Epoch: 17 [18240/19430 (94%)] Loss: 0.001721
Train Epoch: 17 [18400/19430 (95%)] Loss: 0.002529
Train Epoch: 17 [18560/19430 (96%)] Loss: 0.000676
Train Epoch: 17 [18720/19430 (96%)] Loss: 0.000350
Train Epoch: 17 [18880/19430 (97%)] Loss: 0.000577
Train Epoch: 17 [19040/19430 (98%)] Loss: 0.000252
Train Epoch: 17 [19200/19430 (99%)] Loss: 0.000185
Train Epoch: 17 [19360/19430 (100%)] Loss: 0.000140
    epoch          : 17
    Train_loss     : 0.0014647341787574412
    Train_accuracy : 0.9997944078947368
    Train_f1_score : 0.9997944235801697
    Val_loss       : 0.013153864851119579
    Val_accuracy   : 0.9977022058823529
    Val_f1_score   : 0.9977021813392639
Train Epoch: 18 [0/19430 (0%)] Loss: 0.000138
Train Epoch: 18 [160/19430 (1%)] Loss: 0.000242
Train Epoch: 18 [320/19430 (2%)] Loss: 0.000120
Train Epoch: 18 [480/19430 (2%)] Loss: 0.000153
Train Epoch: 18 [640/19430 (3%)] Loss: 0.000414
Train Epoch: 18 [800/19430 (4%)] Loss: 0.000475
Train Epoch: 18 [960/19430 (5%)] Loss: 0.001876
Train Epoch: 18 [1120/19430 (6%)] Loss: 0.000166
Train Epoch: 18 [1280/19430 (7%)] Loss: 0.000425
Train Epoch: 18 [1440/19430 (7%)] Loss: 0.000350
Train Epoch: 18 [1600/19430 (8%)] Loss: 0.000354
Train Epoch: 18 [1760/19430 (9%)] Loss: 0.000621
Train Epoch: 18 [1920/19430 (10%)] Loss: 0.003782
Train Epoch: 18 [2080/19430 (11%)] Loss: 0.014437
Train Epoch: 18 [2240/19430 (12%)] Loss: 0.000600
Train Epoch: 18 [2400/19430 (12%)] Loss: 0.000160
Train Epoch: 18 [2560/19430 (13%)] Loss: 0.000208
Train Epoch: 18 [2720/19430 (14%)] Loss: 0.007446
Train Epoch: 18 [2880/19430 (15%)] Loss: 0.002251
Train Epoch: 18 [3040/19430 (16%)] Loss: 0.000320
Train Epoch: 18 [3200/19430 (16%)] Loss: 0.000286
Train Epoch: 18 [3360/19430 (17%)] Loss: 0.000271
Train Epoch: 18 [3520/19430 (18%)] Loss: 0.000374
Train Epoch: 18 [3680/19430 (19%)] Loss: 0.004330
Train Epoch: 18 [3840/19430 (20%)] Loss: 0.000441
Train Epoch: 18 [4000/19430 (21%)] Loss: 0.000155
Train Epoch: 18 [4160/19430 (21%)] Loss: 0.000901
Train Epoch: 18 [4320/19430 (22%)] Loss: 0.000459
Train Epoch: 18 [4480/19430 (23%)] Loss: 0.000680
Train Epoch: 18 [4640/19430 (24%)] Loss: 0.000277
Train Epoch: 18 [4800/19430 (25%)] Loss: 0.001258
Train Epoch: 18 [4960/19430 (26%)] Loss: 0.000568
Train Epoch: 18 [5120/19430 (26%)] Loss: 0.000393
Train Epoch: 18 [5280/19430 (27%)] Loss: 0.000543
Train Epoch: 18 [5440/19430 (28%)] Loss: 0.000516
Train Epoch: 18 [5600/19430 (29%)] Loss: 0.000570
Train Epoch: 18 [5760/19430 (30%)] Loss: 0.000207
Train Epoch: 18 [5920/19430 (30%)] Loss: 0.000116
Train Epoch: 18 [6080/19430 (31%)] Loss: 0.000163
Train Epoch: 18 [6240/19430 (32%)] Loss: 0.000945
Train Epoch: 18 [6400/19430 (33%)] Loss: 0.000143
Train Epoch: 18 [6560/19430 (34%)] Loss: 0.001005
Train Epoch: 18 [6720/19430 (35%)] Loss: 0.000147
Train Epoch: 18 [6880/19430 (35%)] Loss: 0.002857
Train Epoch: 18 [7040/19430 (36%)] Loss: 0.000905
Train Epoch: 18 [7200/19430 (37%)] Loss: 0.000215
Train Epoch: 18 [7360/19430 (38%)] Loss: 0.000214
Train Epoch: 18 [7520/19430 (39%)] Loss: 0.000903
Train Epoch: 18 [7680/19430 (40%)] Loss: 0.001084
Train Epoch: 18 [7840/19430 (40%)] Loss: 0.000741
Train Epoch: 18 [8000/19430 (41%)] Loss: 0.000294
Train Epoch: 18 [8160/19430 (42%)] Loss: 0.000162
Train Epoch: 18 [8320/19430 (43%)] Loss: 0.000331
Train Epoch: 18 [8480/19430 (44%)] Loss: 0.000138
Train Epoch: 18 [8640/19430 (44%)] Loss: 0.000884
Train Epoch: 18 [8800/19430 (45%)] Loss: 0.025683
Train Epoch: 18 [8960/19430 (46%)] Loss: 0.000336
Train Epoch: 18 [9120/19430 (47%)] Loss: 0.000181
Train Epoch: 18 [9280/19430 (48%)] Loss: 0.001311
Train Epoch: 18 [9440/19430 (49%)] Loss: 0.000339
Train Epoch: 18 [9600/19430 (49%)] Loss: 0.000270
Train Epoch: 18 [9760/19430 (50%)] Loss: 0.000322
Train Epoch: 18 [9920/19430 (51%)] Loss: 0.000487
Train Epoch: 18 [10080/19430 (52%)] Loss: 0.000239
Train Epoch: 18 [10240/19430 (53%)] Loss: 0.000212
Train Epoch: 18 [10400/19430 (54%)] Loss: 0.000874
Train Epoch: 18 [10560/19430 (54%)] Loss: 0.001844
Train Epoch: 18 [10720/19430 (55%)] Loss: 0.001044
Train Epoch: 18 [10880/19430 (56%)] Loss: 0.285167
Train Epoch: 18 [11040/19430 (57%)] Loss: 0.000202
Train Epoch: 18 [11200/19430 (58%)] Loss: 0.000609
Train Epoch: 18 [11360/19430 (58%)] Loss: 0.000352
Train Epoch: 18 [11520/19430 (59%)] Loss: 0.000122
Train Epoch: 18 [11680/19430 (60%)] Loss: 0.000352
Train Epoch: 18 [11840/19430 (61%)] Loss: 0.000823
Train Epoch: 18 [12000/19430 (62%)] Loss: 0.003233
Train Epoch: 18 [12160/19430 (63%)] Loss: 0.000281
Train Epoch: 18 [12320/19430 (63%)] Loss: 0.000314
Train Epoch: 18 [12480/19430 (64%)] Loss: 0.000256
Train Epoch: 18 [12640/19430 (65%)] Loss: 0.000871
Train Epoch: 18 [12800/19430 (66%)] Loss: 0.000926
Train Epoch: 18 [12960/19430 (67%)] Loss: 0.000323
Train Epoch: 18 [13120/19430 (68%)] Loss: 0.000379
Train Epoch: 18 [13280/19430 (68%)] Loss: 0.003634
Train Epoch: 18 [13440/19430 (69%)] Loss: 0.002642
Train Epoch: 18 [13600/19430 (70%)] Loss: 0.001205
Train Epoch: 18 [13760/19430 (71%)] Loss: 0.000291
Train Epoch: 18 [13920/19430 (72%)] Loss: 0.000275
Train Epoch: 18 [14080/19430 (72%)] Loss: 0.000153
Train Epoch: 18 [14240/19430 (73%)] Loss: 0.000421
Train Epoch: 18 [14400/19430 (74%)] Loss: 0.000867
Train Epoch: 18 [14560/19430 (75%)] Loss: 0.001653
Train Epoch: 18 [14720/19430 (76%)] Loss: 0.000892
Train Epoch: 18 [14880/19430 (77%)] Loss: 0.000600
Train Epoch: 18 [15040/19430 (77%)] Loss: 0.001166
Train Epoch: 18 [15200/19430 (78%)] Loss: 0.000264
Train Epoch: 18 [15360/19430 (79%)] Loss: 0.000291
Train Epoch: 18 [15520/19430 (80%)] Loss: 0.000214
Train Epoch: 18 [15680/19430 (81%)] Loss: 0.000482
Train Epoch: 18 [15840/19430 (82%)] Loss: 0.000460
Train Epoch: 18 [16000/19430 (82%)] Loss: 0.000443
Train Epoch: 18 [16160/19430 (83%)] Loss: 0.000182
Train Epoch: 18 [16320/19430 (84%)] Loss: 0.000192
Train Epoch: 18 [16480/19430 (85%)] Loss: 0.000353
Train Epoch: 18 [16640/19430 (86%)] Loss: 0.000181
Train Epoch: 18 [16800/19430 (86%)] Loss: 0.000271
Train Epoch: 18 [16960/19430 (87%)] Loss: 0.000185
Train Epoch: 18 [17120/19430 (88%)] Loss: 0.000451
Train Epoch: 18 [17280/19430 (89%)] Loss: 0.000254
Train Epoch: 18 [17440/19430 (90%)] Loss: 0.000304
Train Epoch: 18 [17600/19430 (91%)] Loss: 0.000357
Train Epoch: 18 [17760/19430 (91%)] Loss: 0.001880
Train Epoch: 18 [17920/19430 (92%)] Loss: 0.000638
Train Epoch: 18 [18080/19430 (93%)] Loss: 0.002343
Train Epoch: 18 [18240/19430 (94%)] Loss: 0.000719
Train Epoch: 18 [18400/19430 (95%)] Loss: 0.000373
Train Epoch: 18 [18560/19430 (96%)] Loss: 0.001256
Train Epoch: 18 [18720/19430 (96%)] Loss: 0.005180
Train Epoch: 18 [18880/19430 (97%)] Loss: 0.000777
Train Epoch: 18 [19040/19430 (98%)] Loss: 0.000325
Train Epoch: 18 [19200/19430 (99%)] Loss: 0.000413
Train Epoch: 18 [19360/19430 (100%)] Loss: 0.000199
    epoch          : 18
    Train_loss     : 0.00345540239602901
    Train_accuracy : 0.9991776315789473
    Train_f1_score : 0.9991776347160339
    Val_loss       : 0.015692572110792218
    Val_accuracy   : 0.9972426470588235
    Val_f1_score   : 0.9972426295280457
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 372, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 493, in _save
    zip_file.write_record(name, buf_value, len(buf_value))
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 92, in <module>
    main(config)
  File "train.py", line 66, in main
    trainer.train()
  File "/opt/ml/project-T4193/base/base_trainer.py", line 99, in train
    self._save_checkpoint(epoch, save_best=best)
  File "/opt/ml/project-T4193/base/base_trainer.py", line 119, in _save_checkpoint
    torch.save(state, filename)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 373, in save
    return
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 259, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:274] . unexpected pos 806088064 vs 806087952
/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
--- Logging error ---
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/logging/__init__.py", line 1085, in emit
    self.flush()
  File "/opt/conda/lib/python3.8/logging/__init__.py", line 1065, in flush
    self.stream.flush()
OSError: [Errno 28] No space left on device
Call stack:
  File "/opt/conda/lib/python3.8/threading.py", line 890, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/opt/conda/lib/python3.8/site-packages/wandb/filesync/upload_job.py", line 67, in run
    success = self.push()
  File "/opt/conda/lib/python3.8/site-packages/wandb/filesync/upload_job.py", line 143, in push
    logger.info("Uploaded file %s", self.save_path)
Message: 'Uploaded file %s'
Arguments: ('/opt/ml/project-T4193/wandb/run-20221030_145345-chg2ogai/files/wandb-summary.json',)
--- Logging error ---
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/logging/__init__.py", line 1085, in emit
    self.flush()
  File "/opt/conda/lib/python3.8/logging/__init__.py", line 1065, in flush
    self.stream.flush()
OSError: [Errno 28] No space left on device
Call stack:
  File "/opt/conda/lib/python3.8/threading.py", line 890, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    se