/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Currently logged in as: qwer55252. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /opt/ml/project-T4193/ENet_Implement/wandb/run-20221025_183852-qolcfbf8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-bird-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1
wandb: üöÄ View run at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1/runs/qolcfbf8
EfficientNet(
  (_conv_stem): Conv2dStaticSamePadding(
    3, 64, kernel_size=(3, 3), stride=(2, 2), bias=False
    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
  )
  (_bn0): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_blocks): ModuleList(
    (0): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        64, 64, kernel_size=(3, 3), stride=[1, 1], groups=64, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        64, 16, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        16, 64, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (3): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        32, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 32, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (4): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        192, 192, kernel_size=(3, 3), stride=[2, 2], groups=192, bias=False
        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        192, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 192, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (5): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (6): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (7): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (8): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (9): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (10): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (11): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        288, 288, kernel_size=(5, 5), stride=[2, 2], groups=288, bias=False
        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        288, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 288, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        288, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (12): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (13): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (14): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (15): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (16): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (17): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (18): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        480, 480, kernel_size=(3, 3), stride=[2, 2], groups=480, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        480, 20, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        20, 480, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (19): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (20): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (21): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (22): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (23): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (24): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (25): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (26): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (27): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (28): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=[1, 1], groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (29): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (30): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (31): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (32): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (33): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (34): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (35): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (36): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (37): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (38): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1344, 1344, kernel_size=(5, 5), stride=[2, 2], groups=1344, bias=False
        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1344, 56, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        56, 1344, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1344, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (39): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (40): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (41): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (42): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (43): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (44): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (45): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (46): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (47): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (48): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (49): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (50): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (51): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2304, 2304, kernel_size=(3, 3), stride=[1, 1], groups=2304, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2304, 96, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        96, 2304, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2304, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (52): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (53): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (54): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        3840, 160, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        160, 3840, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (_conv_head): Conv2dStaticSamePadding(
    640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False
    (static_padding): Identity()
  )
  (_bn1): BatchNorm2d(2560, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)
  (_dropout): Dropout(p=0.5, inplace=False)
  (_fc): Linear(in_features=2560, out_features=18, bias=True)
  (_swish): MemoryEfficientSwish()
)
Train Epoch: 1 [0/17010 (0%)] Loss: 2.952758
Train Epoch: 1 [160/17010 (1%)] Loss: 3.042631
Train Epoch: 1 [320/17010 (2%)] Loss: 2.243673
Train Epoch: 1 [480/17010 (3%)] Loss: 2.612191
Train Epoch: 1 [640/17010 (4%)] Loss: 2.378964
Train Epoch: 1 [800/17010 (5%)] Loss: 2.198900
Train Epoch: 1 [960/17010 (6%)] Loss: 2.646021
Train Epoch: 1 [1120/17010 (7%)] Loss: 2.240860
Train Epoch: 1 [1280/17010 (8%)] Loss: 2.511067
Train Epoch: 1 [1440/17010 (8%)] Loss: 2.243545
Train Epoch: 1 [1600/17010 (9%)] Loss: 2.399929
Train Epoch: 1 [1760/17010 (10%)] Loss: 2.464893
Train Epoch: 1 [1920/17010 (11%)] Loss: 2.253065
Train Epoch: 1 [2080/17010 (12%)] Loss: 2.390954
Train Epoch: 1 [2240/17010 (13%)] Loss: 2.490886
Train Epoch: 1 [2400/17010 (14%)] Loss: 2.464972
Train Epoch: 1 [2560/17010 (15%)] Loss: 2.441034
Train Epoch: 1 [2720/17010 (16%)] Loss: 2.475982
Train Epoch: 1 [2880/17010 (17%)] Loss: 2.216940
Train Epoch: 1 [3040/17010 (18%)] Loss: 2.391717
Train Epoch: 1 [3200/17010 (19%)] Loss: 2.287322
Train Epoch: 1 [3360/17010 (20%)] Loss: 2.257107
Train Epoch: 1 [3520/17010 (21%)] Loss: 2.128194
Train Epoch: 1 [3680/17010 (22%)] Loss: 2.291524
Train Epoch: 1 [3840/17010 (23%)] Loss: 2.476631
Train Epoch: 1 [4000/17010 (24%)] Loss: 2.260664
Train Epoch: 1 [4160/17010 (24%)] Loss: 2.282149
Train Epoch: 1 [4320/17010 (25%)] Loss: 2.482187
Train Epoch: 1 [4480/17010 (26%)] Loss: 2.162066
Train Epoch: 1 [4640/17010 (27%)] Loss: 2.506437
Train Epoch: 1 [4800/17010 (28%)] Loss: 2.424952
Train Epoch: 1 [4960/17010 (29%)] Loss: 2.346063
Train Epoch: 1 [5120/17010 (30%)] Loss: 2.451794
Train Epoch: 1 [5280/17010 (31%)] Loss: 2.274753
Train Epoch: 1 [5440/17010 (32%)] Loss: 2.641923
Train Epoch: 1 [5600/17010 (33%)] Loss: 2.319660
Train Epoch: 1 [5760/17010 (34%)] Loss: 2.454661
Train Epoch: 1 [5920/17010 (35%)] Loss: 2.347742
Train Epoch: 1 [6080/17010 (36%)] Loss: 2.208021
Train Epoch: 1 [6240/17010 (37%)] Loss: 2.050625
Train Epoch: 1 [6400/17010 (38%)] Loss: 2.402012
Train Epoch: 1 [6560/17010 (39%)] Loss: 2.332181
Train Epoch: 1 [6720/17010 (40%)] Loss: 2.413446
Train Epoch: 1 [6880/17010 (40%)] Loss: 2.210293
Train Epoch: 1 [7040/17010 (41%)] Loss: 1.976052
Train Epoch: 1 [7200/17010 (42%)] Loss: 1.687970
Train Epoch: 1 [7360/17010 (43%)] Loss: 2.361535
Train Epoch: 1 [7520/17010 (44%)] Loss: 1.967271
Train Epoch: 1 [7680/17010 (45%)] Loss: 2.192576
Train Epoch: 1 [7840/17010 (46%)] Loss: 2.452658
Train Epoch: 1 [8000/17010 (47%)] Loss: 2.403881
Train Epoch: 1 [8160/17010 (48%)] Loss: 2.069635
Train Epoch: 1 [8320/17010 (49%)] Loss: 1.892156
Train Epoch: 1 [8480/17010 (50%)] Loss: 2.175822
Train Epoch: 1 [8640/17010 (51%)] Loss: 2.294261
Train Epoch: 1 [8800/17010 (52%)] Loss: 2.140938
Train Epoch: 1 [8960/17010 (53%)] Loss: 1.884090
Train Epoch: 1 [9120/17010 (54%)] Loss: 2.013771
Train Epoch: 1 [9280/17010 (55%)] Loss: 2.118639
Train Epoch: 1 [9440/17010 (55%)] Loss: 2.065766
Train Epoch: 1 [9600/17010 (56%)] Loss: 1.836244
Train Epoch: 1 [9760/17010 (57%)] Loss: 1.916356
Train Epoch: 1 [9920/17010 (58%)] Loss: 2.124790
Train Epoch: 1 [10080/17010 (59%)] Loss: 2.096888
Train Epoch: 1 [10240/17010 (60%)] Loss: 2.021324
Train Epoch: 1 [10400/17010 (61%)] Loss: 1.729399
Train Epoch: 1 [10560/17010 (62%)] Loss: 1.880873
Train Epoch: 1 [10720/17010 (63%)] Loss: 1.800778
Train Epoch: 1 [10880/17010 (64%)] Loss: 1.714880
Train Epoch: 1 [11040/17010 (65%)] Loss: 1.684900
Train Epoch: 1 [11200/17010 (66%)] Loss: 1.722810
Train Epoch: 1 [11360/17010 (67%)] Loss: 1.563036
Train Epoch: 1 [11520/17010 (68%)] Loss: 1.728409
Train Epoch: 1 [11680/17010 (69%)] Loss: 1.771573
Train Epoch: 1 [11840/17010 (70%)] Loss: 1.535176
Train Epoch: 1 [12000/17010 (71%)] Loss: 1.238003
Train Epoch: 1 [12160/17010 (71%)] Loss: 1.346466
Train Epoch: 1 [12320/17010 (72%)] Loss: 1.912218
Train Epoch: 1 [12480/17010 (73%)] Loss: 1.580434
Train Epoch: 1 [12640/17010 (74%)] Loss: 1.766758
Train Epoch: 1 [12800/17010 (75%)] Loss: 1.264397
Train Epoch: 1 [12960/17010 (76%)] Loss: 1.755959
Train Epoch: 1 [13120/17010 (77%)] Loss: 1.500136
Train Epoch: 1 [13280/17010 (78%)] Loss: 1.317717
Train Epoch: 1 [13440/17010 (79%)] Loss: 1.412699
Train Epoch: 1 [13600/17010 (80%)] Loss: 1.338849
Train Epoch: 1 [13760/17010 (81%)] Loss: 1.300648
Train Epoch: 1 [13920/17010 (82%)] Loss: 1.082446
Train Epoch: 1 [14080/17010 (83%)] Loss: 1.195785
Train Epoch: 1 [14240/17010 (84%)] Loss: 1.353244
Train Epoch: 1 [14400/17010 (85%)] Loss: 1.738693
Train Epoch: 1 [14560/17010 (86%)] Loss: 1.095537
Train Epoch: 1 [14720/17010 (87%)] Loss: 1.386983
Train Epoch: 1 [14880/17010 (87%)] Loss: 2.027573
Train Epoch: 1 [15040/17010 (88%)] Loss: 1.107572
Train Epoch: 1 [15200/17010 (89%)] Loss: 1.618927
Train Epoch: 1 [15360/17010 (90%)] Loss: 0.965476
Train Epoch: 1 [15520/17010 (91%)] Loss: 1.245607
Train Epoch: 1 [15680/17010 (92%)] Loss: 1.260880
Train Epoch: 1 [15840/17010 (93%)] Loss: 1.675412
Train Epoch: 1 [16000/17010 (94%)] Loss: 1.163876
Train Epoch: 1 [16160/17010 (95%)] Loss: 1.229935
Train Epoch: 1 [16320/17010 (96%)] Loss: 1.558236
Train Epoch: 1 [16480/17010 (97%)] Loss: 1.561666
Train Epoch: 1 [16640/17010 (98%)] Loss: 1.045275
Train Epoch: 1 [16800/17010 (99%)] Loss: 1.296355
Train Epoch: 1 [16960/17010 (100%)] Loss: 1.288423
    epoch          : 1
    Train_loss     : 1.9757024149473448
    Train_accuracy : 0.3660975355054302
    Train_top_k_acc: 0.6690685045948204
    Val_loss       : 2.4538881381352744
    Val_accuracy   : 0.39375
    Val_top_k_acc  : 0.6838541666666667
Warning: Metric 'val_loss' is not found. Model performance monitoring is disabled.
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch1.pth ...
Train Epoch: 2 [0/17010 (0%)] Loss: 1.071406
Train Epoch: 2 [160/17010 (1%)] Loss: 1.225186
Train Epoch: 2 [320/17010 (2%)] Loss: 1.253766
Train Epoch: 2 [480/17010 (3%)] Loss: 1.019329
Train Epoch: 2 [640/17010 (4%)] Loss: 0.886017
Train Epoch: 2 [800/17010 (5%)] Loss: 1.466682
Train Epoch: 2 [960/17010 (6%)] Loss: 1.153569
Train Epoch: 2 [1120/17010 (7%)] Loss: 1.396934
Train Epoch: 2 [1280/17010 (8%)] Loss: 1.410774
Train Epoch: 2 [1440/17010 (8%)] Loss: 1.095007
Train Epoch: 2 [1600/17010 (9%)] Loss: 1.459403
Train Epoch: 2 [1760/17010 (10%)] Loss: 1.122178
Train Epoch: 2 [1920/17010 (11%)] Loss: 0.943001
Train Epoch: 2 [2080/17010 (12%)] Loss: 1.470321
Train Epoch: 2 [2240/17010 (13%)] Loss: 1.452748
Train Epoch: 2 [2400/17010 (14%)] Loss: 0.802936
Train Epoch: 2 [2560/17010 (15%)] Loss: 1.157056
Train Epoch: 2 [2720/17010 (16%)] Loss: 1.269721
Train Epoch: 2 [2880/17010 (17%)] Loss: 0.885291
Train Epoch: 2 [3040/17010 (18%)] Loss: 0.752686
Train Epoch: 2 [3200/17010 (19%)] Loss: 1.520054
Train Epoch: 2 [3360/17010 (20%)] Loss: 1.512749
Train Epoch: 2 [3520/17010 (21%)] Loss: 1.157328
Train Epoch: 2 [3680/17010 (22%)] Loss: 0.950647
Train Epoch: 2 [3840/17010 (23%)] Loss: 1.255695
Train Epoch: 2 [4000/17010 (24%)] Loss: 0.683675
Train Epoch: 2 [4160/17010 (24%)] Loss: 0.874456
Train Epoch: 2 [4320/17010 (25%)] Loss: 1.370117
Train Epoch: 2 [4480/17010 (26%)] Loss: 0.993091
Train Epoch: 2 [4640/17010 (27%)] Loss: 0.707765
Train Epoch: 2 [4800/17010 (28%)] Loss: 1.044678
Train Epoch: 2 [4960/17010 (29%)] Loss: 0.770299
Train Epoch: 2 [5120/17010 (30%)] Loss: 0.934537
Train Epoch: 2 [5280/17010 (31%)] Loss: 1.114431
Train Epoch: 2 [5440/17010 (32%)] Loss: 0.972335
Train Epoch: 2 [5600/17010 (33%)] Loss: 1.000540
Train Epoch: 2 [5760/17010 (34%)] Loss: 1.044679
Train Epoch: 2 [5920/17010 (35%)] Loss: 1.032858
Train Epoch: 2 [6080/17010 (36%)] Loss: 1.097267
Train Epoch: 2 [6240/17010 (37%)] Loss: 0.616272
Train Epoch: 2 [6400/17010 (38%)] Loss: 1.483111
Train Epoch: 2 [6560/17010 (39%)] Loss: 1.009058
Train Epoch: 2 [6720/17010 (40%)] Loss: 1.016665
Train Epoch: 2 [6880/17010 (40%)] Loss: 0.961152
Train Epoch: 2 [7040/17010 (41%)] Loss: 1.132954
Train Epoch: 2 [7200/17010 (42%)] Loss: 0.621453
Train Epoch: 2 [7360/17010 (43%)] Loss: 0.963158
Train Epoch: 2 [7520/17010 (44%)] Loss: 0.823508
Train Epoch: 2 [7680/17010 (45%)] Loss: 1.039809
Train Epoch: 2 [7840/17010 (46%)] Loss: 0.651620
Train Epoch: 2 [8000/17010 (47%)] Loss: 1.012743
Train Epoch: 2 [8160/17010 (48%)] Loss: 0.889128
Train Epoch: 2 [8320/17010 (49%)] Loss: 1.158254
Train Epoch: 2 [8480/17010 (50%)] Loss: 1.248875
Train Epoch: 2 [8640/17010 (51%)] Loss: 0.741539
Train Epoch: 2 [8800/17010 (52%)] Loss: 0.781566
Train Epoch: 2 [8960/17010 (53%)] Loss: 0.692013
Train Epoch: 2 [9120/17010 (54%)] Loss: 0.928497
Train Epoch: 2 [9280/17010 (55%)] Loss: 1.138970
Train Epoch: 2 [9440/17010 (55%)] Loss: 0.938659
Train Epoch: 2 [9600/17010 (56%)] Loss: 1.046878
Train Epoch: 2 [9760/17010 (57%)] Loss: 0.796285
Train Epoch: 2 [9920/17010 (58%)] Loss: 1.257190
Train Epoch: 2 [10080/17010 (59%)] Loss: 0.634683
Train Epoch: 2 [10240/17010 (60%)] Loss: 1.182596
Train Epoch: 2 [10400/17010 (61%)] Loss: 0.778760
Train Epoch: 2 [10560/17010 (62%)] Loss: 0.976385
Train Epoch: 2 [10720/17010 (63%)] Loss: 1.018578
Train Epoch: 2 [10880/17010 (64%)] Loss: 0.941843
Train Epoch: 2 [11040/17010 (65%)] Loss: 0.673414
Train Epoch: 2 [11200/17010 (66%)] Loss: 0.626195
Train Epoch: 2 [11360/17010 (67%)] Loss: 0.606978
Train Epoch: 2 [11520/17010 (68%)] Loss: 0.524764
Train Epoch: 2 [11680/17010 (69%)] Loss: 0.834847
Train Epoch: 2 [11840/17010 (70%)] Loss: 0.782789
Train Epoch: 2 [12000/17010 (71%)] Loss: 0.665104
Train Epoch: 2 [12160/17010 (71%)] Loss: 0.612363
Train Epoch: 2 [12320/17010 (72%)] Loss: 0.695536
Train Epoch: 2 [12480/17010 (73%)] Loss: 0.659474
Train Epoch: 2 [12640/17010 (74%)] Loss: 1.125319
Train Epoch: 2 [12800/17010 (75%)] Loss: 1.184796
Train Epoch: 2 [12960/17010 (76%)] Loss: 0.481372
Train Epoch: 2 [13120/17010 (77%)] Loss: 0.699534
Train Epoch: 2 [13280/17010 (78%)] Loss: 0.688885
Train Epoch: 2 [13440/17010 (79%)] Loss: 0.613235
Train Epoch: 2 [13600/17010 (80%)] Loss: 0.730068
Train Epoch: 2 [13760/17010 (81%)] Loss: 1.158946
Train Epoch: 2 [13920/17010 (82%)] Loss: 0.877370
Train Epoch: 2 [14080/17010 (83%)] Loss: 0.608453
Train Epoch: 2 [14240/17010 (84%)] Loss: 0.876902
Train Epoch: 2 [14400/17010 (85%)] Loss: 0.647565
Train Epoch: 2 [14560/17010 (86%)] Loss: 0.573905
Train Epoch: 2 [14720/17010 (87%)] Loss: 0.568287
Train Epoch: 2 [14880/17010 (87%)] Loss: 0.619200
Train Epoch: 2 [15040/17010 (88%)] Loss: 0.699365
Train Epoch: 2 [15200/17010 (89%)] Loss: 0.809759
Train Epoch: 2 [15360/17010 (90%)] Loss: 0.864016
Train Epoch: 2 [15520/17010 (91%)] Loss: 0.572091
Train Epoch: 2 [15680/17010 (92%)] Loss: 0.438830
Train Epoch: 2 [15840/17010 (93%)] Loss: 0.487523
Train Epoch: 2 [16000/17010 (94%)] Loss: 0.836952
Train Epoch: 2 [16160/17010 (95%)] Loss: 0.802276
Train Epoch: 2 [16320/17010 (96%)] Loss: 1.126255
Train Epoch: 2 [16480/17010 (97%)] Loss: 0.872947
Train Epoch: 2 [16640/17010 (98%)] Loss: 0.939309
Train Epoch: 2 [16800/17010 (99%)] Loss: 0.808704
Train Epoch: 2 [16960/17010 (100%)] Loss: 1.026100
    epoch          : 2
    Train_loss     : 0.9375286782370474
    Train_accuracy : 0.6858683166248956
    Train_top_k_acc: 0.927233448203843
    Val_loss       : 1.875731771066785
    Val_accuracy   : 0.5578125
    Val_top_k_acc  : 0.8916666666666667
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch2.pth ...
Train Epoch: 3 [0/17010 (0%)] Loss: 0.666364
Train Epoch: 3 [160/17010 (1%)] Loss: 1.011444
Train Epoch: 3 [320/17010 (2%)] Loss: 0.676561
Train Epoch: 3 [480/17010 (3%)] Loss: 1.034714
Train Epoch: 3 [640/17010 (4%)] Loss: 0.843710
Train Epoch: 3 [800/17010 (5%)] Loss: 0.730512
Train Epoch: 3 [960/17010 (6%)] Loss: 0.449161
Train Epoch: 3 [1120/17010 (7%)] Loss: 0.370889
Train Epoch: 3 [1280/17010 (8%)] Loss: 0.757469
Train Epoch: 3 [1440/17010 (8%)] Loss: 0.450029
Train Epoch: 3 [1600/17010 (9%)] Loss: 0.542863
Train Epoch: 3 [1760/17010 (10%)] Loss: 0.769649
Train Epoch: 3 [1920/17010 (11%)] Loss: 0.769105
Train Epoch: 3 [2080/17010 (12%)] Loss: 0.746363
Train Epoch: 3 [2240/17010 (13%)] Loss: 0.504584
Train Epoch: 3 [2400/17010 (14%)] Loss: 1.136332
Train Epoch: 3 [2560/17010 (15%)] Loss: 0.864094
Train Epoch: 3 [2720/17010 (16%)] Loss: 0.629550
Train Epoch: 3 [2880/17010 (17%)] Loss: 0.851992
Train Epoch: 3 [3040/17010 (18%)] Loss: 0.448428
Train Epoch: 3 [3200/17010 (19%)] Loss: 0.606957
Train Epoch: 3 [3360/17010 (20%)] Loss: 0.407162
Train Epoch: 3 [3520/17010 (21%)] Loss: 0.723564
Train Epoch: 3 [3680/17010 (22%)] Loss: 0.978593
Train Epoch: 3 [3840/17010 (23%)] Loss: 0.784517
Train Epoch: 3 [4000/17010 (24%)] Loss: 0.380068
Train Epoch: 3 [4160/17010 (24%)] Loss: 0.492844
Train Epoch: 3 [4320/17010 (25%)] Loss: 0.665981
Train Epoch: 3 [4480/17010 (26%)] Loss: 0.607336
Train Epoch: 3 [4640/17010 (27%)] Loss: 0.557917
Train Epoch: 3 [4800/17010 (28%)] Loss: 0.931091
Train Epoch: 3 [4960/17010 (29%)] Loss: 0.521044
Train Epoch: 3 [5120/17010 (30%)] Loss: 0.705418
Train Epoch: 3 [5280/17010 (31%)] Loss: 0.333251
Train Epoch: 3 [5440/17010 (32%)] Loss: 0.768128
Train Epoch: 3 [5600/17010 (33%)] Loss: 0.501015
Train Epoch: 3 [5760/17010 (34%)] Loss: 0.492139
Train Epoch: 3 [5920/17010 (35%)] Loss: 0.499464
Train Epoch: 3 [6080/17010 (36%)] Loss: 0.585631
Train Epoch: 3 [6240/17010 (37%)] Loss: 0.600395
Train Epoch: 3 [6400/17010 (38%)] Loss: 0.462319
Train Epoch: 3 [6560/17010 (39%)] Loss: 0.622800
Train Epoch: 3 [6720/17010 (40%)] Loss: 0.547484
Train Epoch: 3 [6880/17010 (40%)] Loss: 0.873639
Train Epoch: 3 [7040/17010 (41%)] Loss: 1.035784
Train Epoch: 3 [7200/17010 (42%)] Loss: 0.476985
Train Epoch: 3 [7360/17010 (43%)] Loss: 0.475195
Train Epoch: 3 [7520/17010 (44%)] Loss: 0.546360
Train Epoch: 3 [7680/17010 (45%)] Loss: 0.365853
Train Epoch: 3 [7840/17010 (46%)] Loss: 0.841399
Train Epoch: 3 [8000/17010 (47%)] Loss: 0.474181
Train Epoch: 3 [8160/17010 (48%)] Loss: 0.756206
Train Epoch: 3 [8320/17010 (49%)] Loss: 0.979698
Train Epoch: 3 [8480/17010 (50%)] Loss: 0.556481
Train Epoch: 3 [8640/17010 (51%)] Loss: 0.489049
Train Epoch: 3 [8800/17010 (52%)] Loss: 0.600268
Train Epoch: 3 [8960/17010 (53%)] Loss: 0.501989
Train Epoch: 3 [9120/17010 (54%)] Loss: 0.857598
Train Epoch: 3 [9280/17010 (55%)] Loss: 0.439639
Train Epoch: 3 [9440/17010 (55%)] Loss: 0.902115
Train Epoch: 3 [9600/17010 (56%)] Loss: 0.481095
Train Epoch: 3 [9760/17010 (57%)] Loss: 0.392324
Train Epoch: 3 [9920/17010 (58%)] Loss: 0.836620
Train Epoch: 3 [10080/17010 (59%)] Loss: 0.381903
Train Epoch: 3 [10240/17010 (60%)] Loss: 0.621712
Train Epoch: 3 [10400/17010 (61%)] Loss: 0.483929
Train Epoch: 3 [10560/17010 (62%)] Loss: 0.575462
Train Epoch: 3 [10720/17010 (63%)] Loss: 0.387614
Train Epoch: 3 [10880/17010 (64%)] Loss: 0.562806
Train Epoch: 3 [11040/17010 (65%)] Loss: 0.626143
Train Epoch: 3 [11200/17010 (66%)] Loss: 0.521954
Train Epoch: 3 [11360/17010 (67%)] Loss: 0.506963
Train Epoch: 3 [11520/17010 (68%)] Loss: 0.699302
Train Epoch: 3 [11680/17010 (69%)] Loss: 0.782641
Train Epoch: 3 [11840/17010 (70%)] Loss: 0.778169
Train Epoch: 3 [12000/17010 (71%)] Loss: 0.517867
Train Epoch: 3 [12160/17010 (71%)] Loss: 0.558114
Train Epoch: 3 [12320/17010 (72%)] Loss: 0.657266
Train Epoch: 3 [12480/17010 (73%)] Loss: 0.518398
Train Epoch: 3 [12640/17010 (74%)] Loss: 0.708491
Train Epoch: 3 [12800/17010 (75%)] Loss: 0.413582
Train Epoch: 3 [12960/17010 (76%)] Loss: 0.763339
Train Epoch: 3 [13120/17010 (77%)] Loss: 0.550987
Train Epoch: 3 [13280/17010 (78%)] Loss: 0.238562
Train Epoch: 3 [13440/17010 (79%)] Loss: 0.502555
Train Epoch: 3 [13600/17010 (80%)] Loss: 0.648475
Train Epoch: 3 [13760/17010 (81%)] Loss: 0.875880
Train Epoch: 3 [13920/17010 (82%)] Loss: 0.502318
Train Epoch: 3 [14080/17010 (83%)] Loss: 0.726502
Train Epoch: 3 [14240/17010 (84%)] Loss: 0.715246
Train Epoch: 3 [14400/17010 (85%)] Loss: 0.708115
Train Epoch: 3 [14560/17010 (86%)] Loss: 0.687836
Train Epoch: 3 [14720/17010 (87%)] Loss: 0.804072
Train Epoch: 3 [14880/17010 (87%)] Loss: 0.512497
Train Epoch: 3 [15040/17010 (88%)] Loss: 0.935454
Train Epoch: 3 [15200/17010 (89%)] Loss: 0.592605
Train Epoch: 3 [15360/17010 (90%)] Loss: 0.701582
Train Epoch: 3 [15520/17010 (91%)] Loss: 0.268422
Train Epoch: 3 [15680/17010 (92%)] Loss: 0.465147
Train Epoch: 3 [15840/17010 (93%)] Loss: 0.674454
Train Epoch: 3 [16000/17010 (94%)] Loss: 0.702433
Train Epoch: 3 [16160/17010 (95%)] Loss: 0.360886
Train Epoch: 3 [16320/17010 (96%)] Loss: 0.570367
Train Epoch: 3 [16480/17010 (97%)] Loss: 0.716593
Train Epoch: 3 [16640/17010 (98%)] Loss: 1.026893
Train Epoch: 3 [16800/17010 (99%)] Loss: 0.470791
Train Epoch: 3 [16960/17010 (100%)] Loss: 0.709072
    epoch          : 3
    Train_loss     : 0.6077310575970581
    Train_accuracy : 0.7946363304093567
    Train_top_k_acc: 0.9737429511278195
    Val_loss       : 0.7047126625974973
    Val_accuracy   : 0.7223958333333333
    Val_top_k_acc  : 0.9583333333333334
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch3.pth ...
Train Epoch: 4 [0/17010 (0%)] Loss: 0.420834
Train Epoch: 4 [160/17010 (1%)] Loss: 0.335328
Train Epoch: 4 [320/17010 (2%)] Loss: 0.634403
Train Epoch: 4 [480/17010 (3%)] Loss: 0.486466
Train Epoch: 4 [640/17010 (4%)] Loss: 0.687299
Train Epoch: 4 [800/17010 (5%)] Loss: 0.772727
Train Epoch: 4 [960/17010 (6%)] Loss: 0.555279
Train Epoch: 4 [1120/17010 (7%)] Loss: 0.389388
Train Epoch: 4 [1280/17010 (8%)] Loss: 0.459775
Train Epoch: 4 [1440/17010 (8%)] Loss: 0.351667
Train Epoch: 4 [1600/17010 (9%)] Loss: 0.582976
Train Epoch: 4 [1760/17010 (10%)] Loss: 0.486125
Train Epoch: 4 [1920/17010 (11%)] Loss: 0.367915
Train Epoch: 4 [2080/17010 (12%)] Loss: 0.355829
Train Epoch: 4 [2240/17010 (13%)] Loss: 0.441779
Train Epoch: 4 [2400/17010 (14%)] Loss: 0.372389
Train Epoch: 4 [2560/17010 (15%)] Loss: 0.490521
Train Epoch: 4 [2720/17010 (16%)] Loss: 0.301945
Train Epoch: 4 [2880/17010 (17%)] Loss: 0.446665
Train Epoch: 4 [3040/17010 (18%)] Loss: 0.558271
Train Epoch: 4 [3200/17010 (19%)] Loss: 0.250318
Train Epoch: 4 [3360/17010 (20%)] Loss: 0.381582
Train Epoch: 4 [3520/17010 (21%)] Loss: 0.321204
Train Epoch: 4 [3680/17010 (22%)] Loss: 0.489396
Train Epoch: 4 [3840/17010 (23%)] Loss: 0.480712
Train Epoch: 4 [4000/17010 (24%)] Loss: 0.811622
Train Epoch: 4 [4160/17010 (24%)] Loss: 0.527751
Train Epoch: 4 [4320/17010 (25%)] Loss: 0.444420
Train Epoch: 4 [4480/17010 (26%)] Loss: 0.517834
Train Epoch: 4 [4640/17010 (27%)] Loss: 0.695517
Train Epoch: 4 [4800/17010 (28%)] Loss: 0.371803
Train Epoch: 4 [4960/17010 (29%)] Loss: 0.526010
Train Epoch: 4 [5120/17010 (30%)] Loss: 0.321177
Train Epoch: 4 [5280/17010 (31%)] Loss: 0.313472
Train Epoch: 4 [5440/17010 (32%)] Loss: 0.442237
Train Epoch: 4 [5600/17010 (33%)] Loss: 0.521498
Train Epoch: 4 [5760/17010 (34%)] Loss: 0.237926
Train Epoch: 4 [5920/17010 (35%)] Loss: 0.750034
Train Epoch: 4 [6080/17010 (36%)] Loss: 0.427328
Train Epoch: 4 [6240/17010 (37%)] Loss: 0.307055
Train Epoch: 4 [6400/17010 (38%)] Loss: 0.682920
Train Epoch: 4 [6560/17010 (39%)] Loss: 0.378924
Train Epoch: 4 [6720/17010 (40%)] Loss: 0.335475
Train Epoch: 4 [6880/17010 (40%)] Loss: 0.406222
Train Epoch: 4 [7040/17010 (41%)] Loss: 0.985291
Train Epoch: 4 [7200/17010 (42%)] Loss: 0.377949
Train Epoch: 4 [7360/17010 (43%)] Loss: 0.852911
Train Epoch: 4 [7520/17010 (44%)] Loss: 0.641213
Train Epoch: 4 [7680/17010 (45%)] Loss: 0.469629
Train Epoch: 4 [7840/17010 (46%)] Loss: 0.743015
Train Epoch: 4 [8000/17010 (47%)] Loss: 0.873554
Train Epoch: 4 [8160/17010 (48%)] Loss: 0.634426
Train Epoch: 4 [8320/17010 (49%)] Loss: 0.522676
Train Epoch: 4 [8480/17010 (50%)] Loss: 0.405639
Train Epoch: 4 [8640/17010 (51%)] Loss: 0.503205
Train Epoch: 4 [8800/17010 (52%)] Loss: 0.320354
Train Epoch: 4 [8960/17010 (53%)] Loss: 0.864735
Train Epoch: 4 [9120/17010 (54%)] Loss: 0.281177
Train Epoch: 4 [9280/17010 (55%)] Loss: 0.592611
Train Epoch: 4 [9440/17010 (55%)] Loss: 0.849341
Train Epoch: 4 [9600/17010 (56%)] Loss: 0.939472
Train Epoch: 4 [9760/17010 (57%)] Loss: 0.902232
Train Epoch: 4 [9920/17010 (58%)] Loss: 0.309451
Train Epoch: 4 [10080/17010 (59%)] Loss: 0.585049
Train Epoch: 4 [10240/17010 (60%)] Loss: 0.484770
Train Epoch: 4 [10400/17010 (61%)] Loss: 0.236609
Train Epoch: 4 [10560/17010 (62%)] Loss: 0.389195
Train Epoch: 4 [10720/17010 (63%)] Loss: 0.541325
Train Epoch: 4 [10880/17010 (64%)] Loss: 0.201793
Train Epoch: 4 [11040/17010 (65%)] Loss: 0.895018
Train Epoch: 4 [11200/17010 (66%)] Loss: 0.376358
Train Epoch: 4 [11360/17010 (67%)] Loss: 0.305991
Train Epoch: 4 [11520/17010 (68%)] Loss: 0.945195
Train Epoch: 4 [11680/17010 (69%)] Loss: 0.421779
Train Epoch: 4 [11840/17010 (70%)] Loss: 0.520747
Train Epoch: 4 [12000/17010 (71%)] Loss: 0.375938
Train Epoch: 4 [12160/17010 (71%)] Loss: 1.081439
Train Epoch: 4 [12320/17010 (72%)] Loss: 0.567369
Train Epoch: 4 [12480/17010 (73%)] Loss: 0.289427
Train Epoch: 4 [12640/17010 (74%)] Loss: 0.404288
Train Epoch: 4 [12800/17010 (75%)] Loss: 0.363369
Train Epoch: 4 [12960/17010 (76%)] Loss: 0.451909
Train Epoch: 4 [13120/17010 (77%)] Loss: 0.473434
Train Epoch: 4 [13280/17010 (78%)] Loss: 0.604070
Train Epoch: 4 [13440/17010 (79%)] Loss: 0.553420
Train Epoch: 4 [13600/17010 (80%)] Loss: 0.496735
Train Epoch: 4 [13760/17010 (81%)] Loss: 0.233062
Train Epoch: 4 [13920/17010 (82%)] Loss: 0.572730
Train Epoch: 4 [14080/17010 (83%)] Loss: 0.497019
Train Epoch: 4 [14240/17010 (84%)] Loss: 0.418977
Train Epoch: 4 [14400/17010 (85%)] Loss: 0.471347
Train Epoch: 4 [14560/17010 (86%)] Loss: 0.724023
Train Epoch: 4 [14720/17010 (87%)] Loss: 0.335127
Train Epoch: 4 [14880/17010 (87%)] Loss: 0.529651
Train Epoch: 4 [15040/17010 (88%)] Loss: 0.792641
Train Epoch: 4 [15200/17010 (89%)] Loss: 0.573032
Train Epoch: 4 [15360/17010 (90%)] Loss: 0.165520
Train Epoch: 4 [15520/17010 (91%)] Loss: 0.532849
Train Epoch: 4 [15680/17010 (92%)] Loss: 0.317850
Train Epoch: 4 [15840/17010 (93%)] Loss: 0.373686
Train Epoch: 4 [16000/17010 (94%)] Loss: 0.570694
Train Epoch: 4 [16160/17010 (95%)] Loss: 0.345106
Train Epoch: 4 [16320/17010 (96%)] Loss: 0.311245
Train Epoch: 4 [16480/17010 (97%)] Loss: 0.491989
Train Epoch: 4 [16640/17010 (98%)] Loss: 0.394070
Train Epoch: 4 [16800/17010 (99%)] Loss: 0.154790
Train Epoch: 4 [16960/17010 (100%)] Loss: 0.745040
    epoch          : 4
    Train_loss     : 0.47725934638900863
    Train_accuracy : 0.8352130325814536
    Train_top_k_acc: 0.9860197368421053
    Val_loss       : 0.564974981546402
    Val_accuracy   : 0.8151041666666666
    Val_top_k_acc  : 0.9739583333333334
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch4.pth ...
Train Epoch: 5 [0/17010 (0%)] Loss: 0.432323
Train Epoch: 5 [160/17010 (1%)] Loss: 0.552547
Train Epoch: 5 [320/17010 (2%)] Loss: 0.339253
Train Epoch: 5 [480/17010 (3%)] Loss: 0.336265
Train Epoch: 5 [640/17010 (4%)] Loss: 0.503607
Train Epoch: 5 [800/17010 (5%)] Loss: 0.420872
Train Epoch: 5 [960/17010 (6%)] Loss: 0.376294
Train Epoch: 5 [1120/17010 (7%)] Loss: 0.291733
Train Epoch: 5 [1280/17010 (8%)] Loss: 0.455687
Train Epoch: 5 [1440/17010 (8%)] Loss: 0.502701
Train Epoch: 5 [1600/17010 (9%)] Loss: 0.249487
Train Epoch: 5 [1760/17010 (10%)] Loss: 0.414472
Train Epoch: 5 [1920/17010 (11%)] Loss: 0.701748
Train Epoch: 5 [2080/17010 (12%)] Loss: 0.208332
Train Epoch: 5 [2240/17010 (13%)] Loss: 0.361206
Train Epoch: 5 [2400/17010 (14%)] Loss: 0.365345
Train Epoch: 5 [2560/17010 (15%)] Loss: 0.347366
Train Epoch: 5 [2720/17010 (16%)] Loss: 0.307139
Train Epoch: 5 [2880/17010 (17%)] Loss: 0.362676
Train Epoch: 5 [3040/17010 (18%)] Loss: 0.455591
Train Epoch: 5 [3200/17010 (19%)] Loss: 0.391667
Train Epoch: 5 [3360/17010 (20%)] Loss: 0.296627
Train Epoch: 5 [3520/17010 (21%)] Loss: 0.777698
Train Epoch: 5 [3680/17010 (22%)] Loss: 0.170733
Train Epoch: 5 [3840/17010 (23%)] Loss: 0.287144
Train Epoch: 5 [4000/17010 (24%)] Loss: 0.430412
Train Epoch: 5 [4160/17010 (24%)] Loss: 0.181963
Train Epoch: 5 [4320/17010 (25%)] Loss: 0.640726
Train Epoch: 5 [4480/17010 (26%)] Loss: 0.524581
Train Epoch: 5 [4640/17010 (27%)] Loss: 0.452858
Train Epoch: 5 [4800/17010 (28%)] Loss: 0.548129
Train Epoch: 5 [4960/17010 (29%)] Loss: 0.410357
Train Epoch: 5 [5120/17010 (30%)] Loss: 0.348408
Train Epoch: 5 [5280/17010 (31%)] Loss: 0.325471
Train Epoch: 5 [5440/17010 (32%)] Loss: 0.219710
Train Epoch: 5 [5600/17010 (33%)] Loss: 0.188393
Train Epoch: 5 [5760/17010 (34%)] Loss: 0.245336
Train Epoch: 5 [5920/17010 (35%)] Loss: 0.279311
Train Epoch: 5 [6080/17010 (36%)] Loss: 0.368362
Train Epoch: 5 [6240/17010 (37%)] Loss: 0.166900
Train Epoch: 5 [6400/17010 (38%)] Loss: 0.343981
Train Epoch: 5 [6560/17010 (39%)] Loss: 0.695667
Train Epoch: 5 [6720/17010 (40%)] Loss: 0.271788
Train Epoch: 5 [6880/17010 (40%)] Loss: 0.555373
Train Epoch: 5 [7040/17010 (41%)] Loss: 0.590603
Train Epoch: 5 [7200/17010 (42%)] Loss: 0.416926
Train Epoch: 5 [7360/17010 (43%)] Loss: 0.438197
Train Epoch: 5 [7520/17010 (44%)] Loss: 0.354802
Train Epoch: 5 [7680/17010 (45%)] Loss: 0.846471
Train Epoch: 5 [7840/17010 (46%)] Loss: 0.309438
Train Epoch: 5 [8000/17010 (47%)] Loss: 0.399491
Train Epoch: 5 [8160/17010 (48%)] Loss: 0.325150
Train Epoch: 5 [8320/17010 (49%)] Loss: 0.270865
Train Epoch: 5 [8480/17010 (50%)] Loss: 0.433940
Train Epoch: 5 [8640/17010 (51%)] Loss: 0.369525
Train Epoch: 5 [8800/17010 (52%)] Loss: 0.338335
Train Epoch: 5 [8960/17010 (53%)] Loss: 0.492314
Train Epoch: 5 [9120/17010 (54%)] Loss: 0.475116
Train Epoch: 5 [9280/17010 (55%)] Loss: 0.488742
Train Epoch: 5 [9440/17010 (55%)] Loss: 0.329909
Train Epoch: 5 [9600/17010 (56%)] Loss: 0.306762
Train Epoch: 5 [9760/17010 (57%)] Loss: 0.778910
Train Epoch: 5 [9920/17010 (58%)] Loss: 0.460559
Train Epoch: 5 [10080/17010 (59%)] Loss: 0.491170
Train Epoch: 5 [10240/17010 (60%)] Loss: 0.305358
Train Epoch: 5 [10400/17010 (61%)] Loss: 0.161611
Train Epoch: 5 [10560/17010 (62%)] Loss: 0.285608
Train Epoch: 5 [10720/17010 (63%)] Loss: 0.414088
Train Epoch: 5 [10880/17010 (64%)] Loss: 0.296615
Train Epoch: 5 [11040/17010 (65%)] Loss: 0.404848
Train Epoch: 5 [11200/17010 (66%)] Loss: 0.367329
Train Epoch: 5 [11360/17010 (67%)] Loss: 0.336776
Train Epoch: 5 [11520/17010 (68%)] Loss: 0.494594
Train Epoch: 5 [11680/17010 (69%)] Loss: 0.638637
Train Epoch: 5 [11840/17010 (70%)] Loss: 0.441582
Train Epoch: 5 [12000/17010 (71%)] Loss: 0.210676
Train Epoch: 5 [12160/17010 (71%)] Loss: 0.406216
Train Epoch: 5 [12320/17010 (72%)] Loss: 0.155645
Train Epoch: 5 [12480/17010 (73%)] Loss: 0.226489
Train Epoch: 5 [12640/17010 (74%)] Loss: 0.277727
Train Epoch: 5 [12800/17010 (75%)] Loss: 0.181536
Train Epoch: 5 [12960/17010 (76%)] Loss: 0.664788
Train Epoch: 5 [13120/17010 (77%)] Loss: 0.359262
Train Epoch: 5 [13280/17010 (78%)] Loss: 0.385555
Train Epoch: 5 [13440/17010 (79%)] Loss: 0.681291
Train Epoch: 5 [13600/17010 (80%)] Loss: 0.365969
Train Epoch: 5 [13760/17010 (81%)] Loss: 0.444421
Train Epoch: 5 [13920/17010 (82%)] Loss: 0.593624
Train Epoch: 5 [14080/17010 (83%)] Loss: 0.328941
Train Epoch: 5 [14240/17010 (84%)] Loss: 0.220100
Train Epoch: 5 [14400/17010 (85%)] Loss: 0.726818
Train Epoch: 5 [14560/17010 (86%)] Loss: 0.379216
Train Epoch: 5 [14720/17010 (87%)] Loss: 0.236831
Train Epoch: 5 [14880/17010 (87%)] Loss: 0.175965
Train Epoch: 5 [15040/17010 (88%)] Loss: 0.410435
Train Epoch: 5 [15200/17010 (89%)] Loss: 0.443980
Train Epoch: 5 [15360/17010 (90%)] Loss: 0.176537
Train Epoch: 5 [15520/17010 (91%)] Loss: 0.137085
Train Epoch: 5 [15680/17010 (92%)] Loss: 0.228037
Train Epoch: 5 [15840/17010 (93%)] Loss: 0.342479
Train Epoch: 5 [16000/17010 (94%)] Loss: 0.287280
Train Epoch: 5 [16160/17010 (95%)] Loss: 0.213870
Train Epoch: 5 [16320/17010 (96%)] Loss: 0.423952
Train Epoch: 5 [16480/17010 (97%)] Loss: 0.259877
Train Epoch: 5 [16640/17010 (98%)] Loss: 0.255948
Train Epoch: 5 [16800/17010 (99%)] Loss: 0.213755
Train Epoch: 5 [16960/17010 (100%)] Loss: 0.782689
    epoch          : 5
    Train_loss     : 0.38274901949877577
    Train_accuracy : 0.8694588032581453
    Train_top_k_acc: 0.9914826127819549
    Val_loss       : 0.6618670692046483
    Val_accuracy   : 0.759375
    Val_top_k_acc  : 0.9848958333333333
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch5.pth ...
Train Epoch: 6 [0/17010 (0%)] Loss: 0.222095
Train Epoch: 6 [160/17010 (1%)] Loss: 0.445470
Train Epoch: 6 [320/17010 (2%)] Loss: 0.456056
Train Epoch: 6 [480/17010 (3%)] Loss: 0.079820
Train Epoch: 6 [640/17010 (4%)] Loss: 0.361676
Train Epoch: 6 [800/17010 (5%)] Loss: 0.283316
Train Epoch: 6 [960/17010 (6%)] Loss: 0.420448
Train Epoch: 6 [1120/17010 (7%)] Loss: 0.154228
Train Epoch: 6 [1280/17010 (8%)] Loss: 0.528707
Train Epoch: 6 [1440/17010 (8%)] Loss: 0.308197
Train Epoch: 6 [1600/17010 (9%)] Loss: 0.394685
Train Epoch: 6 [1760/17010 (10%)] Loss: 0.285385
Train Epoch: 6 [1920/17010 (11%)] Loss: 0.620382
Train Epoch: 6 [2080/17010 (12%)] Loss: 0.452724
Train Epoch: 6 [2240/17010 (13%)] Loss: 0.353290
Train Epoch: 6 [2400/17010 (14%)] Loss: 0.278911
Train Epoch: 6 [2560/17010 (15%)] Loss: 0.160518
Train Epoch: 6 [2720/17010 (16%)] Loss: 0.268616
Train Epoch: 6 [2880/17010 (17%)] Loss: 0.341212
Train Epoch: 6 [3040/17010 (18%)] Loss: 0.151642
Train Epoch: 6 [3200/17010 (19%)] Loss: 0.198145
Train Epoch: 6 [3360/17010 (20%)] Loss: 0.294992
Train Epoch: 6 [3520/17010 (21%)] Loss: 0.511470
Train Epoch: 6 [3680/17010 (22%)] Loss: 0.165761
Train Epoch: 6 [3840/17010 (23%)] Loss: 0.460151
Train Epoch: 6 [4000/17010 (24%)] Loss: 0.638084
Train Epoch: 6 [4160/17010 (24%)] Loss: 0.143007
Train Epoch: 6 [4320/17010 (25%)] Loss: 0.289338
Train Epoch: 6 [4480/17010 (26%)] Loss: 0.148339
Train Epoch: 6 [4640/17010 (27%)] Loss: 0.322893
Train Epoch: 6 [4800/17010 (28%)] Loss: 0.376176
Train Epoch: 6 [4960/17010 (29%)] Loss: 0.240450
Train Epoch: 6 [5120/17010 (30%)] Loss: 0.160753
Train Epoch: 6 [5280/17010 (31%)] Loss: 0.456487
Train Epoch: 6 [5440/17010 (32%)] Loss: 0.350371
Train Epoch: 6 [5600/17010 (33%)] Loss: 0.197816
Train Epoch: 6 [5760/17010 (34%)] Loss: 0.332256
Train Epoch: 6 [5920/17010 (35%)] Loss: 0.141092
Train Epoch: 6 [6080/17010 (36%)] Loss: 0.450961
Train Epoch: 6 [6240/17010 (37%)] Loss: 0.358291
Train Epoch: 6 [6400/17010 (38%)] Loss: 0.272482
Train Epoch: 6 [6560/17010 (39%)] Loss: 0.482155
Train Epoch: 6 [6720/17010 (40%)] Loss: 0.194093
Train Epoch: 6 [6880/17010 (40%)] Loss: 0.154673
Train Epoch: 6 [7040/17010 (41%)] Loss: 0.335036
Train Epoch: 6 [7200/17010 (42%)] Loss: 0.237176
Train Epoch: 6 [7360/17010 (43%)] Loss: 0.155715
Train Epoch: 6 [7520/17010 (44%)] Loss: 0.486406
Train Epoch: 6 [7680/17010 (45%)] Loss: 0.319485
Train Epoch: 6 [7840/17010 (46%)] Loss: 0.337414
Train Epoch: 6 [8000/17010 (47%)] Loss: 0.251782
Train Epoch: 6 [8160/17010 (48%)] Loss: 0.356233
Train Epoch: 6 [8320/17010 (49%)] Loss: 0.154529
Train Epoch: 6 [8480/17010 (50%)] Loss: 0.352888
Train Epoch: 6 [8640/17010 (51%)] Loss: 0.174999
Train Epoch: 6 [8800/17010 (52%)] Loss: 0.238530
Train Epoch: 6 [8960/17010 (53%)] Loss: 0.315929
Train Epoch: 6 [9120/17010 (54%)] Loss: 0.244090
Train Epoch: 6 [9280/17010 (55%)] Loss: 0.422369
Train Epoch: 6 [9440/17010 (55%)] Loss: 0.404293
Train Epoch: 6 [9600/17010 (56%)] Loss: 0.300284
Train Epoch: 6 [9760/17010 (57%)] Loss: 0.176879
Train Epoch: 6 [9920/17010 (58%)] Loss: 0.464412
Train Epoch: 6 [10080/17010 (59%)] Loss: 0.268070
Train Epoch: 6 [10240/17010 (60%)] Loss: 0.416777
Train Epoch: 6 [10400/17010 (61%)] Loss: 0.318632
Train Epoch: 6 [10560/17010 (62%)] Loss: 0.470165
Train Epoch: 6 [10720/17010 (63%)] Loss: 0.244606
Train Epoch: 6 [10880/17010 (64%)] Loss: 0.339885
Train Epoch: 6 [11040/17010 (65%)] Loss: 0.281737
Train Epoch: 6 [11200/17010 (66%)] Loss: 0.132022
Train Epoch: 6 [11360/17010 (67%)] Loss: 0.221018
Train Epoch: 6 [11520/17010 (68%)] Loss: 0.405045
Train Epoch: 6 [11680/17010 (69%)] Loss: 0.467248
Train Epoch: 6 [11840/17010 (70%)] Loss: 0.464448
Train Epoch: 6 [12000/17010 (71%)] Loss: 0.447222
Train Epoch: 6 [12160/17010 (71%)] Loss: 0.316389
Train Epoch: 6 [12320/17010 (72%)] Loss: 0.340100
Train Epoch: 6 [12480/17010 (73%)] Loss: 0.171020
Train Epoch: 6 [12640/17010 (74%)] Loss: 0.553429
Train Epoch: 6 [12800/17010 (75%)] Loss: 0.955837
Train Epoch: 6 [12960/17010 (76%)] Loss: 0.344900
Train Epoch: 6 [13120/17010 (77%)] Loss: 0.329818
Train Epoch: 6 [13280/17010 (78%)] Loss: 0.513398
Train Epoch: 6 [13440/17010 (79%)] Loss: 0.321149
Train Epoch: 6 [13600/17010 (80%)] Loss: 0.442608
Train Epoch: 6 [13760/17010 (81%)] Loss: 0.594800
Train Epoch: 6 [13920/17010 (82%)] Loss: 0.480032
Train Epoch: 6 [14080/17010 (83%)] Loss: 0.227573
Train Epoch: 6 [14240/17010 (84%)] Loss: 0.277580
Train Epoch: 6 [14400/17010 (85%)] Loss: 0.253592
Train Epoch: 6 [14560/17010 (86%)] Loss: 0.116742
Train Epoch: 6 [14720/17010 (87%)] Loss: 0.209281
Train Epoch: 6 [14880/17010 (87%)] Loss: 0.325978
Train Epoch: 6 [15040/17010 (88%)] Loss: 0.248805
Train Epoch: 6 [15200/17010 (89%)] Loss: 0.189548
Train Epoch: 6 [15360/17010 (90%)] Loss: 0.252493
Train Epoch: 6 [15520/17010 (91%)] Loss: 0.711724
Train Epoch: 6 [15680/17010 (92%)] Loss: 0.314510
Train Epoch: 6 [15840/17010 (93%)] Loss: 0.698417
Train Epoch: 6 [16000/17010 (94%)] Loss: 0.301352
Train Epoch: 6 [16160/17010 (95%)] Loss: 0.348814
Train Epoch: 6 [16320/17010 (96%)] Loss: 0.512626
Train Epoch: 6 [16480/17010 (97%)] Loss: 0.395421
Train Epoch: 6 [16640/17010 (98%)] Loss: 0.195482
Train Epoch: 6 [16800/17010 (99%)] Loss: 0.284672
Train Epoch: 6 [16960/17010 (100%)] Loss: 0.550396
    epoch          : 6
    Train_loss     : 0.3336315309012631
    Train_accuracy : 0.8834717000835423
    Train_top_k_acc: 0.9939497180451128
    Val_loss       : 0.3942139202107986
    Val_accuracy   : 0.8614583333333333
    Val_top_k_acc  : 0.9916666666666667
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch6.pth ...
Train Epoch: 7 [0/17010 (0%)] Loss: 0.160779
Train Epoch: 7 [160/17010 (1%)] Loss: 0.198282
Train Epoch: 7 [320/17010 (2%)] Loss: 0.216261
Train Epoch: 7 [480/17010 (3%)] Loss: 0.426712
Train Epoch: 7 [640/17010 (4%)] Loss: 0.192676
Train Epoch: 7 [800/17010 (5%)] Loss: 0.220024
Train Epoch: 7 [960/17010 (6%)] Loss: 0.489020
Train Epoch: 7 [1120/17010 (7%)] Loss: 0.410298
Train Epoch: 7 [1280/17010 (8%)] Loss: 0.177621
Train Epoch: 7 [1440/17010 (8%)] Loss: 0.307003
Train Epoch: 7 [1600/17010 (9%)] Loss: 0.143468
Train Epoch: 7 [1760/17010 (10%)] Loss: 0.196938
Train Epoch: 7 [1920/17010 (11%)] Loss: 0.260541
Train Epoch: 7 [2080/17010 (12%)] Loss: 0.340659
Train Epoch: 7 [2240/17010 (13%)] Loss: 0.086644
Train Epoch: 7 [2400/17010 (14%)] Loss: 0.107327
Train Epoch: 7 [2560/17010 (15%)] Loss: 0.347361
Train Epoch: 7 [2720/17010 (16%)] Loss: 0.197469
Train Epoch: 7 [2880/17010 (17%)] Loss: 0.205849
Train Epoch: 7 [3040/17010 (18%)] Loss: 0.235513
Train Epoch: 7 [3200/17010 (19%)] Loss: 0.228405
Train Epoch: 7 [3360/17010 (20%)] Loss: 0.228505
Train Epoch: 7 [3520/17010 (21%)] Loss: 0.387634
Train Epoch: 7 [3680/17010 (22%)] Loss: 0.189724
Train Epoch: 7 [3840/17010 (23%)] Loss: 0.458234
Train Epoch: 7 [4000/17010 (24%)] Loss: 0.721040
Train Epoch: 7 [4160/17010 (24%)] Loss: 0.178402
Train Epoch: 7 [4320/17010 (25%)] Loss: 0.217669
Train Epoch: 7 [4480/17010 (26%)] Loss: 0.224722
Train Epoch: 7 [4640/17010 (27%)] Loss: 0.352368
Train Epoch: 7 [4800/17010 (28%)] Loss: 0.086278
Train Epoch: 7 [4960/17010 (29%)] Loss: 0.349886
Train Epoch: 7 [5120/17010 (30%)] Loss: 0.276008
Train Epoch: 7 [5280/17010 (31%)] Loss: 0.173667
Train Epoch: 7 [5440/17010 (32%)] Loss: 0.177421
Train Epoch: 7 [5600/17010 (33%)] Loss: 0.440218
Train Epoch: 7 [5760/17010 (34%)] Loss: 0.515676
Train Epoch: 7 [5920/17010 (35%)] Loss: 0.335185
Train Epoch: 7 [6080/17010 (36%)] Loss: 0.181005
Train Epoch: 7 [6240/17010 (37%)] Loss: 0.256400
Train Epoch: 7 [6400/17010 (38%)] Loss: 0.232680
Train Epoch: 7 [6560/17010 (39%)] Loss: 0.254150
Train Epoch: 7 [6720/17010 (40%)] Loss: 0.373311
Train Epoch: 7 [6880/17010 (40%)] Loss: 0.486320
Train Epoch: 7 [7040/17010 (41%)] Loss: 0.276916
Train Epoch: 7 [7200/17010 (42%)] Loss: 0.290137
Train Epoch: 7 [7360/17010 (43%)] Loss: 0.105300
Train Epoch: 7 [7520/17010 (44%)] Loss: 0.311389
Train Epoch: 7 [7680/17010 (45%)] Loss: 0.378152
Train Epoch: 7 [7840/17010 (46%)] Loss: 0.462900
Train Epoch: 7 [8000/17010 (47%)] Loss: 0.166977
Train Epoch: 7 [8160/17010 (48%)] Loss: 0.224705
Train Epoch: 7 [8320/17010 (49%)] Loss: 0.120779
Train Epoch: 7 [8480/17010 (50%)] Loss: 0.169416
Train Epoch: 7 [8640/17010 (51%)] Loss: 0.197916
Train Epoch: 7 [8800/17010 (52%)] Loss: 0.408768
Train Epoch: 7 [8960/17010 (53%)] Loss: 0.324291
Train Epoch: 7 [9120/17010 (54%)] Loss: 0.708905
Train Epoch: 7 [9280/17010 (55%)] Loss: 0.152188
Train Epoch: 7 [9440/17010 (55%)] Loss: 0.545773
Train Epoch: 7 [9600/17010 (56%)] Loss: 0.139422
Train Epoch: 7 [9760/17010 (57%)] Loss: 0.369490
Train Epoch: 7 [9920/17010 (58%)] Loss: 0.246192
Train Epoch: 7 [10080/17010 (59%)] Loss: 0.189705
Train Epoch: 7 [10240/17010 (60%)] Loss: 0.259169
Train Epoch: 7 [10400/17010 (61%)] Loss: 0.102167
Train Epoch: 7 [10560/17010 (62%)] Loss: 0.220990
Train Epoch: 7 [10720/17010 (63%)] Loss: 0.153741
Train Epoch: 7 [10880/17010 (64%)] Loss: 0.303474
Train Epoch: 7 [11040/17010 (65%)] Loss: 0.299039
Train Epoch: 7 [11200/17010 (66%)] Loss: 0.170361
Train Epoch: 7 [11360/17010 (67%)] Loss: 0.413606
Train Epoch: 7 [11520/17010 (68%)] Loss: 0.171995
Train Epoch: 7 [11680/17010 (69%)] Loss: 0.229707
Train Epoch: 7 [11840/17010 (70%)] Loss: 0.148417
Train Epoch: 7 [12000/17010 (71%)] Loss: 0.306702
Train Epoch: 7 [12160/17010 (71%)] Loss: 0.112242
Train Epoch: 7 [12320/17010 (72%)] Loss: 0.289635
Train Epoch: 7 [12480/17010 (73%)] Loss: 0.262335
Train Epoch: 7 [12640/17010 (74%)] Loss: 0.214057
Train Epoch: 7 [12800/17010 (75%)] Loss: 0.309538
Train Epoch: 7 [12960/17010 (76%)] Loss: 0.248285
Train Epoch: 7 [13120/17010 (77%)] Loss: 0.426892
Train Epoch: 7 [13280/17010 (78%)] Loss: 0.122090
Train Epoch: 7 [13440/17010 (79%)] Loss: 0.433559
Train Epoch: 7 [13600/17010 (80%)] Loss: 0.213377
Train Epoch: 7 [13760/17010 (81%)] Loss: 0.431841
Train Epoch: 7 [13920/17010 (82%)] Loss: 0.301620
Train Epoch: 7 [14080/17010 (83%)] Loss: 0.405466
Train Epoch: 7 [14240/17010 (84%)] Loss: 0.169633
Train Epoch: 7 [14400/17010 (85%)] Loss: 0.131897
Train Epoch: 7 [14560/17010 (86%)] Loss: 0.265156
Train Epoch: 7 [14720/17010 (87%)] Loss: 0.118044
Train Epoch: 7 [14880/17010 (87%)] Loss: 0.528450
Train Epoch: 7 [15040/17010 (88%)] Loss: 0.183749
Train Epoch: 7 [15200/17010 (89%)] Loss: 0.198105
Train Epoch: 7 [15360/17010 (90%)] Loss: 0.219062
Train Epoch: 7 [15520/17010 (91%)] Loss: 0.387261
Train Epoch: 7 [15680/17010 (92%)] Loss: 0.241726
Train Epoch: 7 [15840/17010 (93%)] Loss: 0.452281
Train Epoch: 7 [16000/17010 (94%)] Loss: 0.183404
Train Epoch: 7 [16160/17010 (95%)] Loss: 0.095133
Train Epoch: 7 [16320/17010 (96%)] Loss: 0.204445
Train Epoch: 7 [16480/17010 (97%)] Loss: 0.265699
Train Epoch: 7 [16640/17010 (98%)] Loss: 0.150763
Train Epoch: 7 [16800/17010 (99%)] Loss: 0.132313
Train Epoch: 7 [16960/17010 (100%)] Loss: 0.471762
    epoch          : 7
    Train_loss     : 0.28061390343241227
    Train_accuracy : 0.9023731203007519
    Train_top_k_acc: 0.9958881578947368
    Val_loss       : 0.28626463084171216
    Val_accuracy   : 0.903125
    Val_top_k_acc  : 0.9942708333333333
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch7.pth ...
Train Epoch: 8 [0/17010 (0%)] Loss: 0.415238
Train Epoch: 8 [160/17010 (1%)] Loss: 0.233269
Train Epoch: 8 [320/17010 (2%)] Loss: 0.310375
Train Epoch: 8 [480/17010 (3%)] Loss: 0.165093
Train Epoch: 8 [640/17010 (4%)] Loss: 0.206311
Train Epoch: 8 [800/17010 (5%)] Loss: 0.231010
Train Epoch: 8 [960/17010 (6%)] Loss: 0.185543
Train Epoch: 8 [1120/17010 (7%)] Loss: 0.235341
Train Epoch: 8 [1280/17010 (8%)] Loss: 0.178712
Train Epoch: 8 [1440/17010 (8%)] Loss: 0.146297
Train Epoch: 8 [1600/17010 (9%)] Loss: 0.120679
Train Epoch: 8 [1760/17010 (10%)] Loss: 0.210180
Train Epoch: 8 [1920/17010 (11%)] Loss: 0.353169
Train Epoch: 8 [2080/17010 (12%)] Loss: 0.123357
Train Epoch: 8 [2240/17010 (13%)] Loss: 0.163919
Train Epoch: 8 [2400/17010 (14%)] Loss: 0.324235
Train Epoch: 8 [2560/17010 (15%)] Loss: 0.349495
Train Epoch: 8 [2720/17010 (16%)] Loss: 0.325953
Train Epoch: 8 [2880/17010 (17%)] Loss: 0.487779
Train Epoch: 8 [3040/17010 (18%)] Loss: 0.315138
Train Epoch: 8 [3200/17010 (19%)] Loss: 0.173124
Train Epoch: 8 [3360/17010 (20%)] Loss: 0.394802
Train Epoch: 8 [3520/17010 (21%)] Loss: 0.196405
Train Epoch: 8 [3680/17010 (22%)] Loss: 0.169144
Train Epoch: 8 [3840/17010 (23%)] Loss: 0.149757
Train Epoch: 8 [4000/17010 (24%)] Loss: 0.354401
Train Epoch: 8 [4160/17010 (24%)] Loss: 0.135625
Train Epoch: 8 [4320/17010 (25%)] Loss: 0.693721
Train Epoch: 8 [4480/17010 (26%)] Loss: 0.560668
Train Epoch: 8 [4640/17010 (27%)] Loss: 0.294870
Train Epoch: 8 [4800/17010 (28%)] Loss: 0.258114
Train Epoch: 8 [4960/17010 (29%)] Loss: 0.780107
Train Epoch: 8 [5120/17010 (30%)] Loss: 0.238561
Train Epoch: 8 [5280/17010 (31%)] Loss: 0.170234
Train Epoch: 8 [5440/17010 (32%)] Loss: 0.120412
Train Epoch: 8 [5600/17010 (33%)] Loss: 0.230265
Train Epoch: 8 [5760/17010 (34%)] Loss: 0.250271
Train Epoch: 8 [5920/17010 (35%)] Loss: 0.151295
Train Epoch: 8 [6080/17010 (36%)] Loss: 0.120417
Train Epoch: 8 [6240/17010 (37%)] Loss: 0.038474
Train Epoch: 8 [6400/17010 (38%)] Loss: 0.348753
Train Epoch: 8 [6560/17010 (39%)] Loss: 0.086597
Train Epoch: 8 [6720/17010 (40%)] Loss: 0.131152
Train Epoch: 8 [6880/17010 (40%)] Loss: 0.117949
Train Epoch: 8 [7040/17010 (41%)] Loss: 0.081000
Train Epoch: 8 [7200/17010 (42%)] Loss: 0.106101
Train Epoch: 8 [7360/17010 (43%)] Loss: 0.057515
Train Epoch: 8 [7520/17010 (44%)] Loss: 0.609057
Train Epoch: 8 [7680/17010 (45%)] Loss: 0.162689
Train Epoch: 8 [7840/17010 (46%)] Loss: 0.242064
Train Epoch: 8 [8000/17010 (47%)] Loss: 0.155389
Train Epoch: 8 [8160/17010 (48%)] Loss: 0.314996
Train Epoch: 8 [8320/17010 (49%)] Loss: 0.115849
Train Epoch: 8 [8480/17010 (50%)] Loss: 0.191195
Train Epoch: 8 [8640/17010 (51%)] Loss: 0.078994
Train Epoch: 8 [8800/17010 (52%)] Loss: 0.097593
Train Epoch: 8 [8960/17010 (53%)] Loss: 0.150794
Train Epoch: 8 [9120/17010 (54%)] Loss: 0.494454
Train Epoch: 8 [9280/17010 (55%)] Loss: 0.410039
Train Epoch: 8 [9440/17010 (55%)] Loss: 0.363123
Train Epoch: 8 [9600/17010 (56%)] Loss: 0.529004
Train Epoch: 8 [9760/17010 (57%)] Loss: 0.348359
Train Epoch: 8 [9920/17010 (58%)] Loss: 0.307265
Train Epoch: 8 [10080/17010 (59%)] Loss: 0.303441
Train Epoch: 8 [10240/17010 (60%)] Loss: 0.248166
Train Epoch: 8 [10400/17010 (61%)] Loss: 0.140566
Train Epoch: 8 [10560/17010 (62%)] Loss: 0.238210
Train Epoch: 8 [10720/17010 (63%)] Loss: 0.276793
Train Epoch: 8 [10880/17010 (64%)] Loss: 0.107325
Train Epoch: 8 [11040/17010 (65%)] Loss: 0.351638
Train Epoch: 8 [11200/17010 (66%)] Loss: 0.316603
Train Epoch: 8 [11360/17010 (67%)] Loss: 0.406317
Train Epoch: 8 [11520/17010 (68%)] Loss: 0.433428
Train Epoch: 8 [11680/17010 (69%)] Loss: 0.259606
Train Epoch: 8 [11840/17010 (70%)] Loss: 0.139870
Train Epoch: 8 [12000/17010 (71%)] Loss: 0.123132
Train Epoch: 8 [12160/17010 (71%)] Loss: 0.205684
Train Epoch: 8 [12320/17010 (72%)] Loss: 0.241759
Train Epoch: 8 [12480/17010 (73%)] Loss: 0.207870
Train Epoch: 8 [12640/17010 (74%)] Loss: 0.197084
Train Epoch: 8 [12800/17010 (75%)] Loss: 0.335987
Train Epoch: 8 [12960/17010 (76%)] Loss: 0.128184
Train Epoch: 8 [13120/17010 (77%)] Loss: 0.247262
Train Epoch: 8 [13280/17010 (78%)] Loss: 0.155142
Train Epoch: 8 [13440/17010 (79%)] Loss: 0.405811
Train Epoch: 8 [13600/17010 (80%)] Loss: 0.215151
Train Epoch: 8 [13760/17010 (81%)] Loss: 0.279495
Train Epoch: 8 [13920/17010 (82%)] Loss: 0.195436
Train Epoch: 8 [14080/17010 (83%)] Loss: 0.264055
Train Epoch: 8 [14240/17010 (84%)] Loss: 0.207810
Train Epoch: 8 [14400/17010 (85%)] Loss: 0.158454
Train Epoch: 8 [14560/17010 (86%)] Loss: 0.167723
Train Epoch: 8 [14720/17010 (87%)] Loss: 0.320726
Train Epoch: 8 [14880/17010 (87%)] Loss: 0.278487
Train Epoch: 8 [15040/17010 (88%)] Loss: 0.246922
Train Epoch: 8 [15200/17010 (89%)] Loss: 0.114697
Train Epoch: 8 [15360/17010 (90%)] Loss: 0.282578
Train Epoch: 8 [15520/17010 (91%)] Loss: 0.135667
Train Epoch: 8 [15680/17010 (92%)] Loss: 0.239088
Train Epoch: 8 [15840/17010 (93%)] Loss: 0.210750
Train Epoch: 8 [16000/17010 (94%)] Loss: 0.249434
Train Epoch: 8 [16160/17010 (95%)] Loss: 0.350466
Train Epoch: 8 [16320/17010 (96%)] Loss: 0.238322
Train Epoch: 8 [16480/17010 (97%)] Loss: 0.180770
Train Epoch: 8 [16640/17010 (98%)] Loss: 0.056748
Train Epoch: 8 [16800/17010 (99%)] Loss: 0.431119
Train Epoch: 8 [16960/17010 (100%)] Loss: 0.655759
    epoch          : 8
    Train_loss     : 0.25000673586054517
    Train_accuracy : 0.9131291771094403
    Train_top_k_acc: 0.9961231203007519
    Val_loss       : 0.5778275062640508
    Val_accuracy   : 0.834375
    Val_top_k_acc  : 0.9796875
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch8.pth ...
Train Epoch: 9 [0/17010 (0%)] Loss: 0.172714
Train Epoch: 9 [160/17010 (1%)] Loss: 0.253599
Train Epoch: 9 [320/17010 (2%)] Loss: 0.313910
Train Epoch: 9 [480/17010 (3%)] Loss: 0.390051
Train Epoch: 9 [640/17010 (4%)] Loss: 0.192439
Train Epoch: 9 [800/17010 (5%)] Loss: 0.099404
Train Epoch: 9 [960/17010 (6%)] Loss: 0.078422
Train Epoch: 9 [1120/17010 (7%)] Loss: 0.362160
Train Epoch: 9 [1280/17010 (8%)] Loss: 0.112153
Train Epoch: 9 [1440/17010 (8%)] Loss: 0.237066
Train Epoch: 9 [1600/17010 (9%)] Loss: 0.147285
Train Epoch: 9 [1760/17010 (10%)] Loss: 0.055477
Train Epoch: 9 [1920/17010 (11%)] Loss: 0.280151
Train Epoch: 9 [2080/17010 (12%)] Loss: 0.147466
Train Epoch: 9 [2240/17010 (13%)] Loss: 0.524499
Train Epoch: 9 [2400/17010 (14%)] Loss: 0.262083
Train Epoch: 9 [2560/17010 (15%)] Loss: 0.141779
Train Epoch: 9 [2720/17010 (16%)] Loss: 0.075966
Train Epoch: 9 [2880/17010 (17%)] Loss: 0.144528
Train Epoch: 9 [3040/17010 (18%)] Loss: 0.229905
Train Epoch: 9 [3200/17010 (19%)] Loss: 0.363268
Train Epoch: 9 [3360/17010 (20%)] Loss: 0.089370
Train Epoch: 9 [3520/17010 (21%)] Loss: 0.093688
Train Epoch: 9 [3680/17010 (22%)] Loss: 0.262305
Train Epoch: 9 [3840/17010 (23%)] Loss: 0.230143
Train Epoch: 9 [4000/17010 (24%)] Loss: 0.361876
Train Epoch: 9 [4160/17010 (24%)] Loss: 0.284910
Train Epoch: 9 [4320/17010 (25%)] Loss: 0.237518
Train Epoch: 9 [4480/17010 (26%)] Loss: 0.159162
Train Epoch: 9 [4640/17010 (27%)] Loss: 0.117557
Train Epoch: 9 [4800/17010 (28%)] Loss: 0.120870
Train Epoch: 9 [4960/17010 (29%)] Loss: 0.453493
Train Epoch: 9 [5120/17010 (30%)] Loss: 0.211814
Train Epoch: 9 [5280/17010 (31%)] Loss: 0.173154
Train Epoch: 9 [5440/17010 (32%)] Loss: 0.296297
Train Epoch: 9 [5600/17010 (33%)] Loss: 0.078635
Train Epoch: 9 [5760/17010 (34%)] Loss: 0.169378
Train Epoch: 9 [5920/17010 (35%)] Loss: 0.082639
Train Epoch: 9 [6080/17010 (36%)] Loss: 0.124534
Train Epoch: 9 [6240/17010 (37%)] Loss: 0.041047
Train Epoch: 9 [6400/17010 (38%)] Loss: 0.167193
Train Epoch: 9 [6560/17010 (39%)] Loss: 0.289634
Train Epoch: 9 [6720/17010 (40%)] Loss: 0.527235
Train Epoch: 9 [6880/17010 (40%)] Loss: 0.145276
Train Epoch: 9 [7040/17010 (41%)] Loss: 0.096933
Train Epoch: 9 [7200/17010 (42%)] Loss: 0.374825
Train Epoch: 9 [7360/17010 (43%)] Loss: 0.316915
Train Epoch: 9 [7520/17010 (44%)] Loss: 0.150628
Train Epoch: 9 [7680/17010 (45%)] Loss: 0.125107
Train Epoch: 9 [7840/17010 (46%)] Loss: 0.353880
Train Epoch: 9 [8000/17010 (47%)] Loss: 0.390325
Train Epoch: 9 [8160/17010 (48%)] Loss: 0.489288
Train Epoch: 9 [8320/17010 (49%)] Loss: 0.123995
Train Epoch: 9 [8480/17010 (50%)] Loss: 0.066725
Train Epoch: 9 [8640/17010 (51%)] Loss: 0.186975
Train Epoch: 9 [8800/17010 (52%)] Loss: 0.147969
Train Epoch: 9 [8960/17010 (53%)] Loss: 0.159031
Train Epoch: 9 [9120/17010 (54%)] Loss: 0.266191
Train Epoch: 9 [9280/17010 (55%)] Loss: 0.065223
Train Epoch: 9 [9440/17010 (55%)] Loss: 0.063108
Train Epoch: 9 [9600/17010 (56%)] Loss: 0.758693
Train Epoch: 9 [9760/17010 (57%)] Loss: 0.140973
Train Epoch: 9 [9920/17010 (58%)] Loss: 0.084904
Train Epoch: 9 [10080/17010 (59%)] Loss: 0.307459
Train Epoch: 9 [10240/17010 (60%)] Loss: 0.112401
Train Epoch: 9 [10400/17010 (61%)] Loss: 0.282585
Train Epoch: 9 [10560/17010 (62%)] Loss: 0.077096
Train Epoch: 9 [10720/17010 (63%)] Loss: 0.344076
Train Epoch: 9 [10880/17010 (64%)] Loss: 0.521187
Train Epoch: 9 [11040/17010 (65%)] Loss: 0.087704
Train Epoch: 9 [11200/17010 (66%)] Loss: 0.325305
Train Epoch: 9 [11360/17010 (67%)] Loss: 0.331333
Train Epoch: 9 [11520/17010 (68%)] Loss: 0.208324
Train Epoch: 9 [11680/17010 (69%)] Loss: 0.143519
Train Epoch: 9 [11840/17010 (70%)] Loss: 0.249555
Train Epoch: 9 [12000/17010 (71%)] Loss: 0.263673
Train Epoch: 9 [12160/17010 (71%)] Loss: 0.128879
Train Epoch: 9 [12320/17010 (72%)] Loss: 0.294622
Train Epoch: 9 [12480/17010 (73%)] Loss: 0.134960
Train Epoch: 9 [12640/17010 (74%)] Loss: 0.215978
Train Epoch: 9 [12800/17010 (75%)] Loss: 0.173103
Train Epoch: 9 [12960/17010 (76%)] Loss: 0.210092
Train Epoch: 9 [13120/17010 (77%)] Loss: 0.369310
Train Epoch: 9 [13280/17010 (78%)] Loss: 0.094947
Train Epoch: 9 [13440/17010 (79%)] Loss: 0.227293
Train Epoch: 9 [13600/17010 (80%)] Loss: 0.186118
Train Epoch: 9 [13760/17010 (81%)] Loss: 0.315043
Train Epoch: 9 [13920/17010 (82%)] Loss: 0.223349
Train Epoch: 9 [14080/17010 (83%)] Loss: 0.297604
Train Epoch: 9 [14240/17010 (84%)] Loss: 0.168428
Train Epoch: 9 [14400/17010 (85%)] Loss: 0.309120
Train Epoch: 9 [14560/17010 (86%)] Loss: 0.204477
Train Epoch: 9 [14720/17010 (87%)] Loss: 0.140512
Train Epoch: 9 [14880/17010 (87%)] Loss: 0.134820
Train Epoch: 9 [15040/17010 (88%)] Loss: 0.065511
Train Epoch: 9 [15200/17010 (89%)] Loss: 0.154905
Train Epoch: 9 [15360/17010 (90%)] Loss: 0.167366
Train Epoch: 9 [15520/17010 (91%)] Loss: 0.076463
Train Epoch: 9 [15680/17010 (92%)] Loss: 0.543927
Train Epoch: 9 [15840/17010 (93%)] Loss: 0.079962
Train Epoch: 9 [16000/17010 (94%)] Loss: 0.073082
Train Epoch: 9 [16160/17010 (95%)] Loss: 0.119555
Train Epoch: 9 [16320/17010 (96%)] Loss: 0.139007
Train Epoch: 9 [16480/17010 (97%)] Loss: 0.249915
Train Epoch: 9 [16640/17010 (98%)] Loss: 0.141307
Train Epoch: 9 [16800/17010 (99%)] Loss: 0.067940
Train Epoch: 9 [16960/17010 (100%)] Loss: 0.116677
    epoch          : 9
    Train_loss     : 0.21335846310677498
    Train_accuracy : 0.9260586361737678
    Train_top_k_acc: 0.997062969924812
    Val_loss       : 0.3551939404259125
    Val_accuracy   : 0.8619791666666666
    Val_top_k_acc  : 0.9947916666666666
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch9.pth ...
Train Epoch: 10 [0/17010 (0%)] Loss: 0.346179
Train Epoch: 10 [160/17010 (1%)] Loss: 0.102522
Train Epoch: 10 [320/17010 (2%)] Loss: 0.203311
Train Epoch: 10 [480/17010 (3%)] Loss: 0.116929
Train Epoch: 10 [640/17010 (4%)] Loss: 0.112323
Train Epoch: 10 [800/17010 (5%)] Loss: 0.153080
Train Epoch: 10 [960/17010 (6%)] Loss: 0.200046
Train Epoch: 10 [1120/17010 (7%)] Loss: 0.267626
Train Epoch: 10 [1280/17010 (8%)] Loss: 0.095442
Train Epoch: 10 [1440/17010 (8%)] Loss: 0.269860
Train Epoch: 10 [1600/17010 (9%)] Loss: 0.087842
Train Epoch: 10 [1760/17010 (10%)] Loss: 0.075179
Train Epoch: 10 [1920/17010 (11%)] Loss: 0.060141
Train Epoch: 10 [2080/17010 (12%)] Loss: 0.086589
Train Epoch: 10 [2240/17010 (13%)] Loss: 0.288497
Train Epoch: 10 [2400/17010 (14%)] Loss: 0.104848
Train Epoch: 10 [2560/17010 (15%)] Loss: 0.339008
Train Epoch: 10 [2720/17010 (16%)] Loss: 0.086536
Train Epoch: 10 [2880/17010 (17%)] Loss: 0.102422
Train Epoch: 10 [3040/17010 (18%)] Loss: 0.185773
Train Epoch: 10 [3200/17010 (19%)] Loss: 0.272662
Train Epoch: 10 [3360/17010 (20%)] Loss: 0.317609
Train Epoch: 10 [3520/17010 (21%)] Loss: 0.144610
Train Epoch: 10 [3680/17010 (22%)] Loss: 0.049481
Train Epoch: 10 [3840/17010 (23%)] Loss: 0.405260
Train Epoch: 10 [4000/17010 (24%)] Loss: 0.146692
Train Epoch: 10 [4160/17010 (24%)] Loss: 0.077111
Train Epoch: 10 [4320/17010 (25%)] Loss: 0.334100
Train Epoch: 10 [4480/17010 (26%)] Loss: 0.095403
Train Epoch: 10 [4640/17010 (27%)] Loss: 0.137527
Train Epoch: 10 [4800/17010 (28%)] Loss: 0.072455
Train Epoch: 10 [4960/17010 (29%)] Loss: 0.066609
Train Epoch: 10 [5120/17010 (30%)] Loss: 0.055653
Train Epoch: 10 [5280/17010 (31%)] Loss: 0.097448
Train Epoch: 10 [5440/17010 (32%)] Loss: 0.213291
Train Epoch: 10 [5600/17010 (33%)] Loss: 0.086504
Train Epoch: 10 [5760/17010 (34%)] Loss: 0.052240
Train Epoch: 10 [5920/17010 (35%)] Loss: 0.072527
Train Epoch: 10 [6080/17010 (36%)] Loss: 0.124074
Train Epoch: 10 [6240/17010 (37%)] Loss: 0.055996
Train Epoch: 10 [6400/17010 (38%)] Loss: 0.069972
Train Epoch: 10 [6560/17010 (39%)] Loss: 0.135649
Train Epoch: 10 [6720/17010 (40%)] Loss: 0.096992
Train Epoch: 10 [6880/17010 (40%)] Loss: 0.072491
Train Epoch: 10 [7040/17010 (41%)] Loss: 0.119214
Train Epoch: 10 [7200/17010 (42%)] Loss: 0.128555
Train Epoch: 10 [7360/17010 (43%)] Loss: 0.242620
Train Epoch: 10 [7520/17010 (44%)] Loss: 0.058598
Train Epoch: 10 [7680/17010 (45%)] Loss: 0.222951
Train Epoch: 10 [7840/17010 (46%)] Loss: 0.074862
Train Epoch: 10 [8000/17010 (47%)] Loss: 0.082890
Train Epoch: 10 [8160/17010 (48%)] Loss: 0.106391
Train Epoch: 10 [8320/17010 (49%)] Loss: 0.076967
Train Epoch: 10 [8480/17010 (50%)] Loss: 0.169827
Train Epoch: 10 [8640/17010 (51%)] Loss: 0.161855
Train Epoch: 10 [8800/17010 (52%)] Loss: 0.035996
Train Epoch: 10 [8960/17010 (53%)] Loss: 0.146713
Train Epoch: 10 [9120/17010 (54%)] Loss: 0.099403
Train Epoch: 10 [9280/17010 (55%)] Loss: 0.038464
Train Epoch: 10 [9440/17010 (55%)] Loss: 0.128701
Train Epoch: 10 [9600/17010 (56%)] Loss: 0.032233
Train Epoch: 10 [9760/17010 (57%)] Loss: 0.204882
Train Epoch: 10 [9920/17010 (58%)] Loss: 0.178356
Train Epoch: 10 [10080/17010 (59%)] Loss: 0.219856
Train Epoch: 10 [10240/17010 (60%)] Loss: 0.189746
Train Epoch: 10 [10400/17010 (61%)] Loss: 0.209609
Train Epoch: 10 [10560/17010 (62%)] Loss: 0.062828
Train Epoch: 10 [10720/17010 (63%)] Loss: 0.037798
Train Epoch: 10 [10880/17010 (64%)] Loss: 0.325057
Train Epoch: 10 [11040/17010 (65%)] Loss: 0.157069
Train Epoch: 10 [11200/17010 (66%)] Loss: 0.160291
Train Epoch: 10 [11360/17010 (67%)] Loss: 0.094312
Train Epoch: 10 [11520/17010 (68%)] Loss: 0.199085
Train Epoch: 10 [11680/17010 (69%)] Loss: 0.241533
Train Epoch: 10 [11840/17010 (70%)] Loss: 0.120371
Train Epoch: 10 [12000/17010 (71%)] Loss: 0.072554
Train Epoch: 10 [12160/17010 (71%)] Loss: 0.308628
Train Epoch: 10 [12320/17010 (72%)] Loss: 0.064610
Train Epoch: 10 [12480/17010 (73%)] Loss: 0.437518
Train Epoch: 10 [12640/17010 (74%)] Loss: 0.141654
Train Epoch: 10 [12800/17010 (75%)] Loss: 0.098310
Train Epoch: 10 [12960/17010 (76%)] Loss: 0.107097
Train Epoch: 10 [13120/17010 (77%)] Loss: 0.120287
Train Epoch: 10 [13280/17010 (78%)] Loss: 0.123382
Train Epoch: 10 [13440/17010 (79%)] Loss: 0.108577
Train Epoch: 10 [13600/17010 (80%)] Loss: 0.118232
Train Epoch: 10 [13760/17010 (81%)] Loss: 0.288996
Train Epoch: 10 [13920/17010 (82%)] Loss: 0.144331
Train Epoch: 10 [14080/17010 (83%)] Loss: 0.359486
Train Epoch: 10 [14240/17010 (84%)] Loss: 0.036925
Train Epoch: 10 [14400/17010 (85%)] Loss: 0.175837
Train Epoch: 10 [14560/17010 (86%)] Loss: 0.216934
Train Epoch: 10 [14720/17010 (87%)] Loss: 0.358948
Train Epoch: 10 [14880/17010 (87%)] Loss: 0.546501
Train Epoch: 10 [15040/17010 (88%)] Loss: 0.367635
Train Epoch: 10 [15200/17010 (89%)] Loss: 0.144718
Train Epoch: 10 [15360/17010 (90%)] Loss: 0.145676
Train Epoch: 10 [15520/17010 (91%)] Loss: 0.185007
Train Epoch: 10 [15680/17010 (92%)] Loss: 0.537008
Train Epoch: 10 [15840/17010 (93%)] Loss: 0.137895
Train Epoch: 10 [16000/17010 (94%)] Loss: 0.227366
Train Epoch: 10 [16160/17010 (95%)] Loss: 0.159214
Train Epoch: 10 [16320/17010 (96%)] Loss: 0.064263
Train Epoch: 10 [16480/17010 (97%)] Loss: 0.060447
Train Epoch: 10 [16640/17010 (98%)] Loss: 0.102142
Train Epoch: 10 [16800/17010 (99%)] Loss: 0.369868
Train Epoch: 10 [16960/17010 (100%)] Loss: 0.133256
    epoch          : 10
    Train_loss     : 0.17593945542182354
    Train_accuracy : 0.9398169903926483
    Train_top_k_acc: 0.9977678571428571
    Val_loss       : 0.262325332313776
    Val_accuracy   : 0.91875
    Val_top_k_acc  : 0.9947916666666666
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch10.pth ...
Train Epoch: 11 [0/17010 (0%)] Loss: 0.192184
Train Epoch: 11 [160/17010 (1%)] Loss: 0.223937
Train Epoch: 11 [320/17010 (2%)] Loss: 0.170010
Train Epoch: 11 [480/17010 (3%)] Loss: 0.059762
Train Epoch: 11 [640/17010 (4%)] Loss: 0.040217
Train Epoch: 11 [800/17010 (5%)] Loss: 0.073934
Train Epoch: 11 [960/17010 (6%)] Loss: 0.179911
Train Epoch: 11 [1120/17010 (7%)] Loss: 0.212053
Train Epoch: 11 [1280/17010 (8%)] Loss: 0.231996
Train Epoch: 11 [1440/17010 (8%)] Loss: 0.115452
Train Epoch: 11 [1600/17010 (9%)] Loss: 0.079767
Train Epoch: 11 [1760/17010 (10%)] Loss: 0.284081
Train Epoch: 11 [1920/17010 (11%)] Loss: 0.228132
Train Epoch: 11 [2080/17010 (12%)] Loss: 0.170231
Train Epoch: 11 [2240/17010 (13%)] Loss: 0.395958
Train Epoch: 11 [2400/17010 (14%)] Loss: 0.143982
Train Epoch: 11 [2560/17010 (15%)] Loss: 0.133598
Train Epoch: 11 [2720/17010 (16%)] Loss: 0.068350
Train Epoch: 11 [2880/17010 (17%)] Loss: 0.032380
Train Epoch: 11 [3040/17010 (18%)] Loss: 0.038509
Train Epoch: 11 [3200/17010 (19%)] Loss: 0.434516
Train Epoch: 11 [3360/17010 (20%)] Loss: 0.210472
Train Epoch: 11 [3520/17010 (21%)] Loss: 0.240903
Train Epoch: 11 [3680/17010 (22%)] Loss: 0.284023
Train Epoch: 11 [3840/17010 (23%)] Loss: 0.147408
Train Epoch: 11 [4000/17010 (24%)] Loss: 0.246377
Train Epoch: 11 [4160/17010 (24%)] Loss: 0.194055
Train Epoch: 11 [4320/17010 (25%)] Loss: 0.146331
Train Epoch: 11 [4480/17010 (26%)] Loss: 0.124838
Train Epoch: 11 [4640/17010 (27%)] Loss: 0.104892
Train Epoch: 11 [4800/17010 (28%)] Loss: 0.305099
Train Epoch: 11 [4960/17010 (29%)] Loss: 0.326630
Train Epoch: 11 [5120/17010 (30%)] Loss: 0.036727
Train Epoch: 11 [5280/17010 (31%)] Loss: 0.044320
Train Epoch: 11 [5440/17010 (32%)] Loss: 0.146343
Train Epoch: 11 [5600/17010 (33%)] Loss: 0.139455
Train Epoch: 11 [5760/17010 (34%)] Loss: 0.252409
Train Epoch: 11 [5920/17010 (35%)] Loss: 0.189301
Train Epoch: 11 [6080/17010 (36%)] Loss: 0.131840
Train Epoch: 11 [6240/17010 (37%)] Loss: 0.043707
Train Epoch: 11 [6400/17010 (38%)] Loss: 0.122832
Train Epoch: 11 [6560/17010 (39%)] Loss: 0.085901
Train Epoch: 11 [6720/17010 (40%)] Loss: 0.285002
Train Epoch: 11 [6880/17010 (40%)] Loss: 0.122956
Train Epoch: 11 [7040/17010 (41%)] Loss: 0.390537
Train Epoch: 11 [7200/17010 (42%)] Loss: 0.101597
Train Epoch: 11 [7360/17010 (43%)] Loss: 0.051433
Train Epoch: 11 [7520/17010 (44%)] Loss: 0.220610
Train Epoch: 11 [7680/17010 (45%)] Loss: 0.077371
Train Epoch: 11 [7840/17010 (46%)] Loss: 0.143422
Train Epoch: 11 [8000/17010 (47%)] Loss: 0.068658
Train Epoch: 11 [8160/17010 (48%)] Loss: 0.050052
Train Epoch: 11 [8320/17010 (49%)] Loss: 0.243860
Train Epoch: 11 [8480/17010 (50%)] Loss: 0.060521
Train Epoch: 11 [8640/17010 (51%)] Loss: 0.120255
Train Epoch: 11 [8800/17010 (52%)] Loss: 0.101217
Train Epoch: 11 [8960/17010 (53%)] Loss: 0.036490
Train Epoch: 11 [9120/17010 (54%)] Loss: 0.242002
Train Epoch: 11 [9280/17010 (55%)] Loss: 0.029764
Train Epoch: 11 [9440/17010 (55%)] Loss: 0.082669
Train Epoch: 11 [9600/17010 (56%)] Loss: 0.185026
Train Epoch: 11 [9760/17010 (57%)] Loss: 0.119957
Train Epoch: 11 [9920/17010 (58%)] Loss: 0.172836
Train Epoch: 11 [10080/17010 (59%)] Loss: 0.097314
Train Epoch: 11 [10240/17010 (60%)] Loss: 0.257624
Train Epoch: 11 [10400/17010 (61%)] Loss: 0.051193
Train Epoch: 11 [10560/17010 (62%)] Loss: 0.119814
Train Epoch: 11 [10720/17010 (63%)] Loss: 0.251357
Train Epoch: 11 [10880/17010 (64%)] Loss: 0.174986
Train Epoch: 11 [11040/17010 (65%)] Loss: 0.105187
Train Epoch: 11 [11200/17010 (66%)] Loss: 0.142011
Train Epoch: 11 [11360/17010 (67%)] Loss: 0.021045
Train Epoch: 11 [11520/17010 (68%)] Loss: 0.283673
Train Epoch: 11 [11680/17010 (69%)] Loss: 0.171969
Train Epoch: 11 [11840/17010 (70%)] Loss: 0.050059
Train Epoch: 11 [12000/17010 (71%)] Loss: 0.076818
Train Epoch: 11 [12160/17010 (71%)] Loss: 0.102322
Train Epoch: 11 [12320/17010 (72%)] Loss: 0.067001
Train Epoch: 11 [12480/17010 (73%)] Loss: 0.149937
Train Epoch: 11 [12640/17010 (74%)] Loss: 0.043693
Train Epoch: 11 [12800/17010 (75%)] Loss: 0.135036
Train Epoch: 11 [12960/17010 (76%)] Loss: 0.198109
Train Epoch: 11 [13120/17010 (77%)] Loss: 0.225798
Train Epoch: 11 [13280/17010 (78%)] Loss: 0.150209
Train Epoch: 11 [13440/17010 (79%)] Loss: 0.094998
Train Epoch: 11 [13600/17010 (80%)] Loss: 0.129572
Train Epoch: 11 [13760/17010 (81%)] Loss: 0.228354
Train Epoch: 11 [13920/17010 (82%)] Loss: 0.155826
Train Epoch: 11 [14080/17010 (83%)] Loss: 0.197542
Train Epoch: 11 [14240/17010 (84%)] Loss: 0.149938
Train Epoch: 11 [14400/17010 (85%)] Loss: 0.136213
Train Epoch: 11 [14560/17010 (86%)] Loss: 0.337931
Train Epoch: 11 [14720/17010 (87%)] Loss: 0.136965
Train Epoch: 11 [14880/17010 (87%)] Loss: 0.055100
Train Epoch: 11 [15040/17010 (88%)] Loss: 0.246100
Train Epoch: 11 [15200/17010 (89%)] Loss: 0.284901
Train Epoch: 11 [15360/17010 (90%)] Loss: 0.053833
Train Epoch: 11 [15520/17010 (91%)] Loss: 0.137093
Train Epoch: 11 [15680/17010 (92%)] Loss: 0.054525
Train Epoch: 11 [15840/17010 (93%)] Loss: 0.091467
Train Epoch: 11 [16000/17010 (94%)] Loss: 0.054800
Train Epoch: 11 [16160/17010 (95%)] Loss: 0.391953
Train Epoch: 11 [16320/17010 (96%)] Loss: 0.260534
Train Epoch: 11 [16480/17010 (97%)] Loss: 0.250705
Train Epoch: 11 [16640/17010 (98%)] Loss: 0.557953
Train Epoch: 11 [16800/17010 (99%)] Loss: 0.166905
Train Epoch: 11 [16960/17010 (100%)] Loss: 0.120978
    epoch          : 11
    Train_loss     : 0.1658291607330154
    Train_accuracy : 0.9434001670843777
    Train_top_k_acc: 0.9988251879699248
    Val_loss       : 0.4055680663635333
    Val_accuracy   : 0.8828125
    Val_top_k_acc  : 0.9859375
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch11.pth ...
Train Epoch: 12 [0/17010 (0%)] Loss: 0.045099
Train Epoch: 12 [160/17010 (1%)] Loss: 0.040157
Train Epoch: 12 [320/17010 (2%)] Loss: 0.119708
Train Epoch: 12 [480/17010 (3%)] Loss: 0.096777
Train Epoch: 12 [640/17010 (4%)] Loss: 0.189494
Train Epoch: 12 [800/17010 (5%)] Loss: 0.084595
Train Epoch: 12 [960/17010 (6%)] Loss: 0.108372
Train Epoch: 12 [1120/17010 (7%)] Loss: 0.204303
Train Epoch: 12 [1280/17010 (8%)] Loss: 0.153231
Train Epoch: 12 [1440/17010 (8%)] Loss: 0.110016
Train Epoch: 12 [1600/17010 (9%)] Loss: 0.220335
Train Epoch: 12 [1760/17010 (10%)] Loss: 0.068253
Train Epoch: 12 [1920/17010 (11%)] Loss: 0.030291
Train Epoch: 12 [2080/17010 (12%)] Loss: 0.157358
Train Epoch: 12 [2240/17010 (13%)] Loss: 0.172164
Train Epoch: 12 [2400/17010 (14%)] Loss: 0.212830
Train Epoch: 12 [2560/17010 (15%)] Loss: 0.080531
Train Epoch: 12 [2720/17010 (16%)] Loss: 0.074470
Train Epoch: 12 [2880/17010 (17%)] Loss: 0.127842
Train Epoch: 12 [3040/17010 (18%)] Loss: 0.094274
Train Epoch: 12 [3200/17010 (19%)] Loss: 0.038093
Train Epoch: 12 [3360/17010 (20%)] Loss: 0.127305
Train Epoch: 12 [3520/17010 (21%)] Loss: 0.033598
Train Epoch: 12 [3680/17010 (22%)] Loss: 0.251984
Train Epoch: 12 [3840/17010 (23%)] Loss: 0.025165
Train Epoch: 12 [4000/17010 (24%)] Loss: 0.163151
Train Epoch: 12 [4160/17010 (24%)] Loss: 0.289742
Train Epoch: 12 [4320/17010 (25%)] Loss: 0.024524
Train Epoch: 12 [4480/17010 (26%)] Loss: 0.107057
Train Epoch: 12 [4640/17010 (27%)] Loss: 0.299998
Train Epoch: 12 [4800/17010 (28%)] Loss: 0.052189
Train Epoch: 12 [4960/17010 (29%)] Loss: 0.203549
Train Epoch: 12 [5120/17010 (30%)] Loss: 0.136703
Train Epoch: 12 [5280/17010 (31%)] Loss: 0.066219
Train Epoch: 12 [5440/17010 (32%)] Loss: 0.180465
Train Epoch: 12 [5600/17010 (33%)] Loss: 0.148739
Train Epoch: 12 [5760/17010 (34%)] Loss: 0.235128
Train Epoch: 12 [5920/17010 (35%)] Loss: 0.091573
Train Epoch: 12 [6080/17010 (36%)] Loss: 0.134418
Train Epoch: 12 [6240/17010 (37%)] Loss: 0.233458
Train Epoch: 12 [6400/17010 (38%)] Loss: 0.064704
Train Epoch: 12 [6560/17010 (39%)] Loss: 0.165938
Train Epoch: 12 [6720/17010 (40%)] Loss: 0.048150
Train Epoch: 12 [6880/17010 (40%)] Loss: 0.088321
Train Epoch: 12 [7040/17010 (41%)] Loss: 0.262686
Train Epoch: 12 [7200/17010 (42%)] Loss: 0.070116
Train Epoch: 12 [7360/17010 (43%)] Loss: 0.036494
Train Epoch: 12 [7520/17010 (44%)] Loss: 0.131210
Train Epoch: 12 [7680/17010 (45%)] Loss: 0.059658
Train Epoch: 12 [7840/17010 (46%)] Loss: 0.027829
Train Epoch: 12 [8000/17010 (47%)] Loss: 0.410928
Train Epoch: 12 [8160/17010 (48%)] Loss: 0.411008
Train Epoch: 12 [8320/17010 (49%)] Loss: 0.066201
Train Epoch: 12 [8480/17010 (50%)] Loss: 0.058712
Train Epoch: 12 [8640/17010 (51%)] Loss: 0.256016
Train Epoch: 12 [8800/17010 (52%)] Loss: 0.109652
Train Epoch: 12 [8960/17010 (53%)] Loss: 0.036963
Train Epoch: 12 [9120/17010 (54%)] Loss: 0.194330
Train Epoch: 12 [9280/17010 (55%)] Loss: 0.208513
Train Epoch: 12 [9440/17010 (55%)] Loss: 0.048974
Train Epoch: 12 [9600/17010 (56%)] Loss: 0.165890
Train Epoch: 12 [9760/17010 (57%)] Loss: 0.196652
Train Epoch: 12 [9920/17010 (58%)] Loss: 0.076319
Train Epoch: 12 [10080/17010 (59%)] Loss: 0.068238
Train Epoch: 12 [10240/17010 (60%)] Loss: 0.469650
Train Epoch: 12 [10400/17010 (61%)] Loss: 0.276451
Train Epoch: 12 [10560/17010 (62%)] Loss: 0.313031
Train Epoch: 12 [10720/17010 (63%)] Loss: 0.056235
Train Epoch: 12 [10880/17010 (64%)] Loss: 0.137692
Train Epoch: 12 [11040/17010 (65%)] Loss: 0.247674
Train Epoch: 12 [11200/17010 (66%)] Loss: 0.247459
Train Epoch: 12 [11360/17010 (67%)] Loss: 0.273118
Train Epoch: 12 [11520/17010 (68%)] Loss: 0.120337
Train Epoch: 12 [11680/17010 (69%)] Loss: 0.168902
Train Epoch: 12 [11840/17010 (70%)] Loss: 0.191613
Train Epoch: 12 [12000/17010 (71%)] Loss: 0.064562
Train Epoch: 12 [12160/17010 (71%)] Loss: 0.159637
Train Epoch: 12 [12320/17010 (72%)] Loss: 0.081333
Train Epoch: 12 [12480/17010 (73%)] Loss: 0.244434
Train Epoch: 12 [12640/17010 (74%)] Loss: 0.152946
Train Epoch: 12 [12800/17010 (75%)] Loss: 0.372118
Train Epoch: 12 [12960/17010 (76%)] Loss: 0.138953
Train Epoch: 12 [13120/17010 (77%)] Loss: 0.193779
Train Epoch: 12 [13280/17010 (78%)] Loss: 0.089295
Train Epoch: 12 [13440/17010 (79%)] Loss: 0.025192
Train Epoch: 12 [13600/17010 (80%)] Loss: 0.123253
Train Epoch: 12 [13760/17010 (81%)] Loss: 0.108831
Train Epoch: 12 [13920/17010 (82%)] Loss: 0.122364
Train Epoch: 12 [14080/17010 (83%)] Loss: 0.307065
Train Epoch: 12 [14240/17010 (84%)] Loss: 0.154906
Train Epoch: 12 [14400/17010 (85%)] Loss: 0.102979
Train Epoch: 12 [14560/17010 (86%)] Loss: 0.264090
Train Epoch: 12 [14720/17010 (87%)] Loss: 0.055674
Train Epoch: 12 [14880/17010 (87%)] Loss: 0.249261
Train Epoch: 12 [15040/17010 (88%)] Loss: 0.059510
Train Epoch: 12 [15200/17010 (89%)] Loss: 0.247954
Train Epoch: 12 [15360/17010 (90%)] Loss: 0.059351
Train Epoch: 12 [15520/17010 (91%)] Loss: 0.039294
Train Epoch: 12 [15680/17010 (92%)] Loss: 0.117551
Train Epoch: 12 [15840/17010 (93%)] Loss: 0.166478
Train Epoch: 12 [16000/17010 (94%)] Loss: 0.056191
Train Epoch: 12 [16160/17010 (95%)] Loss: 0.088283
Train Epoch: 12 [16320/17010 (96%)] Loss: 0.040840
Train Epoch: 12 [16480/17010 (97%)] Loss: 0.072914
Train Epoch: 12 [16640/17010 (98%)] Loss: 0.053355
Train Epoch: 12 [16800/17010 (99%)] Loss: 0.112521
Train Epoch: 12 [16960/17010 (100%)] Loss: 0.035713
    epoch          : 12
    Train_loss     : 0.13548508292603256
    Train_accuracy : 0.9539473684210527
    Train_top_k_acc: 0.9991776315789473
    Val_loss       : 0.2955677729720871
    Val_accuracy   : 0.9119791666666667
    Val_top_k_acc  : 0.9942708333333333
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch12.pth ...
Train Epoch: 13 [0/17010 (0%)] Loss: 0.110181
Train Epoch: 13 [160/17010 (1%)] Loss: 0.261385
Train Epoch: 13 [320/17010 (2%)] Loss: 0.058552
Train Epoch: 13 [480/17010 (3%)] Loss: 0.121574
Train Epoch: 13 [640/17010 (4%)] Loss: 0.091013
Train Epoch: 13 [800/17010 (5%)] Loss: 0.049876
Train Epoch: 13 [960/17010 (6%)] Loss: 0.102899
Train Epoch: 13 [1120/17010 (7%)] Loss: 0.114308
Train Epoch: 13 [1280/17010 (8%)] Loss: 0.080146
Train Epoch: 13 [1440/17010 (8%)] Loss: 0.142725
Train Epoch: 13 [1600/17010 (9%)] Loss: 0.079583
Train Epoch: 13 [1760/17010 (10%)] Loss: 0.011747
Train Epoch: 13 [1920/17010 (11%)] Loss: 0.011281
Train Epoch: 13 [2080/17010 (12%)] Loss: 0.130301
Train Epoch: 13 [2240/17010 (13%)] Loss: 0.013654
Train Epoch: 13 [2400/17010 (14%)] Loss: 0.030585
Train Epoch: 13 [2560/17010 (15%)] Loss: 0.012312
Train Epoch: 13 [2720/17010 (16%)] Loss: 0.058422
Train Epoch: 13 [2880/17010 (17%)] Loss: 0.056698
Train Epoch: 13 [3040/17010 (18%)] Loss: 0.179131
Train Epoch: 13 [3200/17010 (19%)] Loss: 0.007600
Train Epoch: 13 [3360/17010 (20%)] Loss: 0.176549
Train Epoch: 13 [3520/17010 (21%)] Loss: 0.023547
Train Epoch: 13 [3680/17010 (22%)] Loss: 0.053677
Train Epoch: 13 [3840/17010 (23%)] Loss: 0.049231
Train Epoch: 13 [4000/17010 (24%)] Loss: 0.011505
Train Epoch: 13 [4160/17010 (24%)] Loss: 0.044381
Train Epoch: 13 [4320/17010 (25%)] Loss: 0.305682
Train Epoch: 13 [4480/17010 (26%)] Loss: 0.053889
Train Epoch: 13 [4640/17010 (27%)] Loss: 0.216430
Train Epoch: 13 [4800/17010 (28%)] Loss: 0.070484
Train Epoch: 13 [4960/17010 (29%)] Loss: 0.052315
Train Epoch: 13 [5120/17010 (30%)] Loss: 0.053713
Train Epoch: 13 [5280/17010 (31%)] Loss: 0.083340
Train Epoch: 13 [5440/17010 (32%)] Loss: 0.071121
Train Epoch: 13 [5600/17010 (33%)] Loss: 0.027724
Train Epoch: 13 [5760/17010 (34%)] Loss: 0.085836
Train Epoch: 13 [5920/17010 (35%)] Loss: 0.145129
Train Epoch: 13 [6080/17010 (36%)] Loss: 0.087924
Train Epoch: 13 [6240/17010 (37%)] Loss: 0.049340
Train Epoch: 13 [6400/17010 (38%)] Loss: 0.083770
Train Epoch: 13 [6560/17010 (39%)] Loss: 0.216240
Train Epoch: 13 [6720/17010 (40%)] Loss: 0.019583
Train Epoch: 13 [6880/17010 (40%)] Loss: 0.185161
Train Epoch: 13 [7040/17010 (41%)] Loss: 0.048371
Train Epoch: 13 [7200/17010 (42%)] Loss: 0.034304
Train Epoch: 13 [7360/17010 (43%)] Loss: 0.138282
Train Epoch: 13 [7520/17010 (44%)] Loss: 0.032594
Train Epoch: 13 [7680/17010 (45%)] Loss: 0.273703
Train Epoch: 13 [7840/17010 (46%)] Loss: 0.242905
Train Epoch: 13 [8000/17010 (47%)] Loss: 0.059485
Train Epoch: 13 [8160/17010 (48%)] Loss: 0.028707
Train Epoch: 13 [8320/17010 (49%)] Loss: 0.007543
Train Epoch: 13 [8480/17010 (50%)] Loss: 0.121470
Train Epoch: 13 [8640/17010 (51%)] Loss: 0.019040
Train Epoch: 13 [8800/17010 (52%)] Loss: 0.041889
Train Epoch: 13 [8960/17010 (53%)] Loss: 0.134834
Train Epoch: 13 [9120/17010 (54%)] Loss: 0.107654
Train Epoch: 13 [9280/17010 (55%)] Loss: 0.060768
Train Epoch: 13 [9440/17010 (55%)] Loss: 0.117201
Train Epoch: 13 [9600/17010 (56%)] Loss: 0.037303
Train Epoch: 13 [9760/17010 (57%)] Loss: 0.112442
Train Epoch: 13 [9920/17010 (58%)] Loss: 0.051623
Train Epoch: 13 [10080/17010 (59%)] Loss: 0.121954
Train Epoch: 13 [10240/17010 (60%)] Loss: 0.042253
Train Epoch: 13 [10400/17010 (61%)] Loss: 0.047686
Train Epoch: 13 [10560/17010 (62%)] Loss: 0.083617
Train Epoch: 13 [10720/17010 (63%)] Loss: 0.374938
Train Epoch: 13 [10880/17010 (64%)] Loss: 0.267505
Train Epoch: 13 [11040/17010 (65%)] Loss: 0.074975
Train Epoch: 13 [11200/17010 (66%)] Loss: 0.205436
Train Epoch: 13 [11360/17010 (67%)] Loss: 0.100441
Train Epoch: 13 [11520/17010 (68%)] Loss: 0.069737
Train Epoch: 13 [11680/17010 (69%)] Loss: 0.115630
Train Epoch: 13 [11840/17010 (70%)] Loss: 0.143605
Train Epoch: 13 [12000/17010 (71%)] Loss: 0.082318
Train Epoch: 13 [12160/17010 (71%)] Loss: 0.027873
Train Epoch: 13 [12320/17010 (72%)] Loss: 0.194498
Train Epoch: 13 [12480/17010 (73%)] Loss: 0.124082
Train Epoch: 13 [12640/17010 (74%)] Loss: 0.054141
Train Epoch: 13 [12800/17010 (75%)] Loss: 0.195896
Train Epoch: 13 [12960/17010 (76%)] Loss: 0.067441
Train Epoch: 13 [13120/17010 (77%)] Loss: 0.090092
Train Epoch: 13 [13280/17010 (78%)] Loss: 0.051510
Train Epoch: 13 [13440/17010 (79%)] Loss: 0.062261
Train Epoch: 13 [13600/17010 (80%)] Loss: 0.022727
Train Epoch: 13 [13760/17010 (81%)] Loss: 0.098978
Train Epoch: 13 [13920/17010 (82%)] Loss: 0.143226
Train Epoch: 13 [14080/17010 (83%)] Loss: 0.048726
Train Epoch: 13 [14240/17010 (84%)] Loss: 0.144179
Train Epoch: 13 [14400/17010 (85%)] Loss: 0.072116
Train Epoch: 13 [14560/17010 (86%)] Loss: 0.062766
Train Epoch: 13 [14720/17010 (87%)] Loss: 0.121662
Train Epoch: 13 [14880/17010 (87%)] Loss: 0.032525
Train Epoch: 13 [15040/17010 (88%)] Loss: 0.052625
Train Epoch: 13 [15200/17010 (89%)] Loss: 0.117420
Train Epoch: 13 [15360/17010 (90%)] Loss: 0.133964
Train Epoch: 13 [15520/17010 (91%)] Loss: 0.104630
Train Epoch: 13 [15680/17010 (92%)] Loss: 0.104403
Train Epoch: 13 [15840/17010 (93%)] Loss: 0.110355
Train Epoch: 13 [16000/17010 (94%)] Loss: 0.237893
Train Epoch: 13 [16160/17010 (95%)] Loss: 0.028096
Train Epoch: 13 [16320/17010 (96%)] Loss: 0.055655
Train Epoch: 13 [16480/17010 (97%)] Loss: 0.046195
Train Epoch: 13 [16640/17010 (98%)] Loss: 0.047106
Train Epoch: 13 [16800/17010 (99%)] Loss: 0.053439
Train Epoch: 13 [16960/17010 (100%)] Loss: 0.059174
    epoch          : 13
    Train_loss     : 0.11440233748206603
    Train_accuracy : 0.9606372702589808
    Train_top_k_acc: 0.9991188909774437
    Val_loss       : 0.16210239160961162
    Val_accuracy   : 0.9453125
    Val_top_k_acc  : 0.9994791666666667
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch13.pth ...
Train Epoch: 14 [0/17010 (0%)] Loss: 0.042398
Train Epoch: 14 [160/17010 (1%)] Loss: 0.127838
Train Epoch: 14 [320/17010 (2%)] Loss: 0.063561
Train Epoch: 14 [480/17010 (3%)] Loss: 0.013713
Train Epoch: 14 [640/17010 (4%)] Loss: 0.052575
Train Epoch: 14 [800/17010 (5%)] Loss: 0.014947
Train Epoch: 14 [960/17010 (6%)] Loss: 0.079482
Train Epoch: 14 [1120/17010 (7%)] Loss: 0.270359
Train Epoch: 14 [1280/17010 (8%)] Loss: 0.149450
Train Epoch: 14 [1440/17010 (8%)] Loss: 0.028624
Train Epoch: 14 [1600/17010 (9%)] Loss: 0.010975
Train Epoch: 14 [1760/17010 (10%)] Loss: 0.127320
Train Epoch: 14 [1920/17010 (11%)] Loss: 0.205992
Train Epoch: 14 [2080/17010 (12%)] Loss: 0.058554
Train Epoch: 14 [2240/17010 (13%)] Loss: 0.035724
Train Epoch: 14 [2400/17010 (14%)] Loss: 0.025290
Train Epoch: 14 [2560/17010 (15%)] Loss: 0.076523
Train Epoch: 14 [2720/17010 (16%)] Loss: 0.091084
Train Epoch: 14 [2880/17010 (17%)] Loss: 0.119031
Train Epoch: 14 [3040/17010 (18%)] Loss: 0.130969
Train Epoch: 14 [3200/17010 (19%)] Loss: 0.147781
Train Epoch: 14 [3360/17010 (20%)] Loss: 0.105311
Train Epoch: 14 [3520/17010 (21%)] Loss: 0.130584
Train Epoch: 14 [3680/17010 (22%)] Loss: 0.006597
Train Epoch: 14 [3840/17010 (23%)] Loss: 0.049653
Train Epoch: 14 [4000/17010 (24%)] Loss: 0.077028
Train Epoch: 14 [4160/17010 (24%)] Loss: 0.263868
Train Epoch: 14 [4320/17010 (25%)] Loss: 0.176065
Train Epoch: 14 [4480/17010 (26%)] Loss: 0.106355
Train Epoch: 14 [4640/17010 (27%)] Loss: 0.128767
Train Epoch: 14 [4800/17010 (28%)] Loss: 0.123862
Train Epoch: 14 [4960/17010 (29%)] Loss: 0.131010
Train Epoch: 14 [5120/17010 (30%)] Loss: 0.016411
Train Epoch: 14 [5280/17010 (31%)] Loss: 0.057105
Train Epoch: 14 [5440/17010 (32%)] Loss: 0.040020
Train Epoch: 14 [5600/17010 (33%)] Loss: 0.102871
Train Epoch: 14 [5760/17010 (34%)] Loss: 0.192415
Train Epoch: 14 [5920/17010 (35%)] Loss: 0.192629
Train Epoch: 14 [6080/17010 (36%)] Loss: 0.074421
Train Epoch: 14 [6240/17010 (37%)] Loss: 0.108381
Train Epoch: 14 [6400/17010 (38%)] Loss: 0.085474
Train Epoch: 14 [6560/17010 (39%)] Loss: 0.065482
Train Epoch: 14 [6720/17010 (40%)] Loss: 0.019588
Train Epoch: 14 [6880/17010 (40%)] Loss: 0.049236
Train Epoch: 14 [7040/17010 (41%)] Loss: 0.007029
Train Epoch: 14 [7200/17010 (42%)] Loss: 0.109705
Train Epoch: 14 [7360/17010 (43%)] Loss: 0.020103
Train Epoch: 14 [7520/17010 (44%)] Loss: 0.161761
Train Epoch: 14 [7680/17010 (45%)] Loss: 0.118106
Train Epoch: 14 [7840/17010 (46%)] Loss: 0.062641
Train Epoch: 14 [8000/17010 (47%)] Loss: 0.030502
Train Epoch: 14 [8160/17010 (48%)] Loss: 0.134328
Train Epoch: 14 [8320/17010 (49%)] Loss: 0.093144
Train Epoch: 14 [8480/17010 (50%)] Loss: 0.136736
Train Epoch: 14 [8640/17010 (51%)] Loss: 0.041750
Train Epoch: 14 [8800/17010 (52%)] Loss: 0.076648
Train Epoch: 14 [8960/17010 (53%)] Loss: 0.251180
Train Epoch: 14 [9120/17010 (54%)] Loss: 0.239645
Train Epoch: 14 [9280/17010 (55%)] Loss: 0.007269
Train Epoch: 14 [9440/17010 (55%)] Loss: 0.035479
Train Epoch: 14 [9600/17010 (56%)] Loss: 0.101092
Train Epoch: 14 [9760/17010 (57%)] Loss: 0.038465
Train Epoch: 14 [9920/17010 (58%)] Loss: 0.028698
Train Epoch: 14 [10080/17010 (59%)] Loss: 0.034453
Train Epoch: 14 [10240/17010 (60%)] Loss: 0.231139
Train Epoch: 14 [10400/17010 (61%)] Loss: 0.049100
Train Epoch: 14 [10560/17010 (62%)] Loss: 0.067300
Train Epoch: 14 [10720/17010 (63%)] Loss: 0.035818
Train Epoch: 14 [10880/17010 (64%)] Loss: 0.006155
Train Epoch: 14 [11040/17010 (65%)] Loss: 0.030531
Train Epoch: 14 [11200/17010 (66%)] Loss: 0.243284
Train Epoch: 14 [11360/17010 (67%)] Loss: 0.045583
Train Epoch: 14 [11520/17010 (68%)] Loss: 0.233218
Train Epoch: 14 [11680/17010 (69%)] Loss: 0.018153
Train Epoch: 14 [11840/17010 (70%)] Loss: 0.038079
Train Epoch: 14 [12000/17010 (71%)] Loss: 0.013990
Train Epoch: 14 [12160/17010 (71%)] Loss: 0.072185
Train Epoch: 14 [12320/17010 (72%)] Loss: 0.101329
Train Epoch: 14 [12480/17010 (73%)] Loss: 0.130034
Train Epoch: 14 [12640/17010 (74%)] Loss: 0.079722
Train Epoch: 14 [12800/17010 (75%)] Loss: 0.024344
Train Epoch: 14 [12960/17010 (76%)] Loss: 0.008979
Train Epoch: 14 [13120/17010 (77%)] Loss: 0.023593
Train Epoch: 14 [13280/17010 (78%)] Loss: 0.112534
Train Epoch: 14 [13440/17010 (79%)] Loss: 0.027450
Train Epoch: 14 [13600/17010 (80%)] Loss: 0.045478
Train Epoch: 14 [13760/17010 (81%)] Loss: 0.261055
Train Epoch: 14 [13920/17010 (82%)] Loss: 0.122546
Train Epoch: 14 [14080/17010 (83%)] Loss: 0.123206
Train Epoch: 14 [14240/17010 (84%)] Loss: 0.041573
Train Epoch: 14 [14400/17010 (85%)] Loss: 0.184165
Train Epoch: 14 [14560/17010 (86%)] Loss: 0.134156
Train Epoch: 14 [14720/17010 (87%)] Loss: 0.030908
Train Epoch: 14 [14880/17010 (87%)] Loss: 0.055902
Train Epoch: 14 [15040/17010 (88%)] Loss: 0.246583
Train Epoch: 14 [15200/17010 (89%)] Loss: 0.326664
Train Epoch: 14 [15360/17010 (90%)] Loss: 0.160439
Train Epoch: 14 [15520/17010 (91%)] Loss: 0.069805
Train Epoch: 14 [15680/17010 (92%)] Loss: 0.114858
Train Epoch: 14 [15840/17010 (93%)] Loss: 0.017869
Train Epoch: 14 [16000/17010 (94%)] Loss: 0.035685
Train Epoch: 14 [16160/17010 (95%)] Loss: 0.012483
Train Epoch: 14 [16320/17010 (96%)] Loss: 0.007325
Train Epoch: 14 [16480/17010 (97%)] Loss: 0.282992
Train Epoch: 14 [16640/17010 (98%)] Loss: 0.041386
Train Epoch: 14 [16800/17010 (99%)] Loss: 0.125205
Train Epoch: 14 [16960/17010 (100%)] Loss: 0.062622
    epoch          : 14
    Train_loss     : 0.09975260428172235
    Train_accuracy : 0.9642400271512114
    Train_top_k_acc: 0.9994125939849624
    Val_loss       : 0.3704787184484303
    Val_accuracy   : 0.8911458333333333
    Val_top_k_acc  : 0.9880208333333333
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch14.pth ...
Train Epoch: 15 [0/17010 (0%)] Loss: 0.104230
/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Network error (ReadTimeout), entering retry loop.
Train Epoch: 15 [160/17010 (1%)] Loss: 0.082487
Train Epoch: 15 [320/17010 (2%)] Loss: 0.024607
Train Epoch: 15 [480/17010 (3%)] Loss: 0.044729
Train Epoch: 15 [640/17010 (4%)] Loss: 0.040600
Train Epoch: 15 [800/17010 (5%)] Loss: 0.124689
Train Epoch: 15 [960/17010 (6%)] Loss: 0.051827
Train Epoch: 15 [1120/17010 (7%)] Loss: 0.045337
Train Epoch: 15 [1280/17010 (8%)] Loss: 0.117637
Train Epoch: 15 [1440/17010 (8%)] Loss: 0.015171
Train Epoch: 15 [1600/17010 (9%)] Loss: 0.150819
Train Epoch: 15 [1760/17010 (10%)] Loss: 0.221630
Train Epoch: 15 [1920/17010 (11%)] Loss: 0.019925
Train Epoch: 15 [2080/17010 (12%)] Loss: 0.092796
Train Epoch: 15 [2240/17010 (13%)] Loss: 0.042294
Train Epoch: 15 [2400/17010 (14%)] Loss: 0.024136
Train Epoch: 15 [2560/17010 (15%)] Loss: 0.254962
Train Epoch: 15 [2720/17010 (16%)] Loss: 0.098288
Train Epoch: 15 [2880/17010 (17%)] Loss: 0.173759
Train Epoch: 15 [3040/17010 (18%)] Loss: 0.361050
Train Epoch: 15 [3200/17010 (19%)] Loss: 0.212002
wandb: Network error (ReadTimeout), entering retry loop.
Train Epoch: 15 [3360/17010 (20%)] Loss: 0.036776
Train Epoch: 15 [3520/17010 (21%)] Loss: 0.077715
Train Epoch: 15 [3680/17010 (22%)] Loss: 0.051953
Train Epoch: 15 [3840/17010 (23%)] Loss: 0.192832
Train Epoch: 15 [4000/17010 (24%)] Loss: 0.075698
Train Epoch: 15 [4160/17010 (24%)] Loss: 0.372214
Train Epoch: 15 [4320/17010 (25%)] Loss: 0.034903
Train Epoch: 15 [4480/17010 (26%)] Loss: 0.093897
Train Epoch: 15 [4640/17010 (27%)] Loss: 0.052921
Train Epoch: 15 [4800/17010 (28%)] Loss: 0.054097
Train Epoch: 15 [4960/17010 (29%)] Loss: 0.251865
Train Epoch: 15 [5120/17010 (30%)] Loss: 0.042725
Train Epoch: 15 [5280/17010 (31%)] Loss: 0.109574
Train Epoch: 15 [5440/17010 (32%)] Loss: 0.102786
Train Epoch: 15 [5600/17010 (33%)] Loss: 0.025354
Train Epoch: 15 [5760/17010 (34%)] Loss: 0.103632
Train Epoch: 15 [5920/17010 (35%)] Loss: 0.075632
Train Epoch: 15 [6080/17010 (36%)] Loss: 0.054044
Train Epoch: 15 [6240/17010 (37%)] Loss: 0.095482
Train Epoch: 15 [6400/17010 (38%)] Loss: 0.016105
Train Epoch: 15 [6560/17010 (39%)] Loss: 0.064387
Train Epoch: 15 [6720/17010 (40%)] Loss: 0.047881
Train Epoch: 15 [6880/17010 (40%)] Loss: 0.203975
Train Epoch: 15 [7040/17010 (41%)] Loss: 0.076957
Train Epoch: 15 [7200/17010 (42%)] Loss: 0.148632
Train Epoch: 15 [7360/17010 (43%)] Loss: 0.134581
Train Epoch: 15 [7520/17010 (44%)] Loss: 0.071239
Train Epoch: 15 [7680/17010 (45%)] Loss: 0.143164
Train Epoch: 15 [7840/17010 (46%)] Loss: 0.084184
Train Epoch: 15 [8000/17010 (47%)] Loss: 0.075246
Train Epoch: 15 [8160/17010 (48%)] Loss: 0.038228
Train Epoch: 15 [8320/17010 (49%)] Loss: 0.109085
Train Epoch: 15 [8480/17010 (50%)] Loss: 0.076348
Train Epoch: 15 [8640/17010 (51%)] Loss: 0.086671
Train Epoch: 15 [8800/17010 (52%)] Loss: 0.119107
Train Epoch: 15 [8960/17010 (53%)] Loss: 0.089135
Train Epoch: 15 [9120/17010 (54%)] Loss: 0.203868
Train Epoch: 15 [9280/17010 (55%)] Loss: 0.031309
Train Epoch: 15 [9440/17010 (55%)] Loss: 0.053573
Train Epoch: 15 [9600/17010 (56%)] Loss: 0.009955
Train Epoch: 15 [9760/17010 (57%)] Loss: 0.052618
Train Epoch: 15 [9920/17010 (58%)] Loss: 0.070401
Train Epoch: 15 [10080/17010 (59%)] Loss: 0.059945
Train Epoch: 15 [10240/17010 (60%)] Loss: 0.204512
Train Epoch: 15 [10400/17010 (61%)] Loss: 0.347172
Train Epoch: 15 [10560/17010 (62%)] Loss: 0.298442
Train Epoch: 15 [10720/17010 (63%)] Loss: 0.786093
Train Epoch: 15 [10880/17010 (64%)] Loss: 0.206070
Train Epoch: 15 [11040/17010 (65%)] Loss: 0.165477
Train Epoch: 15 [11200/17010 (66%)] Loss: 0.146633
Train Epoch: 15 [11360/17010 (67%)] Loss: 0.358067
Train Epoch: 15 [11520/17010 (68%)] Loss: 0.345776
Train Epoch: 15 [11680/17010 (69%)] Loss: 0.359022
Train Epoch: 15 [11840/17010 (70%)] Loss: 0.416157
Train Epoch: 15 [12000/17010 (71%)] Loss: 0.158451
Train Epoch: 15 [12160/17010 (71%)] Loss: 0.028642
Train Epoch: 15 [12320/17010 (72%)] Loss: 0.101443
Train Epoch: 15 [12480/17010 (73%)] Loss: 0.256663
Train Epoch: 15 [12640/17010 (74%)] Loss: 0.030776
Train Epoch: 15 [12800/17010 (75%)] Loss: 0.013623
Train Epoch: 15 [12960/17010 (76%)] Loss: 0.046678
Train Epoch: 15 [13120/17010 (77%)] Loss: 0.170544
Train Epoch: 15 [13280/17010 (78%)] Loss: 0.076015
Train Epoch: 15 [13440/17010 (79%)] Loss: 0.038638
Train Epoch: 15 [13600/17010 (80%)] Loss: 0.025685
Train Epoch: 15 [13760/17010 (81%)] Loss: 0.079333
Train Epoch: 15 [13920/17010 (82%)] Loss: 0.277252
Train Epoch: 15 [14080/17010 (83%)] Loss: 0.038960
Train Epoch: 15 [14240/17010 (84%)] Loss: 0.485571
Train Epoch: 15 [14400/17010 (85%)] Loss: 0.138613
Train Epoch: 15 [14560/17010 (86%)] Loss: 0.064809
Train Epoch: 15 [14720/17010 (87%)] Loss: 0.195785
Train Epoch: 15 [14880/17010 (87%)] Loss: 0.094555
Train Epoch: 15 [15040/17010 (88%)] Loss: 0.015855
Train Epoch: 15 [15200/17010 (89%)] Loss: 0.127434
Train Epoch: 15 [15360/17010 (90%)] Loss: 0.300679
Train Epoch: 15 [15520/17010 (91%)] Loss: 0.219232
Train Epoch: 15 [15680/17010 (92%)] Loss: 0.047055
Train Epoch: 15 [15840/17010 (93%)] Loss: 0.137485
Train Epoch: 15 [16000/17010 (94%)] Loss: 0.083094
Train Epoch: 15 [16160/17010 (95%)] Loss: 0.200297
Train Epoch: 15 [16320/17010 (96%)] Loss: 0.038780
Train Epoch: 15 [16480/17010 (97%)] Loss: 0.051714
Train Epoch: 15 [16640/17010 (98%)] Loss: 0.031350
Train Epoch: 15 [16800/17010 (99%)] Loss: 0.116459
Train Epoch: 15 [16960/17010 (100%)] Loss: 0.141495
    epoch          : 15
    Train_loss     : 0.10430048894455754
    Train_accuracy : 0.9647556390977443
    Train_top_k_acc: 0.9991188909774437
    Val_loss       : 0.29758634176881366
    Val_accuracy   : 0.9057291666666667
    Val_top_k_acc  : 0.99375
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch15.pth ...
Train Epoch: 16 [0/17010 (0%)] Loss: 0.132161
Train Epoch: 16 [160/17010 (1%)] Loss: 0.093494
Train Epoch: 16 [320/17010 (2%)] Loss: 0.049023
Train Epoch: 16 [480/17010 (3%)] Loss: 0.166251
Train Epoch: 16 [640/17010 (4%)] Loss: 0.104217
Train Epoch: 16 [800/17010 (5%)] Loss: 0.099946
Train Epoch: 16 [960/17010 (6%)] Loss: 0.044880
Train Epoch: 16 [1120/17010 (7%)] Loss: 0.115502
Train Epoch: 16 [1280/17010 (8%)] Loss: 0.033996
Train Epoch: 16 [1440/17010 (8%)] Loss: 0.065745
Train Epoch: 16 [1600/17010 (9%)] Loss: 0.079614
Train Epoch: 16 [1760/17010 (10%)] Loss: 0.029525
Train Epoch: 16 [1920/17010 (11%)] Loss: 0.048709
Train Epoch: 16 [2080/17010 (12%)] Loss: 0.098999
Train Epoch: 16 [2240/17010 (13%)] Loss: 0.009792
Train Epoch: 16 [2400/17010 (14%)] Loss: 0.041367
Train Epoch: 16 [2560/17010 (15%)] Loss: 0.094559
Train Epoch: 16 [2720/17010 (16%)] Loss: 0.067731
Train Epoch: 16 [2880/17010 (17%)] Loss: 0.010508
Train Epoch: 16 [3040/17010 (18%)] Loss: 0.060976
Train Epoch: 16 [3200/17010 (19%)] Loss: 0.055201
Train Epoch: 16 [3360/17010 (20%)] Loss: 0.041896
Train Epoch: 16 [3520/17010 (21%)] Loss: 0.166403
Train Epoch: 16 [3680/17010 (22%)] Loss: 0.025237
Train Epoch: 16 [3840/17010 (23%)] Loss: 0.370580
Train Epoch: 16 [4000/17010 (24%)] Loss: 0.023945
Train Epoch: 16 [4160/17010 (24%)] Loss: 0.026985
Train Epoch: 16 [4320/17010 (25%)] Loss: 0.146785
Train Epoch: 16 [4480/17010 (26%)] Loss: 0.028163
Train Epoch: 16 [4640/17010 (27%)] Loss: 0.024076
Train Epoch: 16 [4800/17010 (28%)] Loss: 0.013564
Train Epoch: 16 [4960/17010 (29%)] Loss: 0.088788
Train Epoch: 16 [5120/17010 (30%)] Loss: 0.009325
Train Epoch: 16 [5280/17010 (31%)] Loss: 0.012977
Train Epoch: 16 [5440/17010 (32%)] Loss: 0.081103
Train Epoch: 16 [5600/17010 (33%)] Loss: 0.015536
Train Epoch: 16 [5760/17010 (34%)] Loss: 0.036090
Train Epoch: 16 [5920/17010 (35%)] Loss: 0.070820
Train Epoch: 16 [6080/17010 (36%)] Loss: 0.085017
Train Epoch: 16 [6240/17010 (37%)] Loss: 0.014629
Train Epoch: 16 [6400/17010 (38%)] Loss: 0.030685
Train Epoch: 16 [6560/17010 (39%)] Loss: 0.046334
Train Epoch: 16 [6720/17010 (40%)] Loss: 0.014497
Train Epoch: 16 [6880/17010 (40%)] Loss: 0.013759
Train Epoch: 16 [7040/17010 (41%)] Loss: 0.107985
Train Epoch: 16 [7200/17010 (42%)] Loss: 0.024837
Train Epoch: 16 [7360/17010 (43%)] Loss: 0.016136
Train Epoch: 16 [7520/17010 (44%)] Loss: 0.020308
Train Epoch: 16 [7680/17010 (45%)] Loss: 0.004048
Train Epoch: 16 [7840/17010 (46%)] Loss: 0.030383
Train Epoch: 16 [8000/17010 (47%)] Loss: 0.024429
Train Epoch: 16 [8160/17010 (48%)] Loss: 0.041202
Train Epoch: 16 [8320/17010 (49%)] Loss: 0.069072
Train Epoch: 16 [8480/17010 (50%)] Loss: 0.162901
Train Epoch: 16 [8640/17010 (51%)] Loss: 0.027631
Train Epoch: 16 [8800/17010 (52%)] Loss: 0.225719
Train Epoch: 16 [8960/17010 (53%)] Loss: 0.023826
Train Epoch: 16 [9120/17010 (54%)] Loss: 0.017126
Train Epoch: 16 [9280/17010 (55%)] Loss: 0.051026
Train Epoch: 16 [9440/17010 (55%)] Loss: 0.324466
Train Epoch: 16 [9600/17010 (56%)] Loss: 0.182410
Train Epoch: 16 [9760/17010 (57%)] Loss: 0.017575
Train Epoch: 16 [9920/17010 (58%)] Loss: 0.313402
Train Epoch: 16 [10080/17010 (59%)] Loss: 0.062018
Train Epoch: 16 [10240/17010 (60%)] Loss: 0.184795
Train Epoch: 16 [10400/17010 (61%)] Loss: 0.105858
Train Epoch: 16 [10560/17010 (62%)] Loss: 0.146208
Train Epoch: 16 [10720/17010 (63%)] Loss: 0.315066
Train Epoch: 16 [10880/17010 (64%)] Loss: 0.170084
Train Epoch: 16 [11040/17010 (65%)] Loss: 0.073828
Train Epoch: 16 [11200/17010 (66%)] Loss: 0.041577
Train Epoch: 16 [11360/17010 (67%)] Loss: 0.027618
Train Epoch: 16 [11520/17010 (68%)] Loss: 0.091911
Train Epoch: 16 [11680/17010 (69%)] Loss: 0.005988
Train Epoch: 16 [11840/17010 (70%)] Loss: 0.057866
Train Epoch: 16 [12000/17010 (71%)] Loss: 0.063158
Train Epoch: 16 [12160/17010 (71%)] Loss: 0.009593
Train Epoch: 16 [12320/17010 (72%)] Loss: 0.028642
Train Epoch: 16 [12480/17010 (73%)] Loss: 0.012450
Train Epoch: 16 [12640/17010 (74%)] Loss: 0.063507
Train Epoch: 16 [12800/17010 (75%)] Loss: 0.013386
Train Epoch: 16 [12960/17010 (76%)] Loss: 0.130368
Train Epoch: 16 [13120/17010 (77%)] Loss: 0.033895
Train Epoch: 16 [13280/17010 (78%)] Loss: 0.005002
Train Epoch: 16 [13440/17010 (79%)] Loss: 0.011752
Train Epoch: 16 [13600/17010 (80%)] Loss: 0.100088
Train Epoch: 16 [13760/17010 (81%)] Loss: 0.033738
Train Epoch: 16 [13920/17010 (82%)] Loss: 0.027007
Train Epoch: 16 [14080/17010 (83%)] Loss: 0.028852
Train Epoch: 16 [14240/17010 (84%)] Loss: 0.029028
Train Epoch: 16 [14400/17010 (85%)] Loss: 0.064141
Train Epoch: 16 [14560/17010 (86%)] Loss: 0.012655
Train Epoch: 16 [14720/17010 (87%)] Loss: 0.146590
Train Epoch: 16 [14880/17010 (87%)] Loss: 0.015917
Train Epoch: 16 [15040/17010 (88%)] Loss: 0.448867
Train Epoch: 16 [15200/17010 (89%)] Loss: 0.149252
Train Epoch: 16 [15360/17010 (90%)] Loss: 0.007679
Train Epoch: 16 [15520/17010 (91%)] Loss: 0.006819
Train Epoch: 16 [15680/17010 (92%)] Loss: 0.031921
Train Epoch: 16 [15840/17010 (93%)] Loss: 0.084330
Train Epoch: 16 [16000/17010 (94%)] Loss: 0.135095
Train Epoch: 16 [16160/17010 (95%)] Loss: 0.031867
Train Epoch: 16 [16320/17010 (96%)] Loss: 0.085411
Train Epoch: 16 [16480/17010 (97%)] Loss: 0.007877
Train Epoch: 16 [16640/17010 (98%)] Loss: 0.071157
Train Epoch: 16 [16800/17010 (99%)] Loss: 0.239600
Train Epoch: 16 [16960/17010 (100%)] Loss: 0.014530
    epoch          : 16
    Train_loss     : 0.0841319859023486
    Train_accuracy : 0.9718045112781954
    Train_top_k_acc: 0.9992951127819549
    Val_loss       : 0.3281791266907627
    Val_accuracy   : 0.9
    Val_top_k_acc  : 0.9932291666666667
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch16.pth ...
Train Epoch: 17 [0/17010 (0%)] Loss: 0.216622
Train Epoch: 17 [160/17010 (1%)] Loss: 0.031876
Train Epoch: 17 [320/17010 (2%)] Loss: 0.034800
Train Epoch: 17 [480/17010 (3%)] Loss: 0.164607
Train Epoch: 17 [640/17010 (4%)] Loss: 0.008304
Train Epoch: 17 [800/17010 (5%)] Loss: 0.020944
Train Epoch: 17 [960/17010 (6%)] Loss: 0.030649
Train Epoch: 17 [1120/17010 (7%)] Loss: 0.020948
Train Epoch: 17 [1280/17010 (8%)] Loss: 0.010148
Train Epoch: 17 [1440/17010 (8%)] Loss: 0.016080
Train Epoch: 17 [1600/17010 (9%)] Loss: 0.039000
Train Epoch: 17 [1760/17010 (10%)] Loss: 0.012539
Train Epoch: 17 [1920/17010 (11%)] Loss: 0.006673
Train Epoch: 17 [2080/17010 (12%)] Loss: 0.006251
Train Epoch: 17 [2240/17010 (13%)] Loss: 0.142791
Train Epoch: 17 [2400/17010 (14%)] Loss: 0.018133
Train Epoch: 17 [2560/17010 (15%)] Loss: 0.106954
Train Epoch: 17 [2720/17010 (16%)] Loss: 0.131287
Train Epoch: 17 [2880/17010 (17%)] Loss: 0.121707
Train Epoch: 17 [3040/17010 (18%)] Loss: 0.015314
Train Epoch: 17 [3200/17010 (19%)] Loss: 0.064754
Train Epoch: 17 [3360/17010 (20%)] Loss: 0.011691
Train Epoch: 17 [3520/17010 (21%)] Loss: 0.030150
Train Epoch: 17 [3680/17010 (22%)] Loss: 0.076123
Train Epoch: 17 [3840/17010 (23%)] Loss: 0.187864
Train Epoch: 17 [4000/17010 (24%)] Loss: 0.013574
Train Epoch: 17 [4160/17010 (24%)] Loss: 0.158071
Train Epoch: 17 [4320/17010 (25%)] Loss: 0.020132
Train Epoch: 17 [4480/17010 (26%)] Loss: 0.034530
Train Epoch: 17 [4640/17010 (27%)] Loss: 0.136827
Train Epoch: 17 [4800/17010 (28%)] Loss: 0.175816
Train Epoch: 17 [4960/17010 (29%)] Loss: 0.112362
Train Epoch: 17 [5120/17010 (30%)] Loss: 0.021752
Train Epoch: 17 [5280/17010 (31%)] Loss: 0.012127
Train Epoch: 17 [5440/17010 (32%)] Loss: 0.262382
Train Epoch: 17 [5600/17010 (33%)] Loss: 0.030711
Train Epoch: 17 [5760/17010 (34%)] Loss: 0.058863
Train Epoch: 17 [5920/17010 (35%)] Loss: 0.052785
Train Epoch: 17 [6080/17010 (36%)] Loss: 0.191534
Train Epoch: 17 [6240/17010 (37%)] Loss: 0.349387
Train Epoch: 17 [6400/17010 (38%)] Loss: 0.170078
Train Epoch: 17 [6560/17010 (39%)] Loss: 0.051462
Train Epoch: 17 [6720/17010 (40%)] Loss: 0.071200
Train Epoch: 17 [6880/17010 (40%)] Loss: 0.199026
Train Epoch: 17 [7040/17010 (41%)] Loss: 0.049060
Train Epoch: 17 [7200/17010 (42%)] Loss: 0.131726
Train Epoch: 17 [7360/17010 (43%)] Loss: 0.159821
Train Epoch: 17 [7520/17010 (44%)] Loss: 0.036195
Train Epoch: 17 [7680/17010 (45%)] Loss: 0.030163
Train Epoch: 17 [7840/17010 (46%)] Loss: 0.131274
Train Epoch: 17 [8000/17010 (47%)] Loss: 0.012183
Train Epoch: 17 [8160/17010 (48%)] Loss: 0.070319
Train Epoch: 17 [8320/17010 (49%)] Loss: 0.006959
Train Epoch: 17 [8480/17010 (50%)] Loss: 0.040456
Train Epoch: 17 [8640/17010 (51%)] Loss: 0.010763
Train Epoch: 17 [8800/17010 (52%)] Loss: 0.008416
Train Epoch: 17 [8960/17010 (53%)] Loss: 0.003512
Train Epoch: 17 [9120/17010 (54%)] Loss: 0.011638
Train Epoch: 17 [9280/17010 (55%)] Loss: 0.017731
Train Epoch: 17 [9440/17010 (55%)] Loss: 0.009145
Train Epoch: 17 [9600/17010 (56%)] Loss: 0.037995
Train Epoch: 17 [9760/17010 (57%)] Loss: 0.397730
Train Epoch: 17 [9920/17010 (58%)] Loss: 0.067246
Train Epoch: 17 [10080/17010 (59%)] Loss: 0.019666
Train Epoch: 17 [10240/17010 (60%)] Loss: 0.251297
Train Epoch: 17 [10400/17010 (61%)] Loss: 0.026612
Train Epoch: 17 [10560/17010 (62%)] Loss: 0.004569
Train Epoch: 17 [10720/17010 (63%)] Loss: 0.324080
Train Epoch: 17 [10880/17010 (64%)] Loss: 0.041805
Train Epoch: 17 [11040/17010 (65%)] Loss: 0.046319
Train Epoch: 17 [11200/17010 (66%)] Loss: 0.250500
Train Epoch: 17 [11360/17010 (67%)] Loss: 0.122574
Train Epoch: 17 [11520/17010 (68%)] Loss: 0.049365
Train Epoch: 17 [11680/17010 (69%)] Loss: 0.122320
Train Epoch: 17 [11840/17010 (70%)] Loss: 0.019155
Train Epoch: 17 [12000/17010 (71%)] Loss: 0.080692
Train Epoch: 17 [12160/17010 (71%)] Loss: 0.116691
Train Epoch: 17 [12320/17010 (72%)] Loss: 0.036487
Train Epoch: 17 [12480/17010 (73%)] Loss: 0.053139
Train Epoch: 17 [12640/17010 (74%)] Loss: 0.014987
Train Epoch: 17 [12800/17010 (75%)] Loss: 0.069135
Train Epoch: 17 [12960/17010 (76%)] Loss: 0.062285
Train Epoch: 17 [13120/17010 (77%)] Loss: 0.006463
Train Epoch: 17 [13280/17010 (78%)] Loss: 0.048449
Train Epoch: 17 [13440/17010 (79%)] Loss: 0.083353
Train Epoch: 17 [13600/17010 (80%)] Loss: 0.182006
Train Epoch: 17 [13760/17010 (81%)] Loss: 0.182003
Train Epoch: 17 [13920/17010 (82%)] Loss: 0.061997
Train Epoch: 17 [14080/17010 (83%)] Loss: 0.011185
Train Epoch: 17 [14240/17010 (84%)] Loss: 0.101760
Train Epoch: 17 [14400/17010 (85%)] Loss: 0.023304
Train Epoch: 17 [14560/17010 (86%)] Loss: 0.094703
Train Epoch: 17 [14720/17010 (87%)] Loss: 0.114890
Train Epoch: 17 [14880/17010 (87%)] Loss: 0.061082
Train Epoch: 17 [15040/17010 (88%)] Loss: 0.011940
Train Epoch: 17 [15200/17010 (89%)] Loss: 0.102379
Train Epoch: 17 [15360/17010 (90%)] Loss: 0.099870
Train Epoch: 17 [15520/17010 (91%)] Loss: 0.011015
Train Epoch: 17 [15680/17010 (92%)] Loss: 0.183484
Train Epoch: 17 [15840/17010 (93%)] Loss: 0.079514
Train Epoch: 17 [16000/17010 (94%)] Loss: 0.110569
Train Epoch: 17 [16160/17010 (95%)] Loss: 0.093933
Train Epoch: 17 [16320/17010 (96%)] Loss: 0.158104
Train Epoch: 17 [16480/17010 (97%)] Loss: 0.334176
Train Epoch: 17 [16640/17010 (98%)] Loss: 0.089191
Train Epoch: 17 [16800/17010 (99%)] Loss: 0.052956
Train Epoch: 17 [16960/17010 (100%)] Loss: 0.027771
    epoch          : 17
    Train_loss     : 0.0817590486212315
    Train_accuracy : 0.9732860797827904
    Train_top_k_acc: 0.9994125939849624
    Val_loss       : 0.2339718056357621
    Val_accuracy   : 0.9348958333333334
    Val_top_k_acc  : 0.9947916666666666
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch17.pth ...
Train Epoch: 18 [0/17010 (0%)] Loss: 0.061157
Train Epoch: 18 [160/17010 (1%)] Loss: 0.019459
Train Epoch: 18 [320/17010 (2%)] Loss: 0.102290
Train Epoch: 18 [480/17010 (3%)] Loss: 0.014995
Train Epoch: 18 [640/17010 (4%)] Loss: 0.220091
Train Epoch: 18 [800/17010 (5%)] Loss: 0.016553
Train Epoch: 18 [960/17010 (6%)] Loss: 0.018505
Train Epoch: 18 [1120/17010 (7%)] Loss: 0.004455
Train Epoch: 18 [1280/17010 (8%)] Loss: 0.049322
Train Epoch: 18 [1440/17010 (8%)] Loss: 0.062700
Train Epoch: 18 [1600/17010 (9%)] Loss: 0.013030
Train Epoch: 18 [1760/17010 (10%)] Loss: 0.159545
Train Epoch: 18 [1920/17010 (11%)] Loss: 0.035342
Train Epoch: 18 [2080/17010 (12%)] Loss: 0.101794
Train Epoch: 18 [2240/17010 (13%)] Loss: 0.184137
Train Epoch: 18 [2400/17010 (14%)] Loss: 0.050781
Train Epoch: 18 [2560/17010 (15%)] Loss: 0.014014
Train Epoch: 18 [2720/17010 (16%)] Loss: 0.119355
Train Epoch: 18 [2880/17010 (17%)] Loss: 0.018665
Train Epoch: 18 [3040/17010 (18%)] Loss: 0.049608
Train Epoch: 18 [3200/17010 (19%)] Loss: 0.145718
Train Epoch: 18 [3360/17010 (20%)] Loss: 0.010227
Train Epoch: 18 [3520/17010 (21%)] Loss: 0.017721
Train Epoch: 18 [3680/17010 (22%)] Loss: 0.080766
Train Epoch: 18 [3840/17010 (23%)] Loss: 0.051345
Train Epoch: 18 [4000/17010 (24%)] Loss: 0.119673
Train Epoch: 18 [4160/17010 (24%)] Loss: 0.040700
Train Epoch: 18 [4320/17010 (25%)] Loss: 0.102260
Train Epoch: 18 [4480/17010 (26%)] Loss: 0.046061
Train Epoch: 18 [4640/17010 (27%)] Loss: 0.013530
Train Epoch: 18 [4800/17010 (28%)] Loss: 0.064937
Train Epoch: 18 [4960/17010 (29%)] Loss: 0.004904
Train Epoch: 18 [5120/17010 (30%)] Loss: 0.013845
Train Epoch: 18 [5280/17010 (31%)] Loss: 0.114665
Train Epoch: 18 [5440/17010 (32%)] Loss: 0.014038
Train Epoch: 18 [5600/17010 (33%)] Loss: 0.186844
Train Epoch: 18 [5760/17010 (34%)] Loss: 0.094493
Train Epoch: 18 [5920/17010 (35%)] Loss: 0.032647
Train Epoch: 18 [6080/17010 (36%)] Loss: 0.102881
Train Epoch: 18 [6240/17010 (37%)] Loss: 0.076961
Train Epoch: 18 [6400/17010 (38%)] Loss: 0.463450
Train Epoch: 18 [6560/17010 (39%)] Loss: 0.186749
Train Epoch: 18 [6720/17010 (40%)] Loss: 0.029826
Train Epoch: 18 [6880/17010 (40%)] Loss: 0.027790
Train Epoch: 18 [7040/17010 (41%)] Loss: 0.096529
Train Epoch: 18 [7200/17010 (42%)] Loss: 0.027184
Train Epoch: 18 [7360/17010 (43%)] Loss: 0.154842
Train Epoch: 18 [7520/17010 (44%)] Loss: 0.082101
Train Epoch: 18 [7680/17010 (45%)] Loss: 0.012093
Train Epoch: 18 [7840/17010 (46%)] Loss: 0.013589
Train Epoch: 18 [8000/17010 (47%)] Loss: 0.011674
Train Epoch: 18 [8160/17010 (48%)] Loss: 0.109742
Train Epoch: 18 [8320/17010 (49%)] Loss: 0.115851
Train Epoch: 18 [8480/17010 (50%)] Loss: 0.004460
Train Epoch: 18 [8640/17010 (51%)] Loss: 0.042373
Train Epoch: 18 [8800/17010 (52%)] Loss: 0.021834
Train Epoch: 18 [8960/17010 (53%)] Loss: 0.009286
Train Epoch: 18 [9120/17010 (54%)] Loss: 0.171809
Train Epoch: 18 [9280/17010 (55%)] Loss: 0.005140
Train Epoch: 18 [9440/17010 (55%)] Loss: 0.088974
Train Epoch: 18 [9600/17010 (56%)] Loss: 0.046900
Train Epoch: 18 [9760/17010 (57%)] Loss: 0.025399
Train Epoch: 18 [9920/17010 (58%)] Loss: 0.010865
Train Epoch: 18 [10080/17010 (59%)] Loss: 0.042749
Train Epoch: 18 [10240/17010 (60%)] Loss: 0.015244
Train Epoch: 18 [10400/17010 (61%)] Loss: 0.004480
Train Epoch: 18 [10560/17010 (62%)] Loss: 0.066927
Train Epoch: 18 [10720/17010 (63%)] Loss: 0.004465
Train Epoch: 18 [10880/17010 (64%)] Loss: 0.250328
Train Epoch: 18 [11040/17010 (65%)] Loss: 0.187326
Train Epoch: 18 [11200/17010 (66%)] Loss: 0.002247
Train Epoch: 18 [11360/17010 (67%)] Loss: 0.158451
Train Epoch: 18 [11520/17010 (68%)] Loss: 0.038720
Train Epoch: 18 [11680/17010 (69%)] Loss: 0.012602
Train Epoch: 18 [11840/17010 (70%)] Loss: 0.013061
Train Epoch: 18 [12000/17010 (71%)] Loss: 0.006591
Train Epoch: 18 [12160/17010 (71%)] Loss: 0.006231
Train Epoch: 18 [12320/17010 (72%)] Loss: 0.032802
Train Epoch: 18 [12480/17010 (73%)] Loss: 0.003807
Train Epoch: 18 [12640/17010 (74%)] Loss: 0.031527
Train Epoch: 18 [12800/17010 (75%)] Loss: 0.004955
Train Epoch: 18 [12960/17010 (76%)] Loss: 0.170366
Train Epoch: 18 [13120/17010 (77%)] Loss: 0.008164
Train Epoch: 18 [13280/17010 (78%)] Loss: 0.013598
Train Epoch: 18 [13440/17010 (79%)] Loss: 0.008154
Train Epoch: 18 [13600/17010 (80%)] Loss: 0.033274
Train Epoch: 18 [13760/17010 (81%)] Loss: 0.014623
Train Epoch: 18 [13920/17010 (82%)] Loss: 0.012498
Train Epoch: 18 [14080/17010 (83%)] Loss: 0.096158
Train Epoch: 18 [14240/17010 (84%)] Loss: 0.052013
Train Epoch: 18 [14400/17010 (85%)] Loss: 0.027729
Train Epoch: 18 [14560/17010 (86%)] Loss: 0.083238
Train Epoch: 18 [14720/17010 (87%)] Loss: 0.054414
Train Epoch: 18 [14880/17010 (87%)] Loss: 0.008942
Train Epoch: 18 [15040/17010 (88%)] Loss: 0.034792
Train Epoch: 18 [15200/17010 (89%)] Loss: 0.060244
Train Epoch: 18 [15360/17010 (90%)] Loss: 0.006656
Train Epoch: 18 [15520/17010 (91%)] Loss: 0.041556
Train Epoch: 18 [15680/17010 (92%)] Loss: 0.014011
Train Epoch: 18 [15840/17010 (93%)] Loss: 0.024033
Train Epoch: 18 [16000/17010 (94%)] Loss: 0.007165
Train Epoch: 18 [16160/17010 (95%)] Loss: 0.055527
Train Epoch: 18 [16320/17010 (96%)] Loss: 0.079858
Train Epoch: 18 [16480/17010 (97%)] Loss: 0.011723
Train Epoch: 18 [16640/17010 (98%)] Loss: 0.058760
Train Epoch: 18 [16800/17010 (99%)] Loss: 0.021287
Train Epoch: 18 [16960/17010 (100%)] Loss: 0.030068
    epoch          : 18
    Train_loss     : 0.05724645223025639
    Train_accuracy : 0.9805568609022557
    Train_top_k_acc: 0.9997062969924813
    Val_loss       : 0.2922268863922606
    Val_accuracy   : 0.9140625
    Val_top_k_acc  : 0.990625
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch18.pth ...
Train Epoch: 19 [0/17010 (0%)] Loss: 0.000771
Train Epoch: 19 [160/17010 (1%)] Loss: 0.057887
Train Epoch: 19 [320/17010 (2%)] Loss: 0.020528
Train Epoch: 19 [480/17010 (3%)] Loss: 0.007388
Train Epoch: 19 [640/17010 (4%)] Loss: 0.135037
Train Epoch: 19 [800/17010 (5%)] Loss: 0.013174
Train Epoch: 19 [960/17010 (6%)] Loss: 0.009051
Train Epoch: 19 [1120/17010 (7%)] Loss: 0.005230
Train Epoch: 19 [1280/17010 (8%)] Loss: 0.166741
Train Epoch: 19 [1440/17010 (8%)] Loss: 0.003943
Train Epoch: 19 [1600/17010 (9%)] Loss: 0.006690
Train Epoch: 19 [1760/17010 (10%)] Loss: 0.016544
Train Epoch: 19 [1920/17010 (11%)] Loss: 0.017973
Train Epoch: 19 [2080/17010 (12%)] Loss: 0.003270
Train Epoch: 19 [2240/17010 (13%)] Loss: 0.016921
Train Epoch: 19 [2400/17010 (14%)] Loss: 0.004423
Train Epoch: 19 [2560/17010 (15%)] Loss: 0.074472
Train Epoch: 19 [2720/17010 (16%)] Loss: 0.007289
Train Epoch: 19 [2880/17010 (17%)] Loss: 0.038020
Train Epoch: 19 [3040/17010 (18%)] Loss: 0.166033
Train Epoch: 19 [3200/17010 (19%)] Loss: 0.069741
Train Epoch: 19 [3360/17010 (20%)] Loss: 0.220116
Train Epoch: 19 [3520/17010 (21%)] Loss: 0.027033
Train Epoch: 19 [3680/17010 (22%)] Loss: 0.010694
Train Epoch: 19 [3840/17010 (23%)] Loss: 0.056459
Train Epoch: 19 [4000/17010 (24%)] Loss: 0.117396
Train Epoch: 19 [4160/17010 (24%)] Loss: 0.084256
Train Epoch: 19 [4320/17010 (25%)] Loss: 0.131048
Train Epoch: 19 [4480/17010 (26%)] Loss: 0.093567
Train Epoch: 19 [4640/17010 (27%)] Loss: 0.030385
Train Epoch: 19 [4800/17010 (28%)] Loss: 0.120293
Train Epoch: 19 [4960/17010 (29%)] Loss: 0.025551
Train Epoch: 19 [5120/17010 (30%)] Loss: 0.004627
Train Epoch: 19 [5280/17010 (31%)] Loss: 0.074449
Train Epoch: 19 [5440/17010 (32%)] Loss: 0.085773
Train Epoch: 19 [5600/17010 (33%)] Loss: 0.087452
Train Epoch: 19 [5760/17010 (34%)] Loss: 0.052916
Train Epoch: 19 [5920/17010 (35%)] Loss: 0.021816
Train Epoch: 19 [6080/17010 (36%)] Loss: 0.012577
Train Epoch: 19 [6240/17010 (37%)] Loss: 0.013246
Train Epoch: 19 [6400/17010 (38%)] Loss: 0.008527
Train Epoch: 19 [6560/17010 (39%)] Loss: 0.119666
Train Epoch: 19 [6720/17010 (40%)] Loss: 0.006152
Train Epoch: 19 [6880/17010 (40%)] Loss: 0.014742
Train Epoch: 19 [7040/17010 (41%)] Loss: 0.009978
Train Epoch: 19 [7200/17010 (42%)] Loss: 0.004508
Train Epoch: 19 [7360/17010 (43%)] Loss: 0.082742
Train Epoch: 19 [7520/17010 (44%)] Loss: 0.039655
Train Epoch: 19 [7680/17010 (45%)] Loss: 0.007920
Train Epoch: 19 [7840/17010 (46%)] Loss: 0.112511
Train Epoch: 19 [8000/17010 (47%)] Loss: 0.035634
Train Epoch: 19 [8160/17010 (48%)] Loss: 0.016796
Train Epoch: 19 [8320/17010 (49%)] Loss: 0.011981
Train Epoch: 19 [8480/17010 (50%)] Loss: 0.106778
Train Epoch: 19 [8640/17010 (51%)] Loss: 0.027188
Train Epoch: 19 [8800/17010 (52%)] Loss: 0.077434
Train Epoch: 19 [8960/17010 (53%)] Loss: 0.004375
Train Epoch: 19 [9120/17010 (54%)] Loss: 0.011785
Train Epoch: 19 [9280/17010 (55%)] Loss: 0.156629
Train Epoch: 19 [9440/17010 (55%)] Loss: 0.019615
Train Epoch: 19 [9600/17010 (56%)] Loss: 0.006887
Train Epoch: 19 [9760/17010 (57%)] Loss: 0.056098
Train Epoch: 19 [9920/17010 (58%)] Loss: 0.004234
Train Epoch: 19 [10080/17010 (59%)] Loss: 0.003909
Train Epoch: 19 [10240/17010 (60%)] Loss: 0.002016
Train Epoch: 19 [10400/17010 (61%)] Loss: 0.117791
Train Epoch: 19 [10560/17010 (62%)] Loss: 0.011968
Train Epoch: 19 [10720/17010 (63%)] Loss: 0.097377
Train Epoch: 19 [10880/17010 (64%)] Loss: 0.010679
Train Epoch: 19 [11040/17010 (65%)] Loss: 0.003321
Train Epoch: 19 [11200/17010 (66%)] Loss: 0.109091
Train Epoch: 19 [11360/17010 (67%)] Loss: 0.123343
Train Epoch: 19 [11520/17010 (68%)] Loss: 0.010314
Train Epoch: 19 [11680/17010 (69%)] Loss: 0.034489
Train Epoch: 19 [11840/17010 (70%)] Loss: 0.076920
Train Epoch: 19 [12000/17010 (71%)] Loss: 0.036372
Train Epoch: 19 [12160/17010 (71%)] Loss: 0.088720
Train Epoch: 19 [12320/17010 (72%)] Loss: 0.160892
Train Epoch: 19 [12480/17010 (73%)] Loss: 0.091738
Train Epoch: 19 [12640/17010 (74%)] Loss: 0.009290
Train Epoch: 19 [12800/17010 (75%)] Loss: 0.069170
Train Epoch: 19 [12960/17010 (76%)] Loss: 0.006965
Train Epoch: 19 [13120/17010 (77%)] Loss: 0.024540
Train Epoch: 19 [13280/17010 (78%)] Loss: 0.167612
Train Epoch: 19 [13440/17010 (79%)] Loss: 0.012294
Train Epoch: 19 [13600/17010 (80%)] Loss: 0.006566
Train Epoch: 19 [13760/17010 (81%)] Loss: 0.182423
Train Epoch: 19 [13920/17010 (82%)] Loss: 0.015361
Train Epoch: 19 [14080/17010 (83%)] Loss: 0.046854
Train Epoch: 19 [14240/17010 (84%)] Loss: 0.076821
Train Epoch: 19 [14400/17010 (85%)] Loss: 0.035996
Train Epoch: 19 [14560/17010 (86%)] Loss: 0.095567
Train Epoch: 19 [14720/17010 (87%)] Loss: 0.107649
Train Epoch: 19 [14880/17010 (87%)] Loss: 0.022789
Train Epoch: 19 [15040/17010 (88%)] Loss: 0.016299
Train Epoch: 19 [15200/17010 (89%)] Loss: 0.005466
Train Epoch: 19 [15360/17010 (90%)] Loss: 0.003750
Train Epoch: 19 [15520/17010 (91%)] Loss: 0.008275
Train Epoch: 19 [15680/17010 (92%)] Loss: 0.043302
Train Epoch: 19 [15840/17010 (93%)] Loss: 0.006289
Train Epoch: 19 [16000/17010 (94%)] Loss: 0.137424
Train Epoch: 19 [16160/17010 (95%)] Loss: 0.006525
Train Epoch: 19 [16320/17010 (96%)] Loss: 0.020297
Train Epoch: 19 [16480/17010 (97%)] Loss: 0.020119
Train Epoch: 19 [16640/17010 (98%)] Loss: 0.146761
Train Epoch: 19 [16800/17010 (99%)] Loss: 0.100673
Train Epoch: 19 [16960/17010 (100%)] Loss: 0.002334
    epoch          : 19
    Train_loss     : 0.05969725909189468
    Train_accuracy : 0.9804981203007519
    Train_top_k_acc: 0.9997062969924813
    Val_loss       : 0.20858491799256929
    Val_accuracy   : 0.9432291666666667
    Val_top_k_acc  : 0.9958333333333333
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch19.pth ...
Train Epoch: 20 [0/17010 (0%)] Loss: 0.020678
Train Epoch: 20 [160/17010 (1%)] Loss: 0.298561
Train Epoch: 20 [320/17010 (2%)] Loss: 0.069267
Train Epoch: 20 [480/17010 (3%)] Loss: 0.032480
Train Epoch: 20 [640/17010 (4%)] Loss: 0.074240
Train Epoch: 20 [800/17010 (5%)] Loss: 0.035751
Train Epoch: 20 [960/17010 (6%)] Loss: 0.004672
Train Epoch: 20 [1120/17010 (7%)] Loss: 0.054513
Train Epoch: 20 [1280/17010 (8%)] Loss: 0.055583
Train Epoch: 20 [1440/17010 (8%)] Loss: 0.151564
Train Epoch: 20 [1600/17010 (9%)] Loss: 0.014601
Train Epoch: 20 [1760/17010 (10%)] Loss: 0.008358
Train Epoch: 20 [1920/17010 (11%)] Loss: 0.056594
Train Epoch: 20 [2080/17010 (12%)] Loss: 0.279017
Train Epoch: 20 [2240/17010 (13%)] Loss: 0.108102
Train Epoch: 20 [2400/17010 (14%)] Loss: 0.108332
Train Epoch: 20 [2560/17010 (15%)] Loss: 0.002918
Train Epoch: 20 [2720/17010 (16%)] Loss: 0.168353
Train Epoch: 20 [2880/17010 (17%)] Loss: 0.009026
Train Epoch: 20 [3040/17010 (18%)] Loss: 0.112505
Train Epoch: 20 [3200/17010 (19%)] Loss: 0.009040
Train Epoch: 20 [3360/17010 (20%)] Loss: 0.076500
Train Epoch: 20 [3520/17010 (21%)] Loss: 0.016432
Train Epoch: 20 [3680/17010 (22%)] Loss: 0.122108
Train Epoch: 20 [3840/17010 (23%)] Loss: 0.008905
Train Epoch: 20 [4000/17010 (24%)] Loss: 0.043501
Train Epoch: 20 [4160/17010 (24%)] Loss: 0.018083
Train Epoch: 20 [4320/17010 (25%)] Loss: 0.005002
Train Epoch: 20 [4480/17010 (26%)] Loss: 0.029923
Train Epoch: 20 [4640/17010 (27%)] Loss: 0.071260
Train Epoch: 20 [4800/17010 (28%)] Loss: 0.031641
Train Epoch: 20 [4960/17010 (29%)] Loss: 0.029079
Train Epoch: 20 [5120/17010 (30%)] Loss: 0.012118
Train Epoch: 20 [5280/17010 (31%)] Loss: 0.192572
Train Epoch: 20 [5440/17010 (32%)] Loss: 0.012902
Train Epoch: 20 [5600/17010 (33%)] Loss: 0.072711
Train Epoch: 20 [5760/17010 (34%)] Loss: 0.060613
Train Epoch: 20 [5920/17010 (35%)] Loss: 0.004771
Train Epoch: 20 [6080/17010 (36%)] Loss: 0.151310
Train Epoch: 20 [6240/17010 (37%)] Loss: 0.224139
Train Epoch: 20 [6400/17010 (38%)] Loss: 0.103493
Train Epoch: 20 [6560/17010 (39%)] Loss: 0.031981
Train Epoch: 20 [6720/17010 (40%)] Loss: 0.117786
Train Epoch: 20 [6880/17010 (40%)] Loss: 0.312361
Train Epoch: 20 [7040/17010 (41%)] Loss: 0.158906
Train Epoch: 20 [7200/17010 (42%)] Loss: 0.035615
Train Epoch: 20 [7360/17010 (43%)] Loss: 0.076510
Train Epoch: 20 [7520/17010 (44%)] Loss: 0.164535
Train Epoch: 20 [7680/17010 (45%)] Loss: 0.021406
Train Epoch: 20 [7840/17010 (46%)] Loss: 0.106489
Train Epoch: 20 [8000/17010 (47%)] Loss: 0.007615
Train Epoch: 20 [8160/17010 (48%)] Loss: 0.010824
Train Epoch: 20 [8320/17010 (49%)] Loss: 0.142247
Train Epoch: 20 [8480/17010 (50%)] Loss: 0.007867
Train Epoch: 20 [8640/17010 (51%)] Loss: 0.012934
Train Epoch: 20 [8800/17010 (52%)] Loss: 0.005648
Train Epoch: 20 [8960/17010 (53%)] Loss: 0.181468
Train Epoch: 20 [9120/17010 (54%)] Loss: 0.049419
Train Epoch: 20 [9280/17010 (55%)] Loss: 0.068849
Train Epoch: 20 [9440/17010 (55%)] Loss: 0.186306
Train Epoch: 20 [9600/17010 (56%)] Loss: 0.105755
Train Epoch: 20 [9760/17010 (57%)] Loss: 0.023878
Train Epoch: 20 [9920/17010 (58%)] Loss: 0.127824
Train Epoch: 20 [10080/17010 (59%)] Loss: 0.034054
Train Epoch: 20 [10240/17010 (60%)] Loss: 0.003195
Train Epoch: 20 [10400/17010 (61%)] Loss: 0.009027
Train Epoch: 20 [10560/17010 (62%)] Loss: 0.053794
Train Epoch: 20 [10720/17010 (63%)] Loss: 0.230335
Train Epoch: 20 [10880/17010 (64%)] Loss: 0.230227
Train Epoch: 20 [11040/17010 (65%)] Loss: 0.078209
Train Epoch: 20 [11200/17010 (66%)] Loss: 0.016164
Train Epoch: 20 [11360/17010 (67%)] Loss: 0.107745
Train Epoch: 20 [11520/17010 (68%)] Loss: 0.016889
Train Epoch: 20 [11680/17010 (69%)] Loss: 0.225620
Train Epoch: 20 [11840/17010 (70%)] Loss: 0.006728
Train Epoch: 20 [12000/17010 (71%)] Loss: 0.026533
Train Epoch: 20 [12160/17010 (71%)] Loss: 0.012870
Train Epoch: 20 [12320/17010 (72%)] Loss: 0.053646
Train Epoch: 20 [12480/17010 (73%)] Loss: 0.032516
Train Epoch: 20 [12640/17010 (74%)] Loss: 0.033873
Train Epoch: 20 [12800/17010 (75%)] Loss: 0.016334
Train Epoch: 20 [12960/17010 (76%)] Loss: 0.116950
Train Epoch: 20 [13120/17010 (77%)] Loss: 0.041289
Train Epoch: 20 [13280/17010 (78%)] Loss: 0.078177
Train Epoch: 20 [13440/17010 (79%)] Loss: 0.011710
Train Epoch: 20 [13600/17010 (80%)] Loss: 0.216721
Train Epoch: 20 [13760/17010 (81%)] Loss: 0.082493
Train Epoch: 20 [13920/17010 (82%)] Loss: 0.085926
Train Epoch: 20 [14080/17010 (83%)] Loss: 0.050147
Train Epoch: 20 [14240/17010 (84%)] Loss: 0.173790
Train Epoch: 20 [14400/17010 (85%)] Loss: 0.027386
Train Epoch: 20 [14560/17010 (86%)] Loss: 0.236947
Train Epoch: 20 [14720/17010 (87%)] Loss: 0.107174
Train Epoch: 20 [14880/17010 (87%)] Loss: 0.010866
Train Epoch: 20 [15040/17010 (88%)] Loss: 0.023303
Train Epoch: 20 [15200/17010 (89%)] Loss: 0.010478
Train Epoch: 20 [15360/17010 (90%)] Loss: 0.047607
Train Epoch: 20 [15520/17010 (91%)] Loss: 0.069601
Train Epoch: 20 [15680/17010 (92%)] Loss: 0.129192
Train Epoch: 20 [15840/17010 (93%)] Loss: 0.075775
Train Epoch: 20 [16000/17010 (94%)] Loss: 0.031139
Train Epoch: 20 [16160/17010 (95%)] Loss: 0.133780
Train Epoch: 20 [16320/17010 (96%)] Loss: 0.028660
Train Epoch: 20 [16480/17010 (97%)] Loss: 0.036004
Train Epoch: 20 [16640/17010 (98%)] Loss: 0.003652
Train Epoch: 20 [16800/17010 (99%)] Loss: 0.007992
Train Epoch: 20 [16960/17010 (100%)] Loss: 0.014755
    epoch          : 20
    Train_loss     : 0.06438992896442883
    Train_accuracy : 0.9794407894736842
    Train_top_k_acc: 0.9997650375939849
    Val_loss       : 0.1183240352353702
    Val_accuracy   : 0.9578125
    Val_top_k_acc  : 0.9989583333333333
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch20.pth ...
Train Epoch: 21 [0/17010 (0%)] Loss: 0.026652
Train Epoch: 21 [160/17010 (1%)] Loss: 0.004053
Train Epoch: 21 [320/17010 (2%)] Loss: 0.045696
Train Epoch: 21 [480/17010 (3%)] Loss: 0.038134
Train Epoch: 21 [640/17010 (4%)] Loss: 0.011590
Train Epoch: 21 [800/17010 (5%)] Loss: 0.008578
Train Epoch: 21 [960/17010 (6%)] Loss: 0.075013
Train Epoch: 21 [1120/17010 (7%)] Loss: 0.100881
Train Epoch: 21 [1280/17010 (8%)] Loss: 0.021388
Train Epoch: 21 [1440/17010 (8%)] Loss: 0.365042
Train Epoch: 21 [1600/17010 (9%)] Loss: 0.034813
Train Epoch: 21 [1760/17010 (10%)] Loss: 0.013371
Train Epoch: 21 [1920/17010 (11%)] Loss: 0.016081
Train Epoch: 21 [2080/17010 (12%)] Loss: 0.016465
Train Epoch: 21 [2240/17010 (13%)] Loss: 0.028911
Train Epoch: 21 [2400/17010 (14%)] Loss: 0.113501
Train Epoch: 21 [2560/17010 (15%)] Loss: 0.071585
Train Epoch: 21 [2720/17010 (16%)] Loss: 0.006392
Train Epoch: 21 [2880/17010 (17%)] Loss: 0.031365
Train Epoch: 21 [3040/17010 (18%)] Loss: 0.005622
Train Epoch: 21 [3200/17010 (19%)] Loss: 0.011451
Train Epoch: 21 [3360/17010 (20%)] Loss: 0.009462
Train Epoch: 21 [3520/17010 (21%)] Loss: 0.003317
Train Epoch: 21 [3680/17010 (22%)] Loss: 0.090804
Train Epoch: 21 [3840/17010 (23%)] Loss: 0.015016
Train Epoch: 21 [4000/17010 (24%)] Loss: 0.004701
Train Epoch: 21 [4160/17010 (24%)] Loss: 0.002962
Train Epoch: 21 [4320/17010 (25%)] Loss: 0.037130
Train Epoch: 21 [4480/17010 (26%)] Loss: 0.011808
Train Epoch: 21 [4640/17010 (27%)] Loss: 0.004303
Train Epoch: 21 [4800/17010 (28%)] Loss: 0.013669
Train Epoch: 21 [4960/17010 (29%)] Loss: 0.005322
Train Epoch: 21 [5120/17010 (30%)] Loss: 0.000643
Train Epoch: 21 [5280/17010 (31%)] Loss: 0.004127
Train Epoch: 21 [5440/17010 (32%)] Loss: 0.003503
Train Epoch: 21 [5600/17010 (33%)] Loss: 0.005297
Train Epoch: 21 [5760/17010 (34%)] Loss: 0.020324
Train Epoch: 21 [5920/17010 (35%)] Loss: 0.010987
Train Epoch: 21 [6080/17010 (36%)] Loss: 0.003735
Train Epoch: 21 [6240/17010 (37%)] Loss: 0.143838
Train Epoch: 21 [6400/17010 (38%)] Loss: 0.002792
Train Epoch: 21 [6560/17010 (39%)] Loss: 0.003245
Train Epoch: 21 [6720/17010 (40%)] Loss: 0.012515
Train Epoch: 21 [6880/17010 (40%)] Loss: 0.026882
Train Epoch: 21 [7040/17010 (41%)] Loss: 0.008426
Train Epoch: 21 [7200/17010 (42%)] Loss: 0.123737
Train Epoch: 21 [7360/17010 (43%)] Loss: 0.010531
Train Epoch: 21 [7520/17010 (44%)] Loss: 0.012722
Train Epoch: 21 [7680/17010 (45%)] Loss: 0.010155
Train Epoch: 21 [7840/17010 (46%)] Loss: 0.115004
Train Epoch: 21 [8000/17010 (47%)] Loss: 0.014919
Train Epoch: 21 [8160/17010 (48%)] Loss: 0.023425
Train Epoch: 21 [8320/17010 (49%)] Loss: 0.002985
Train Epoch: 21 [8480/17010 (50%)] Loss: 0.023045
Train Epoch: 21 [8640/17010 (51%)] Loss: 0.011045
Train Epoch: 21 [8800/17010 (52%)] Loss: 0.044675
Train Epoch: 21 [8960/17010 (53%)] Loss: 0.009885
Train Epoch: 21 [9120/17010 (54%)] Loss: 0.025413
Train Epoch: 21 [9280/17010 (55%)] Loss: 0.003690
Train Epoch: 21 [9440/17010 (55%)] Loss: 0.003366
Train Epoch: 21 [9600/17010 (56%)] Loss: 0.004883
Train Epoch: 21 [9760/17010 (57%)] Loss: 0.093555
Train Epoch: 21 [9920/17010 (58%)] Loss: 0.044922
Train Epoch: 21 [10080/17010 (59%)] Loss: 0.035893
Train Epoch: 21 [10240/17010 (60%)] Loss: 0.020916
Train Epoch: 21 [10400/17010 (61%)] Loss: 0.001269
Train Epoch: 21 [10560/17010 (62%)] Loss: 0.010316
Train Epoch: 21 [10720/17010 (63%)] Loss: 0.006191
Train Epoch: 21 [10880/17010 (64%)] Loss: 0.032372
Train Epoch: 21 [11040/17010 (65%)] Loss: 0.007837
Train Epoch: 21 [11200/17010 (66%)] Loss: 0.002727
Train Epoch: 21 [11360/17010 (67%)] Loss: 0.030046
Train Epoch: 21 [11520/17010 (68%)] Loss: 0.001381
Train Epoch: 21 [11680/17010 (69%)] Loss: 0.034161
Train Epoch: 21 [11840/17010 (70%)] Loss: 0.236244
Train Epoch: 21 [12000/17010 (71%)] Loss: 0.001424
Train Epoch: 21 [12160/17010 (71%)] Loss: 0.006855
Train Epoch: 21 [12320/17010 (72%)] Loss: 0.027196
Train Epoch: 21 [12480/17010 (73%)] Loss: 0.018209
Train Epoch: 21 [12640/17010 (74%)] Loss: 0.011362
Train Epoch: 21 [12800/17010 (75%)] Loss: 0.086695
Train Epoch: 21 [12960/17010 (76%)] Loss: 0.031298
Train Epoch: 21 [13120/17010 (77%)] Loss: 0.217902
Train Epoch: 21 [13280/17010 (78%)] Loss: 0.057540
Train Epoch: 21 [13440/17010 (79%)] Loss: 0.098558
Train Epoch: 21 [13600/17010 (80%)] Loss: 0.055761
Train Epoch: 21 [13760/17010 (81%)] Loss: 0.060279
Train Epoch: 21 [13920/17010 (82%)] Loss: 0.010917
Train Epoch: 21 [14080/17010 (83%)] Loss: 0.070558
Train Epoch: 21 [14240/17010 (84%)] Loss: 0.042791
Train Epoch: 21 [14400/17010 (85%)] Loss: 0.003489
Train Epoch: 21 [14560/17010 (86%)] Loss: 0.009961
Train Epoch: 21 [14720/17010 (87%)] Loss: 0.060239
Train Epoch: 21 [14880/17010 (87%)] Loss: 0.009095
Train Epoch: 21 [15040/17010 (88%)] Loss: 0.007138
Train Epoch: 21 [15200/17010 (89%)] Loss: 0.008944
Train Epoch: 21 [15360/17010 (90%)] Loss: 0.013885
Train Epoch: 21 [15520/17010 (91%)] Loss: 0.029629
Train Epoch: 21 [15680/17010 (92%)] Loss: 0.038425
Train Epoch: 21 [15840/17010 (93%)] Loss: 0.004865
Train Epoch: 21 [16000/17010 (94%)] Loss: 0.020790
Train Epoch: 21 [16160/17010 (95%)] Loss: 0.037874
Train Epoch: 21 [16320/17010 (96%)] Loss: 0.007523
Train Epoch: 21 [16480/17010 (97%)] Loss: 0.008434
Train Epoch: 21 [16640/17010 (98%)] Loss: 0.108556
Train Epoch: 21 [16800/17010 (99%)] Loss: 0.002642
Train Epoch: 21 [16960/17010 (100%)] Loss: 0.040077
    epoch          : 21
    Train_loss     : 0.035784073054192664
    Train_accuracy : 0.9884281015037594
    Train_top_k_acc: 0.9998825187969925
    Val_loss       : 0.16814133207080886
    Val_accuracy   : 0.9546875
    Val_top_k_acc  : 0.9984375
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch21.pth ...
Train Epoch: 22 [0/17010 (0%)] Loss: 0.020336
Train Epoch: 22 [160/17010 (1%)] Loss: 0.006746
Train Epoch: 22 [320/17010 (2%)] Loss: 0.043950
Train Epoch: 22 [480/17010 (3%)] Loss: 0.012877
Train Epoch: 22 [640/17010 (4%)] Loss: 0.031586
Train Epoch: 22 [800/17010 (5%)] Loss: 0.024039
Train Epoch: 22 [960/17010 (6%)] Loss: 0.032336
Train Epoch: 22 [1120/17010 (7%)] Loss: 0.120616
Train Epoch: 22 [1280/17010 (8%)] Loss: 0.022150
Train Epoch: 22 [1440/17010 (8%)] Loss: 0.256467
Train Epoch: 22 [1600/17010 (9%)] Loss: 0.001572
Train Epoch: 22 [1760/17010 (10%)] Loss: 0.036062
Train Epoch: 22 [1920/17010 (11%)] Loss: 0.032169
Train Epoch: 22 [2080/17010 (12%)] Loss: 0.066719
Train Epoch: 22 [2240/17010 (13%)] Loss: 0.165446
Train Epoch: 22 [2400/17010 (14%)] Loss: 0.027044
Train Epoch: 22 [2560/17010 (15%)] Loss: 0.186884
Train Epoch: 22 [2720/17010 (16%)] Loss: 0.026920
Train Epoch: 22 [2880/17010 (17%)] Loss: 0.029951
Train Epoch: 22 [3040/17010 (18%)] Loss: 0.019389
Train Epoch: 22 [3200/17010 (19%)] Loss: 0.111455
Train Epoch: 22 [3360/17010 (20%)] Loss: 0.011203
Train Epoch: 22 [3520/17010 (21%)] Loss: 0.125511
Train Epoch: 22 [3680/17010 (22%)] Loss: 0.003304
Train Epoch: 22 [3840/17010 (23%)] Loss: 0.016709
Train Epoch: 22 [4000/17010 (24%)] Loss: 0.022665
Train Epoch: 22 [4160/17010 (24%)] Loss: 0.342523
Train Epoch: 22 [4320/17010 (25%)] Loss: 0.108611
Train Epoch: 22 [4480/17010 (26%)] Loss: 0.009732
Train Epoch: 22 [4640/17010 (27%)] Loss: 0.050998
Train Epoch: 22 [4800/17010 (28%)] Loss: 0.165042
Train Epoch: 22 [4960/17010 (29%)] Loss: 0.102112
Train Epoch: 22 [5120/17010 (30%)] Loss: 0.063295
Train Epoch: 22 [5280/17010 (31%)] Loss: 0.329933
Train Epoch: 22 [5440/17010 (32%)] Loss: 0.092263
Train Epoch: 22 [5600/17010 (33%)] Loss: 0.011911
Train Epoch: 22 [5760/17010 (34%)] Loss: 0.019460
Train Epoch: 22 [5920/17010 (35%)] Loss: 0.014804
Train Epoch: 22 [6080/17010 (36%)] Loss: 0.002909
Train Epoch: 22 [6240/17010 (37%)] Loss: 0.276114
Train Epoch: 22 [6400/17010 (38%)] Loss: 0.017176
Train Epoch: 22 [6560/17010 (39%)] Loss: 0.051037
Train Epoch: 22 [6720/17010 (40%)] Loss: 0.016777
Train Epoch: 22 [6880/17010 (40%)] Loss: 0.008270
Train Epoch: 22 [7040/17010 (41%)] Loss: 0.007900
Train Epoch: 22 [7200/17010 (42%)] Loss: 0.241270
Train Epoch: 22 [7360/17010 (43%)] Loss: 0.035888
Train Epoch: 22 [7520/17010 (44%)] Loss: 0.402367
Train Epoch: 22 [7680/17010 (45%)] Loss: 0.019352
Train Epoch: 22 [7840/17010 (46%)] Loss: 0.011918
Train Epoch: 22 [8000/17010 (47%)] Loss: 0.007499
Train Epoch: 22 [8160/17010 (48%)] Loss: 0.130241
Train Epoch: 22 [8320/17010 (49%)] Loss: 0.004831
Train Epoch: 22 [8480/17010 (50%)] Loss: 0.041187
Train Epoch: 22 [8640/17010 (51%)] Loss: 0.049576
Train Epoch: 22 [8800/17010 (52%)] Loss: 0.139278
Train Epoch: 22 [8960/17010 (53%)] Loss: 0.050634
Train Epoch: 22 [9120/17010 (54%)] Loss: 0.016684
Train Epoch: 22 [9280/17010 (55%)] Loss: 0.086101
Train Epoch: 22 [9440/17010 (55%)] Loss: 0.063929
Train Epoch: 22 [9600/17010 (56%)] Loss: 0.026812
Train Epoch: 22 [9760/17010 (57%)] Loss: 0.188871
Train Epoch: 22 [9920/17010 (58%)] Loss: 0.064572
Train Epoch: 22 [10080/17010 (59%)] Loss: 0.050168
Train Epoch: 22 [10240/17010 (60%)] Loss: 0.012500
Train Epoch: 22 [10400/17010 (61%)] Loss: 0.027334
Train Epoch: 22 [10560/17010 (62%)] Loss: 0.012343
Train Epoch: 22 [10720/17010 (63%)] Loss: 0.013040
Train Epoch: 22 [10880/17010 (64%)] Loss: 0.002988
Train Epoch: 22 [11040/17010 (65%)] Loss: 0.002658
Train Epoch: 22 [11200/17010 (66%)] Loss: 0.054436
Train Epoch: 22 [11360/17010 (67%)] Loss: 0.003774
Train Epoch: 22 [11520/17010 (68%)] Loss: 0.054273
Train Epoch: 22 [11680/17010 (69%)] Loss: 0.005454
Train Epoch: 22 [11840/17010 (70%)] Loss: 0.002618
Train Epoch: 22 [12000/17010 (71%)] Loss: 0.043776
Train Epoch: 22 [12160/17010 (71%)] Loss: 0.004089
Train Epoch: 22 [12320/17010 (72%)] Loss: 0.051843
Train Epoch: 22 [12480/17010 (73%)] Loss: 0.012867
Train Epoch: 22 [12640/17010 (74%)] Loss: 0.023220
Train Epoch: 22 [12800/17010 (75%)] Loss: 0.040669
Train Epoch: 22 [12960/17010 (76%)] Loss: 0.012660
Train Epoch: 22 [13120/17010 (77%)] Loss: 0.067532
Train Epoch: 22 [13280/17010 (78%)] Loss: 0.098092
Train Epoch: 22 [13440/17010 (79%)] Loss: 0.006205
Train Epoch: 22 [13600/17010 (80%)] Loss: 0.029544
Train Epoch: 22 [13760/17010 (81%)] Loss: 0.001549
Train Epoch: 22 [13920/17010 (82%)] Loss: 0.084609
Train Epoch: 22 [14080/17010 (83%)] Loss: 0.072410
Train Epoch: 22 [14240/17010 (84%)] Loss: 0.030217
Train Epoch: 22 [14400/17010 (85%)] Loss: 0.016959
Train Epoch: 22 [14560/17010 (86%)] Loss: 0.001740
Train Epoch: 22 [14720/17010 (87%)] Loss: 0.011591
Train Epoch: 22 [14880/17010 (87%)] Loss: 0.006549
Train Epoch: 22 [15040/17010 (88%)] Loss: 0.010561
Train Epoch: 22 [15200/17010 (89%)] Loss: 0.007476
Train Epoch: 22 [15360/17010 (90%)] Loss: 0.022239
Train Epoch: 22 [15520/17010 (91%)] Loss: 0.006026
Train Epoch: 22 [15680/17010 (92%)] Loss: 0.011943
Train Epoch: 22 [15840/17010 (93%)] Loss: 0.006994
Train Epoch: 22 [16000/17010 (94%)] Loss: 0.115971
Train Epoch: 22 [16160/17010 (95%)] Loss: 0.011600
Train Epoch: 22 [16320/17010 (96%)] Loss: 0.002051
Train Epoch: 22 [16480/17010 (97%)] Loss: 0.241002
Train Epoch: 22 [16640/17010 (98%)] Loss: 0.014204
Train Epoch: 22 [16800/17010 (99%)] Loss: 0.018582
Train Epoch: 22 [16960/17010 (100%)] Loss: 0.021314
    epoch          : 22
    Train_loss     : 0.05620709484897351
    Train_accuracy : 0.9805568609022557
    Train_top_k_acc: 0.9995300751879699
    Val_loss       : 0.23148039657389746
    Val_accuracy   : 0.9369791666666667
    Val_top_k_acc  : 0.9979166666666667
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch22.pth ...
Train Epoch: 23 [0/17010 (0%)] Loss: 0.041925
Train Epoch: 23 [160/17010 (1%)] Loss: 0.013227
Train Epoch: 23 [320/17010 (2%)] Loss: 0.006734
Train Epoch: 23 [480/17010 (3%)] Loss: 0.090372
Train Epoch: 23 [640/17010 (4%)] Loss: 0.051459
Train Epoch: 23 [800/17010 (5%)] Loss: 0.022066
Train Epoch: 23 [960/17010 (6%)] Loss: 0.110824
Train Epoch: 23 [1120/17010 (7%)] Loss: 0.090127
Train Epoch: 23 [1280/17010 (8%)] Loss: 0.068176
Train Epoch: 23 [1440/17010 (8%)] Loss: 0.013617
Train Epoch: 23 [1600/17010 (9%)] Loss: 0.006678
Train Epoch: 23 [1760/17010 (10%)] Loss: 0.024952
Train Epoch: 23 [1920/17010 (11%)] Loss: 0.033574
Train Epoch: 23 [2080/17010 (12%)] Loss: 0.055105
Train Epoch: 23 [2240/17010 (13%)] Loss: 0.006222
Train Epoch: 23 [2400/17010 (14%)] Loss: 0.009144
Train Epoch: 23 [2560/17010 (15%)] Loss: 0.025719
Train Epoch: 23 [2720/17010 (16%)] Loss: 0.025307
Train Epoch: 23 [2880/17010 (17%)] Loss: 0.007149
Train Epoch: 23 [3040/17010 (18%)] Loss: 0.011022
Train Epoch: 23 [3200/17010 (19%)] Loss: 0.263192
Train Epoch: 23 [3360/17010 (20%)] Loss: 0.033345
Train Epoch: 23 [3520/17010 (21%)] Loss: 0.005156
Train Epoch: 23 [3680/17010 (22%)] Loss: 0.027794
Train Epoch: 23 [3840/17010 (23%)] Loss: 0.023425
Train Epoch: 23 [4000/17010 (24%)] Loss: 0.003011
Train Epoch: 23 [4160/17010 (24%)] Loss: 0.019641
Train Epoch: 23 [4320/17010 (25%)] Loss: 0.001717
Train Epoch: 23 [4480/17010 (26%)] Loss: 0.015746
Train Epoch: 23 [4640/17010 (27%)] Loss: 0.021698
Train Epoch: 23 [4800/17010 (28%)] Loss: 0.020442
Train Epoch: 23 [4960/17010 (29%)] Loss: 0.001159
Train Epoch: 23 [5120/17010 (30%)] Loss: 0.014702
Train Epoch: 23 [5280/17010 (31%)] Loss: 0.002462
Train Epoch: 23 [5440/17010 (32%)] Loss: 0.010280
Train Epoch: 23 [5600/17010 (33%)] Loss: 0.001749
Train Epoch: 23 [5760/17010 (34%)] Loss: 0.042574
Train Epoch: 23 [5920/17010 (35%)] Loss: 0.011679
Train Epoch: 23 [6080/17010 (36%)] Loss: 0.022489
Train Epoch: 23 [6240/17010 (37%)] Loss: 0.028650
Train Epoch: 23 [6400/17010 (38%)] Loss: 0.041052
Train Epoch: 23 [6560/17010 (39%)] Loss: 0.002114
Train Epoch: 23 [6720/17010 (40%)] Loss: 0.005969
Train Epoch: 23 [6880/17010 (40%)] Loss: 0.016943
Train Epoch: 23 [7040/17010 (41%)] Loss: 0.001831
Train Epoch: 23 [7200/17010 (42%)] Loss: 0.031264
Train Epoch: 23 [7360/17010 (43%)] Loss: 0.047153
Train Epoch: 23 [7520/17010 (44%)] Loss: 0.015168
Train Epoch: 23 [7680/17010 (45%)] Loss: 0.005057
Train Epoch: 23 [7840/17010 (46%)] Loss: 0.011285
Train Epoch: 23 [8000/17010 (47%)] Loss: 0.016006
Train Epoch: 23 [8160/17010 (48%)] Loss: 0.004199
Train Epoch: 23 [8320/17010 (49%)] Loss: 0.001549
Train Epoch: 23 [8480/17010 (50%)] Loss: 0.005314
Train Epoch: 23 [8640/17010 (51%)] Loss: 0.042911
Train Epoch: 23 [8800/17010 (52%)] Loss: 0.014819
Train Epoch: 23 [8960/17010 (53%)] Loss: 0.030409
Train Epoch: 23 [9120/17010 (54%)] Loss: 0.013073
Train Epoch: 23 [9280/17010 (55%)] Loss: 0.047150
Train Epoch: 23 [9440/17010 (55%)] Loss: 0.063488
Train Epoch: 23 [9600/17010 (56%)] Loss: 0.041796
Train Epoch: 23 [9760/17010 (57%)] Loss: 0.002346
Train Epoch: 23 [9920/17010 (58%)] Loss: 0.005215
Train Epoch: 23 [10080/17010 (59%)] Loss: 0.002304
Train Epoch: 23 [10240/17010 (60%)] Loss: 0.006660
Train Epoch: 23 [10400/17010 (61%)] Loss: 0.003496
Train Epoch: 23 [10560/17010 (62%)] Loss: 0.030270
Train Epoch: 23 [10720/17010 (63%)] Loss: 0.019421
Train Epoch: 23 [10880/17010 (64%)] Loss: 0.039217
Train Epoch: 23 [11040/17010 (65%)] Loss: 0.008436
Train Epoch: 23 [11200/17010 (66%)] Loss: 0.031451
Train Epoch: 23 [11360/17010 (67%)] Loss: 0.030474
Train Epoch: 23 [11520/17010 (68%)] Loss: 0.011093
Train Epoch: 23 [11680/17010 (69%)] Loss: 0.030676
Train Epoch: 23 [11840/17010 (70%)] Loss: 0.004227
Train Epoch: 23 [12000/17010 (71%)] Loss: 0.007530
Train Epoch: 23 [12160/17010 (71%)] Loss: 0.007111
Train Epoch: 23 [12320/17010 (72%)] Loss: 0.017553
Train Epoch: 23 [12480/17010 (73%)] Loss: 0.037832
Train Epoch: 23 [12640/17010 (74%)] Loss: 0.030090
Train Epoch: 23 [12800/17010 (75%)] Loss: 0.056509
Train Epoch: 23 [12960/17010 (76%)] Loss: 0.005618
Train Epoch: 23 [13120/17010 (77%)] Loss: 0.004576
Train Epoch: 23 [13280/17010 (78%)] Loss: 0.047051
Train Epoch: 23 [13440/17010 (79%)] Loss: 0.101047
Train Epoch: 23 [13600/17010 (80%)] Loss: 0.047866
Train Epoch: 23 [13760/17010 (81%)] Loss: 0.072033
Train Epoch: 23 [13920/17010 (82%)] Loss: 0.000970
Train Epoch: 23 [14080/17010 (83%)] Loss: 0.014369
Train Epoch: 23 [14240/17010 (84%)] Loss: 0.031219
Train Epoch: 23 [14400/17010 (85%)] Loss: 0.007733
Train Epoch: 23 [14560/17010 (86%)] Loss: 0.012723
Train Epoch: 23 [14720/17010 (87%)] Loss: 0.015213
Train Epoch: 23 [14880/17010 (87%)] Loss: 0.027148
Train Epoch: 23 [15040/17010 (88%)] Loss: 0.026308
Train Epoch: 23 [15200/17010 (89%)] Loss: 0.017908
Train Epoch: 23 [15360/17010 (90%)] Loss: 0.017333
Train Epoch: 23 [15520/17010 (91%)] Loss: 0.001893
Train Epoch: 23 [15680/17010 (92%)] Loss: 0.029867
Train Epoch: 23 [15840/17010 (93%)] Loss: 0.003728
Train Epoch: 23 [16000/17010 (94%)] Loss: 0.003140
Train Epoch: 23 [16160/17010 (95%)] Loss: 0.009876
Train Epoch: 23 [16320/17010 (96%)] Loss: 0.009254
Train Epoch: 23 [16480/17010 (97%)] Loss: 0.061007
Train Epoch: 23 [16640/17010 (98%)] Loss: 0.103516
Train Epoch: 23 [16800/17010 (99%)] Loss: 0.009831
Train Epoch: 23 [16960/17010 (100%)] Loss: 0.005121
    epoch          : 23
    Train_loss     : 0.029570990181394547
    Train_accuracy : 0.9906015037593985
    Train_top_k_acc: 1.0
    Val_loss       : 0.12873591987687785
    Val_accuracy   : 0.9651041666666667
    Val_top_k_acc  : 0.9989583333333333
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch23.pth ...
Train Epoch: 24 [0/17010 (0%)] Loss: 0.029904
Train Epoch: 24 [160/17010 (1%)] Loss: 0.002309
Train Epoch: 24 [320/17010 (2%)] Loss: 0.006888
Train Epoch: 24 [480/17010 (3%)] Loss: 0.002266
Train Epoch: 24 [640/17010 (4%)] Loss: 0.056469
Train Epoch: 24 [800/17010 (5%)] Loss: 0.001825
Train Epoch: 24 [960/17010 (6%)] Loss: 0.351558
Train Epoch: 24 [1120/17010 (7%)] Loss: 0.025823
Train Epoch: 24 [1280/17010 (8%)] Loss: 0.003800
Train Epoch: 24 [1440/17010 (8%)] Loss: 0.111583
Train Epoch: 24 [1600/17010 (9%)] Loss: 0.001398
Train Epoch: 24 [1760/17010 (10%)] Loss: 0.058123
Train Epoch: 24 [1920/17010 (11%)] Loss: 0.021447
Train Epoch: 24 [2080/17010 (12%)] Loss: 0.011417
Train Epoch: 24 [2240/17010 (13%)] Loss: 0.006747
Train Epoch: 24 [2400/17010 (14%)] Loss: 0.028132
Train Epoch: 24 [2560/17010 (15%)] Loss: 0.070965
Train Epoch: 24 [2720/17010 (16%)] Loss: 0.007493
Train Epoch: 24 [2880/17010 (17%)] Loss: 0.000546
Train Epoch: 24 [3040/17010 (18%)] Loss: 0.027475
Train Epoch: 24 [3200/17010 (19%)] Loss: 0.061907
Train Epoch: 24 [3360/17010 (20%)] Loss: 0.003038
Train Epoch: 24 [3520/17010 (21%)] Loss: 0.010333
Train Epoch: 24 [3680/17010 (22%)] Loss: 0.009727
Train Epoch: 24 [3840/17010 (23%)] Loss: 0.002602
Train Epoch: 24 [4000/17010 (24%)] Loss: 0.004376
Train Epoch: 24 [4160/17010 (24%)] Loss: 0.070713
Train Epoch: 24 [4320/17010 (25%)] Loss: 0.002263
Train Epoch: 24 [4480/17010 (26%)] Loss: 0.000878
Train Epoch: 24 [4640/17010 (27%)] Loss: 0.000932
Train Epoch: 24 [4800/17010 (28%)] Loss: 0.088942
Train Epoch: 24 [4960/17010 (29%)] Loss: 0.017017
Train Epoch: 24 [5120/17010 (30%)] Loss: 0.032007
Train Epoch: 24 [5280/17010 (31%)] Loss: 0.247631
Train Epoch: 24 [5440/17010 (32%)] Loss: 0.011419
Train Epoch: 24 [5600/17010 (33%)] Loss: 0.007504
Train Epoch: 24 [5760/17010 (34%)] Loss: 0.014297
Train Epoch: 24 [5920/17010 (35%)] Loss: 0.006089
Train Epoch: 24 [6080/17010 (36%)] Loss: 0.020372
Train Epoch: 24 [6240/17010 (37%)] Loss: 0.010850
Train Epoch: 24 [6400/17010 (38%)] Loss: 0.083832
Train Epoch: 24 [6560/17010 (39%)] Loss: 0.081652
Train Epoch: 24 [6720/17010 (40%)] Loss: 0.017401
Train Epoch: 24 [6880/17010 (40%)] Loss: 0.362183
Train Epoch: 24 [7040/17010 (41%)] Loss: 0.065981
Train Epoch: 24 [7200/17010 (42%)] Loss: 0.005971
Train Epoch: 24 [7360/17010 (43%)] Loss: 0.007011
Train Epoch: 24 [7520/17010 (44%)] Loss: 0.008190
Train Epoch: 24 [7680/17010 (45%)] Loss: 0.143862
Train Epoch: 24 [7840/17010 (46%)] Loss: 0.012428
Train Epoch: 24 [8000/17010 (47%)] Loss: 0.079772
Train Epoch: 24 [8160/17010 (48%)] Loss: 0.004962
Train Epoch: 24 [8320/17010 (49%)] Loss: 0.304652
Train Epoch: 24 [8480/17010 (50%)] Loss: 0.175886
Train Epoch: 24 [8640/17010 (51%)] Loss: 0.012766
Train Epoch: 24 [8800/17010 (52%)] Loss: 0.005289
Train Epoch: 24 [8960/17010 (53%)] Loss: 0.047322
Train Epoch: 24 [9120/17010 (54%)] Loss: 0.005397
Train Epoch: 24 [9280/17010 (55%)] Loss: 0.061468
Train Epoch: 24 [9440/17010 (55%)] Loss: 0.003715
Train Epoch: 24 [9600/17010 (56%)] Loss: 0.008189
Train Epoch: 24 [9760/17010 (57%)] Loss: 0.002012
Train Epoch: 24 [9920/17010 (58%)] Loss: 0.077050
Train Epoch: 24 [10080/17010 (59%)] Loss: 0.002374
Train Epoch: 24 [10240/17010 (60%)] Loss: 0.026712
Train Epoch: 24 [10400/17010 (61%)] Loss: 0.016133
Train Epoch: 24 [10560/17010 (62%)] Loss: 0.002833
Train Epoch: 24 [10720/17010 (63%)] Loss: 0.054683
Train Epoch: 24 [10880/17010 (64%)] Loss: 0.007691
Train Epoch: 24 [11040/17010 (65%)] Loss: 0.047894
Train Epoch: 24 [11200/17010 (66%)] Loss: 0.014883
Train Epoch: 24 [11360/17010 (67%)] Loss: 0.015416
Train Epoch: 24 [11520/17010 (68%)] Loss: 0.008774
Train Epoch: 24 [11680/17010 (69%)] Loss: 0.011533
Train Epoch: 24 [11840/17010 (70%)] Loss: 0.089723
Train Epoch: 24 [12000/17010 (71%)] Loss: 0.001564
Train Epoch: 24 [12160/17010 (71%)] Loss: 0.047884
Train Epoch: 24 [12320/17010 (72%)] Loss: 0.166818
Train Epoch: 24 [12480/17010 (73%)] Loss: 0.501205
Train Epoch: 24 [12640/17010 (74%)] Loss: 0.096682
Train Epoch: 24 [12800/17010 (75%)] Loss: 0.040341
Train Epoch: 24 [12960/17010 (76%)] Loss: 0.004179
Train Epoch: 24 [13120/17010 (77%)] Loss: 0.014564
Train Epoch: 24 [13280/17010 (78%)] Loss: 0.013762
Train Epoch: 24 [13440/17010 (79%)] Loss: 0.017223
Train Epoch: 24 [13600/17010 (80%)] Loss: 0.026099
Train Epoch: 24 [13760/17010 (81%)] Loss: 0.019165
Train Epoch: 24 [13920/17010 (82%)] Loss: 0.024209
Train Epoch: 24 [14080/17010 (83%)] Loss: 0.048115
Train Epoch: 24 [14240/17010 (84%)] Loss: 0.145447
Train Epoch: 24 [14400/17010 (85%)] Loss: 0.006005
Train Epoch: 24 [14560/17010 (86%)] Loss: 0.150119
Train Epoch: 24 [14720/17010 (87%)] Loss: 0.073814
Train Epoch: 24 [14880/17010 (87%)] Loss: 0.007099
Train Epoch: 24 [15040/17010 (88%)] Loss: 0.004675
Train Epoch: 24 [15200/17010 (89%)] Loss: 0.034824
Train Epoch: 24 [15360/17010 (90%)] Loss: 0.063207
Train Epoch: 24 [15520/17010 (91%)] Loss: 0.008649
Train Epoch: 24 [15680/17010 (92%)] Loss: 0.025677
Train Epoch: 24 [15840/17010 (93%)] Loss: 0.006408
Train Epoch: 24 [16000/17010 (94%)] Loss: 0.034300
Train Epoch: 24 [16160/17010 (95%)] Loss: 0.112460
Train Epoch: 24 [16320/17010 (96%)] Loss: 0.010316
Train Epoch: 24 [16480/17010 (97%)] Loss: 0.007603
Train Epoch: 24 [16640/17010 (98%)] Loss: 0.014128
Train Epoch: 24 [16800/17010 (99%)] Loss: 0.009550
Train Epoch: 24 [16960/17010 (100%)] Loss: 0.006680
    epoch          : 24
    Train_loss     : 0.041146290961423665
    Train_accuracy : 0.9865484022556391
    Train_top_k_acc: 0.9998825187969925
    Val_loss       : 0.16795857491282126
    Val_accuracy   : 0.9520833333333333
    Val_top_k_acc  : 0.9979166666666667
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch24.pth ...
Train Epoch: 25 [0/17010 (0%)] Loss: 0.001907
Train Epoch: 25 [160/17010 (1%)] Loss: 0.005426
Train Epoch: 25 [320/17010 (2%)] Loss: 0.005712
Train Epoch: 25 [480/17010 (3%)] Loss: 0.034781
Train Epoch: 25 [640/17010 (4%)] Loss: 0.005103
Train Epoch: 25 [800/17010 (5%)] Loss: 0.034411
Train Epoch: 25 [960/17010 (6%)] Loss: 0.010483
Train Epoch: 25 [1120/17010 (7%)] Loss: 0.001058
Train Epoch: 25 [1280/17010 (8%)] Loss: 0.045344
Train Epoch: 25 [1440/17010 (8%)] Loss: 0.021963
Train Epoch: 25 [1600/17010 (9%)] Loss: 0.008858
Train Epoch: 25 [1760/17010 (10%)] Loss: 0.027514
Train Epoch: 25 [1920/17010 (11%)] Loss: 0.008148
Train Epoch: 25 [2080/17010 (12%)] Loss: 0.020135
Train Epoch: 25 [2240/17010 (13%)] Loss: 0.001769
Train Epoch: 25 [2400/17010 (14%)] Loss: 0.079795
Train Epoch: 25 [2560/17010 (15%)] Loss: 0.004092
Train Epoch: 25 [2720/17010 (16%)] Loss: 0.008092
Train Epoch: 25 [2880/17010 (17%)] Loss: 0.064778
Train Epoch: 25 [3040/17010 (18%)] Loss: 0.005619
Train Epoch: 25 [3200/17010 (19%)] Loss: 0.033817
Train Epoch: 25 [3360/17010 (20%)] Loss: 0.039067
Train Epoch: 25 [3520/17010 (21%)] Loss: 0.011008
Train Epoch: 25 [3680/17010 (22%)] Loss: 0.006159
Train Epoch: 25 [3840/17010 (23%)] Loss: 0.012355
Train Epoch: 25 [4000/17010 (24%)] Loss: 0.003405
Train Epoch: 25 [4160/17010 (24%)] Loss: 0.011455
Train Epoch: 25 [4320/17010 (25%)] Loss: 0.021709
Train Epoch: 25 [4480/17010 (26%)] Loss: 0.002064
Train Epoch: 25 [4640/17010 (27%)] Loss: 0.124760
Train Epoch: 25 [4800/17010 (28%)] Loss: 0.042162
Train Epoch: 25 [4960/17010 (29%)] Loss: 0.018210
Train Epoch: 25 [5120/17010 (30%)] Loss: 0.050982
Train Epoch: 25 [5280/17010 (31%)] Loss: 0.001821
Train Epoch: 25 [5440/17010 (32%)] Loss: 0.025674
Train Epoch: 25 [5600/17010 (33%)] Loss: 0.008838
Train Epoch: 25 [5760/17010 (34%)] Loss: 0.001793
Train Epoch: 25 [5920/17010 (35%)] Loss: 0.007967
Train Epoch: 25 [6080/17010 (36%)] Loss: 0.012961
Train Epoch: 25 [6240/17010 (37%)] Loss: 0.009488
Train Epoch: 25 [6400/17010 (38%)] Loss: 0.002003
Train Epoch: 25 [6560/17010 (39%)] Loss: 0.060170
Train Epoch: 25 [6720/17010 (40%)] Loss: 0.008641
Train Epoch: 25 [6880/17010 (40%)] Loss: 0.005907
Train Epoch: 25 [7040/17010 (41%)] Loss: 0.041374
Train Epoch: 25 [7200/17010 (42%)] Loss: 0.289252
Train Epoch: 25 [7360/17010 (43%)] Loss: 0.002947
Train Epoch: 25 [7520/17010 (44%)] Loss: 0.010099
Train Epoch: 25 [7680/17010 (45%)] Loss: 0.005670
Train Epoch: 25 [7840/17010 (46%)] Loss: 0.003127
Train Epoch: 25 [8000/17010 (47%)] Loss: 0.010883
Train Epoch: 25 [8160/17010 (48%)] Loss: 0.005328
Train Epoch: 25 [8320/17010 (49%)] Loss: 0.005445
Train Epoch: 25 [8480/17010 (50%)] Loss: 0.099948
Train Epoch: 25 [8640/17010 (51%)] Loss: 0.032468
Train Epoch: 25 [8800/17010 (52%)] Loss: 0.027912
Train Epoch: 25 [8960/17010 (53%)] Loss: 0.090429
Train Epoch: 25 [9120/17010 (54%)] Loss: 0.000860
Train Epoch: 25 [9280/17010 (55%)] Loss: 0.031335
Train Epoch: 25 [9440/17010 (55%)] Loss: 0.051430
Train Epoch: 25 [9600/17010 (56%)] Loss: 0.047312
Train Epoch: 25 [9760/17010 (57%)] Loss: 0.048251
Train Epoch: 25 [9920/17010 (58%)] Loss: 0.078695
Train Epoch: 25 [10080/17010 (59%)] Loss: 0.015857
Train Epoch: 25 [10240/17010 (60%)] Loss: 0.005728
Train Epoch: 25 [10400/17010 (61%)] Loss: 0.083683
Train Epoch: 25 [10560/17010 (62%)] Loss: 0.008892
Train Epoch: 25 [10720/17010 (63%)] Loss: 0.001438
Train Epoch: 25 [10880/17010 (64%)] Loss: 0.010491
Train Epoch: 25 [11040/17010 (65%)] Loss: 0.046049
Train Epoch: 25 [11200/17010 (66%)] Loss: 0.027796
Train Epoch: 25 [11360/17010 (67%)] Loss: 0.069840
Train Epoch: 25 [11520/17010 (68%)] Loss: 0.002864
Train Epoch: 25 [11680/17010 (69%)] Loss: 0.031400
Train Epoch: 25 [11840/17010 (70%)] Loss: 0.036114
Train Epoch: 25 [12000/17010 (71%)] Loss: 0.081832
Train Epoch: 25 [12160/17010 (71%)] Loss: 0.114286
Train Epoch: 25 [12320/17010 (72%)] Loss: 0.149251
Train Epoch: 25 [12480/17010 (73%)] Loss: 0.003291
Train Epoch: 25 [12640/17010 (74%)] Loss: 0.060275
Train Epoch: 25 [12800/17010 (75%)] Loss: 0.144154
Train Epoch: 25 [12960/17010 (76%)] Loss: 0.016352
Train Epoch: 25 [13120/17010 (77%)] Loss: 0.033615
Train Epoch: 25 [13280/17010 (78%)] Loss: 0.013863
Train Epoch: 25 [13440/17010 (79%)] Loss: 0.043054
Train Epoch: 25 [13600/17010 (80%)] Loss: 0.037393
Train Epoch: 25 [13760/17010 (81%)] Loss: 0.069019
Train Epoch: 25 [13920/17010 (82%)] Loss: 0.002353
Train Epoch: 25 [14080/17010 (83%)] Loss: 0.019025
Train Epoch: 25 [14240/17010 (84%)] Loss: 0.005405
Train Epoch: 25 [14400/17010 (85%)] Loss: 0.135991
Train Epoch: 25 [14560/17010 (86%)] Loss: 0.079623
Train Epoch: 25 [14720/17010 (87%)] Loss: 0.005040
Train Epoch: 25 [14880/17010 (87%)] Loss: 0.009992
Train Epoch: 25 [15040/17010 (88%)] Loss: 0.012693
Train Epoch: 25 [15200/17010 (89%)] Loss: 0.040650
Train Epoch: 25 [15360/17010 (90%)] Loss: 0.012033
Train Epoch: 25 [15520/17010 (91%)] Loss: 0.036700
Train Epoch: 25 [15680/17010 (92%)] Loss: 0.004369
Train Epoch: 25 [15840/17010 (93%)] Loss: 0.043670
Train Epoch: 25 [16000/17010 (94%)] Loss: 0.046109
Train Epoch: 25 [16160/17010 (95%)] Loss: 0.097698
Train Epoch: 25 [16320/17010 (96%)] Loss: 0.003855
Train Epoch: 25 [16480/17010 (97%)] Loss: 0.078433
Train Epoch: 25 [16640/17010 (98%)] Loss: 0.005240
Train Epoch: 25 [16800/17010 (99%)] Loss: 0.006213
Train Epoch: 25 [16960/17010 (100%)] Loss: 0.001560
    epoch          : 25
    Train_loss     : 0.04332632251863746
    Train_accuracy : 0.9855498120300752
    Train_top_k_acc: 0.9998825187969925
    Val_loss       : 0.20275478100326533
    Val_accuracy   : 0.946875
    Val_top_k_acc  : 0.9984375
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch25.pth ...
Train Epoch: 26 [0/17010 (0%)] Loss: 0.066595
Train Epoch: 26 [160/17010 (1%)] Loss: 0.011146
Train Epoch: 26 [320/17010 (2%)] Loss: 0.008721
Train Epoch: 26 [480/17010 (3%)] Loss: 0.025157
Train Epoch: 26 [640/17010 (4%)] Loss: 0.002035
Train Epoch: 26 [800/17010 (5%)] Loss: 0.028834
Train Epoch: 26 [960/17010 (6%)] Loss: 0.021280
Train Epoch: 26 [1120/17010 (7%)] Loss: 0.004575
Train Epoch: 26 [1280/17010 (8%)] Loss: 0.007095
Train Epoch: 26 [1440/17010 (8%)] Loss: 0.014577
Train Epoch: 26 [1600/17010 (9%)] Loss: 0.004303
Train Epoch: 26 [1760/17010 (10%)] Loss: 0.006289
Train Epoch: 26 [1920/17010 (11%)] Loss: 0.007587
Train Epoch: 26 [2080/17010 (12%)] Loss: 0.044223
Train Epoch: 26 [2240/17010 (13%)] Loss: 0.009096
Train Epoch: 26 [2400/17010 (14%)] Loss: 0.005808
Train Epoch: 26 [2560/17010 (15%)] Loss: 0.013095
Train Epoch: 26 [2720/17010 (16%)] Loss: 0.003459
Train Epoch: 26 [2880/17010 (17%)] Loss: 0.002746
Train Epoch: 26 [3040/17010 (18%)] Loss: 0.005131
Train Epoch: 26 [3200/17010 (19%)] Loss: 0.003282
Train Epoch: 26 [3360/17010 (20%)] Loss: 0.015920
Train Epoch: 26 [3520/17010 (21%)] Loss: 0.017745
Train Epoch: 26 [3680/17010 (22%)] Loss: 0.036696
Train Epoch: 26 [3840/17010 (23%)] Loss: 0.006270
Train Epoch: 26 [4000/17010 (24%)] Loss: 0.004246
Train Epoch: 26 [4160/17010 (24%)] Loss: 0.125682
Train Epoch: 26 [4320/17010 (25%)] Loss: 0.016395
Train Epoch: 26 [4480/17010 (26%)] Loss: 0.022322
Train Epoch: 26 [4640/17010 (27%)] Loss: 0.040507
Train Epoch: 26 [4800/17010 (28%)] Loss: 0.002441
Train Epoch: 26 [4960/17010 (29%)] Loss: 0.059166
Train Epoch: 26 [5120/17010 (30%)] Loss: 0.013483
Train Epoch: 26 [5280/17010 (31%)] Loss: 0.004335
Train Epoch: 26 [5440/17010 (32%)] Loss: 0.052060
Train Epoch: 26 [5600/17010 (33%)] Loss: 0.027030
Train Epoch: 26 [5760/17010 (34%)] Loss: 0.003227
Train Epoch: 26 [5920/17010 (35%)] Loss: 0.002527
Train Epoch: 26 [6080/17010 (36%)] Loss: 0.007920
Train Epoch: 26 [6240/17010 (37%)] Loss: 0.013179
Train Epoch: 26 [6400/17010 (38%)] Loss: 0.008242
Train Epoch: 26 [6560/17010 (39%)] Loss: 0.040828
Train Epoch: 26 [6720/17010 (40%)] Loss: 0.070804
Train Epoch: 26 [6880/17010 (40%)] Loss: 0.085077
Train Epoch: 26 [7040/17010 (41%)] Loss: 0.078059
Train Epoch: 26 [7200/17010 (42%)] Loss: 0.045435
Train Epoch: 26 [7360/17010 (43%)] Loss: 0.010003
Train Epoch: 26 [7520/17010 (44%)] Loss: 0.063870
Train Epoch: 26 [7680/17010 (45%)] Loss: 0.003701
Train Epoch: 26 [7840/17010 (46%)] Loss: 0.001493
Train Epoch: 26 [8000/17010 (47%)] Loss: 0.008355
Train Epoch: 26 [8160/17010 (48%)] Loss: 0.007388
Train Epoch: 26 [8320/17010 (49%)] Loss: 0.007413
Train Epoch: 26 [8480/17010 (50%)] Loss: 0.235063
Train Epoch: 26 [8640/17010 (51%)] Loss: 0.026659
Train Epoch: 26 [8800/17010 (52%)] Loss: 0.000567
Train Epoch: 26 [8960/17010 (53%)] Loss: 0.027132
Train Epoch: 26 [9120/17010 (54%)] Loss: 0.005552
Train Epoch: 26 [9280/17010 (55%)] Loss: 0.006133
Train Epoch: 26 [9440/17010 (55%)] Loss: 0.001188
Train Epoch: 26 [9600/17010 (56%)] Loss: 0.029303
Train Epoch: 26 [9760/17010 (57%)] Loss: 0.005691
Train Epoch: 26 [9920/17010 (58%)] Loss: 0.000711
Train Epoch: 26 [10080/17010 (59%)] Loss: 0.008193
Train Epoch: 26 [10240/17010 (60%)] Loss: 0.221996
Train Epoch: 26 [10400/17010 (61%)] Loss: 0.041437
Train Epoch: 26 [10560/17010 (62%)] Loss: 0.047320
Train Epoch: 26 [10720/17010 (63%)] Loss: 0.005384
Train Epoch: 26 [10880/17010 (64%)] Loss: 0.001601
Train Epoch: 26 [11040/17010 (65%)] Loss: 0.045679
Train Epoch: 26 [11200/17010 (66%)] Loss: 0.068201
Train Epoch: 26 [11360/17010 (67%)] Loss: 0.157352
Train Epoch: 26 [11520/17010 (68%)] Loss: 0.041813
Train Epoch: 26 [11680/17010 (69%)] Loss: 0.002685
Train Epoch: 26 [11840/17010 (70%)] Loss: 0.061088
Train Epoch: 26 [12000/17010 (71%)] Loss: 0.005358
Train Epoch: 26 [12160/17010 (71%)] Loss: 0.020868
Train Epoch: 26 [12320/17010 (72%)] Loss: 0.021209
Train Epoch: 26 [12480/17010 (73%)] Loss: 0.015716
Train Epoch: 26 [12640/17010 (74%)] Loss: 0.014792
Train Epoch: 26 [12800/17010 (75%)] Loss: 0.021203
Train Epoch: 26 [12960/17010 (76%)] Loss: 0.316736
Train Epoch: 26 [13120/17010 (77%)] Loss: 0.215222
Train Epoch: 26 [13280/17010 (78%)] Loss: 0.026820
Train Epoch: 26 [13440/17010 (79%)] Loss: 0.089736
Train Epoch: 26 [13600/17010 (80%)] Loss: 0.001253
Train Epoch: 26 [13760/17010 (81%)] Loss: 0.004206
Train Epoch: 26 [13920/17010 (82%)] Loss: 0.019627
Train Epoch: 26 [14080/17010 (83%)] Loss: 0.022312
Train Epoch: 26 [14240/17010 (84%)] Loss: 0.056471
Train Epoch: 26 [14400/17010 (85%)] Loss: 0.011281
Train Epoch: 26 [14560/17010 (86%)] Loss: 0.274269
Train Epoch: 26 [14720/17010 (87%)] Loss: 0.008766
Train Epoch: 26 [14880/17010 (87%)] Loss: 0.026871
Train Epoch: 26 [15040/17010 (88%)] Loss: 0.002352
Train Epoch: 26 [15200/17010 (89%)] Loss: 0.026902
Train Epoch: 26 [15360/17010 (90%)] Loss: 0.018815
Train Epoch: 26 [15520/17010 (91%)] Loss: 0.022444
Train Epoch: 26 [15680/17010 (92%)] Loss: 0.002884
Train Epoch: 26 [15840/17010 (93%)] Loss: 0.078297
Train Epoch: 26 [16000/17010 (94%)] Loss: 0.021465
Train Epoch: 26 [16160/17010 (95%)] Loss: 0.001906
Train Epoch: 26 [16320/17010 (96%)] Loss: 0.005409
Train Epoch: 26 [16480/17010 (97%)] Loss: 0.021954
Train Epoch: 26 [16640/17010 (98%)] Loss: 0.006318
Train Epoch: 26 [16800/17010 (99%)] Loss: 0.018384
Train Epoch: 26 [16960/17010 (100%)] Loss: 0.024052
    epoch          : 26
    Train_loss     : 0.032331785560606126
    Train_accuracy : 0.9891329887218046
    Train_top_k_acc: 1.0
    Val_loss       : 0.2384224310046799
    Val_accuracy   : 0.9401041666666666
    Val_top_k_acc  : 0.9932291666666667
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch26.pth ...
Train Epoch: 27 [0/17010 (0%)] Loss: 0.005216
Train Epoch: 27 [160/17010 (1%)] Loss: 0.007862
Train Epoch: 27 [320/17010 (2%)] Loss: 0.073553
Train Epoch: 27 [480/17010 (3%)] Loss: 0.062184
Train Epoch: 27 [640/17010 (4%)] Loss: 0.003459
Train Epoch: 27 [800/17010 (5%)] Loss: 0.025512
Train Epoch: 27 [960/17010 (6%)] Loss: 0.066323
Train Epoch: 27 [1120/17010 (7%)] Loss: 0.009882
Train Epoch: 27 [1280/17010 (8%)] Loss: 0.068330
Train Epoch: 27 [1440/17010 (8%)] Loss: 0.002549
Train Epoch: 27 [1600/17010 (9%)] Loss: 0.073258
Train Epoch: 27 [1760/17010 (10%)] Loss: 0.010443
Train Epoch: 27 [1920/17010 (11%)] Loss: 0.014738
Train Epoch: 27 [2080/17010 (12%)] Loss: 0.004347
Train Epoch: 27 [2240/17010 (13%)] Loss: 0.047907
Train Epoch: 27 [2400/17010 (14%)] Loss: 0.045628
Train Epoch: 27 [2560/17010 (15%)] Loss: 0.003977
Train Epoch: 27 [2720/17010 (16%)] Loss: 0.027940
Train Epoch: 27 [2880/17010 (17%)] Loss: 0.124984
Train Epoch: 27 [3040/17010 (18%)] Loss: 0.037052
Train Epoch: 27 [3200/17010 (19%)] Loss: 0.007029
Train Epoch: 27 [3360/17010 (20%)] Loss: 0.004623
Train Epoch: 27 [3520/17010 (21%)] Loss: 0.004083
Train Epoch: 27 [3680/17010 (22%)] Loss: 0.005127
Train Epoch: 27 [3840/17010 (23%)] Loss: 0.112326
Train Epoch: 27 [4000/17010 (24%)] Loss: 0.014454
Train Epoch: 27 [4160/17010 (24%)] Loss: 0.031993
Train Epoch: 27 [4320/17010 (25%)] Loss: 0.014485
Train Epoch: 27 [4480/17010 (26%)] Loss: 0.014666
Train Epoch: 27 [4640/17010 (27%)] Loss: 0.004515
Train Epoch: 27 [4800/17010 (28%)] Loss: 0.009472
Train Epoch: 27 [4960/17010 (29%)] Loss: 0.029263
Train Epoch: 27 [5120/17010 (30%)] Loss: 0.042266
Train Epoch: 27 [5280/17010 (31%)] Loss: 0.005052
Train Epoch: 27 [5440/17010 (32%)] Loss: 0.005607
Train Epoch: 27 [5600/17010 (33%)] Loss: 0.010059
Train Epoch: 27 [5760/17010 (34%)] Loss: 0.015726
Train Epoch: 27 [5920/17010 (35%)] Loss: 0.138185
Train Epoch: 27 [6080/17010 (36%)] Loss: 0.009537
Train Epoch: 27 [6240/17010 (37%)] Loss: 0.005973
Train Epoch: 27 [6400/17010 (38%)] Loss: 0.050560
Train Epoch: 27 [6560/17010 (39%)] Loss: 0.001401
Train Epoch: 27 [6720/17010 (40%)] Loss: 0.006584
Train Epoch: 27 [6880/17010 (40%)] Loss: 0.001204
Train Epoch: 27 [7040/17010 (41%)] Loss: 0.003777
Train Epoch: 27 [7200/17010 (42%)] Loss: 0.036877
Train Epoch: 27 [7360/17010 (43%)] Loss: 0.000509
Train Epoch: 27 [7520/17010 (44%)] Loss: 0.011101
Train Epoch: 27 [7680/17010 (45%)] Loss: 0.009297
Train Epoch: 27 [7840/17010 (46%)] Loss: 0.009824
Train Epoch: 27 [8000/17010 (47%)] Loss: 0.006698
Train Epoch: 27 [8160/17010 (48%)] Loss: 0.002778
Train Epoch: 27 [8320/17010 (49%)] Loss: 0.047081
Train Epoch: 27 [8480/17010 (50%)] Loss: 0.000407
Train Epoch: 27 [8640/17010 (51%)] Loss: 0.000694
Train Epoch: 27 [8800/17010 (52%)] Loss: 0.012654
Train Epoch: 27 [8960/17010 (53%)] Loss: 0.002599
Train Epoch: 27 [9120/17010 (54%)] Loss: 0.000843
Train Epoch: 27 [9280/17010 (55%)] Loss: 0.004548
Train Epoch: 27 [9440/17010 (55%)] Loss: 0.116847
Train Epoch: 27 [9600/17010 (56%)] Loss: 0.268637
Train Epoch: 27 [9760/17010 (57%)] Loss: 0.009111
Train Epoch: 27 [9920/17010 (58%)] Loss: 0.010813
Train Epoch: 27 [10080/17010 (59%)] Loss: 0.082905
Train Epoch: 27 [10240/17010 (60%)] Loss: 0.222072
Train Epoch: 27 [10400/17010 (61%)] Loss: 0.042975
Train Epoch: 27 [10560/17010 (62%)] Loss: 0.022972
Train Epoch: 27 [10720/17010 (63%)] Loss: 0.012442
Train Epoch: 27 [10880/17010 (64%)] Loss: 0.073501
Train Epoch: 27 [11040/17010 (65%)] Loss: 0.002285
Train Epoch: 27 [11200/17010 (66%)] Loss: 0.025288
Train Epoch: 27 [11360/17010 (67%)] Loss: 0.016052
Train Epoch: 27 [11520/17010 (68%)] Loss: 0.005565
Train Epoch: 27 [11680/17010 (69%)] Loss: 0.193719
Train Epoch: 27 [11840/17010 (70%)] Loss: 0.089584
Train Epoch: 27 [12000/17010 (71%)] Loss: 0.030733
Train Epoch: 27 [12160/17010 (71%)] Loss: 0.001812
Train Epoch: 27 [12320/17010 (72%)] Loss: 0.002193
Train Epoch: 27 [12480/17010 (73%)] Loss: 0.012993
Train Epoch: 27 [12640/17010 (74%)] Loss: 0.063654
Train Epoch: 27 [12800/17010 (75%)] Loss: 0.021823
Train Epoch: 27 [12960/17010 (76%)] Loss: 0.020696
Train Epoch: 27 [13120/17010 (77%)] Loss: 0.050011
Train Epoch: 27 [13280/17010 (78%)] Loss: 0.048994
Train Epoch: 27 [13440/17010 (79%)] Loss: 0.111870
Train Epoch: 27 [13600/17010 (80%)] Loss: 0.032478
Train Epoch: 27 [13760/17010 (81%)] Loss: 0.017454
Train Epoch: 27 [13920/17010 (82%)] Loss: 0.015195
Train Epoch: 27 [14080/17010 (83%)] Loss: 0.014830
Train Epoch: 27 [14240/17010 (84%)] Loss: 0.002595
Train Epoch: 27 [14400/17010 (85%)] Loss: 0.150885
Train Epoch: 27 [14560/17010 (86%)] Loss: 0.112291
Train Epoch: 27 [14720/17010 (87%)] Loss: 0.141511
Train Epoch: 27 [14880/17010 (87%)] Loss: 0.166047
Train Epoch: 27 [15040/17010 (88%)] Loss: 0.018520
Train Epoch: 27 [15200/17010 (89%)] Loss: 0.005462
Train Epoch: 27 [15360/17010 (90%)] Loss: 0.001477
Train Epoch: 27 [15520/17010 (91%)] Loss: 0.071651
Train Epoch: 27 [15680/17010 (92%)] Loss: 0.003116
Train Epoch: 27 [15840/17010 (93%)] Loss: 0.318981
Train Epoch: 27 [16000/17010 (94%)] Loss: 0.004909
Train Epoch: 27 [16160/17010 (95%)] Loss: 0.002284
Train Epoch: 27 [16320/17010 (96%)] Loss: 0.005017
Train Epoch: 27 [16480/17010 (97%)] Loss: 0.007030
Train Epoch: 27 [16640/17010 (98%)] Loss: 0.155098
Train Epoch: 27 [16800/17010 (99%)] Loss: 0.155281
Train Epoch: 27 [16960/17010 (100%)] Loss: 0.006085
    epoch          : 27
    Train_loss     : 0.0366037833249775
    Train_accuracy : 0.9877950083542189
    Train_top_k_acc: 0.9999412593984962
    Val_loss       : 0.1591896972541387
    Val_accuracy   : 0.9520833333333333
    Val_top_k_acc  : 0.9984375
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch27.pth ...
Train Epoch: 28 [0/17010 (0%)] Loss: 0.010994
Train Epoch: 28 [160/17010 (1%)] Loss: 0.004619
Train Epoch: 28 [320/17010 (2%)] Loss: 0.003265
Train Epoch: 28 [480/17010 (3%)] Loss: 0.081616
Train Epoch: 28 [640/17010 (4%)] Loss: 0.107494
Train Epoch: 28 [800/17010 (5%)] Loss: 0.077163
Train Epoch: 28 [960/17010 (6%)] Loss: 0.007601
Train Epoch: 28 [1120/17010 (7%)] Loss: 0.003020
Train Epoch: 28 [1280/17010 (8%)] Loss: 0.009316
Train Epoch: 28 [1440/17010 (8%)] Loss: 0.051428
Train Epoch: 28 [1600/17010 (9%)] Loss: 0.014492
Train Epoch: 28 [1760/17010 (10%)] Loss: 0.002728
Train Epoch: 28 [1920/17010 (11%)] Loss: 0.031462
Train Epoch: 28 [2080/17010 (12%)] Loss: 0.001190
Train Epoch: 28 [2240/17010 (13%)] Loss: 0.001438
Train Epoch: 28 [2400/17010 (14%)] Loss: 0.012685
Train Epoch: 28 [2560/17010 (15%)] Loss: 0.000974
Train Epoch: 28 [2720/17010 (16%)] Loss: 0.002206
Train Epoch: 28 [2880/17010 (17%)] Loss: 0.009167
Train Epoch: 28 [3040/17010 (18%)] Loss: 0.025037
Train Epoch: 28 [3200/17010 (19%)] Loss: 0.409483
Train Epoch: 28 [3360/17010 (20%)] Loss: 0.009055
Train Epoch: 28 [3520/17010 (21%)] Loss: 0.068066
Train Epoch: 28 [3680/17010 (22%)] Loss: 0.018684
Train Epoch: 28 [3840/17010 (23%)] Loss: 0.051972
Train Epoch: 28 [4000/17010 (24%)] Loss: 0.068272
Train Epoch: 28 [4160/17010 (24%)] Loss: 0.005206
Train Epoch: 28 [4320/17010 (25%)] Loss: 0.016216
Train Epoch: 28 [4480/17010 (26%)] Loss: 0.059408
Train Epoch: 28 [4640/17010 (27%)] Loss: 0.001728
Train Epoch: 28 [4800/17010 (28%)] Loss: 0.020867
Train Epoch: 28 [4960/17010 (29%)] Loss: 0.052473
Train Epoch: 28 [5120/17010 (30%)] Loss: 0.002173
Train Epoch: 28 [5280/17010 (31%)] Loss: 0.037008
Train Epoch: 28 [5440/17010 (32%)] Loss: 0.006092
Train Epoch: 28 [5600/17010 (33%)] Loss: 0.143119
Train Epoch: 28 [5760/17010 (34%)] Loss: 0.069665
Train Epoch: 28 [5920/17010 (35%)] Loss: 0.004598
Train Epoch: 28 [6080/17010 (36%)] Loss: 0.004111
Train Epoch: 28 [6240/17010 (37%)] Loss: 0.067203
Train Epoch: 28 [6400/17010 (38%)] Loss: 0.008455
Train Epoch: 28 [6560/17010 (39%)] Loss: 0.123890
Train Epoch: 28 [6720/17010 (40%)] Loss: 0.005505
Train Epoch: 28 [6880/17010 (40%)] Loss: 0.023606
Train Epoch: 28 [7040/17010 (41%)] Loss: 0.026139
Train Epoch: 28 [7200/17010 (42%)] Loss: 0.012313
Train Epoch: 28 [7360/17010 (43%)] Loss: 0.002199
Train Epoch: 28 [7520/17010 (44%)] Loss: 0.023987
Train Epoch: 28 [7680/17010 (45%)] Loss: 0.006841
Train Epoch: 28 [7840/17010 (46%)] Loss: 0.023330
Train Epoch: 28 [8000/17010 (47%)] Loss: 0.003666
Train Epoch: 28 [8160/17010 (48%)] Loss: 0.005333
Train Epoch: 28 [8320/17010 (49%)] Loss: 0.139907
Train Epoch: 28 [8480/17010 (50%)] Loss: 0.025665
Train Epoch: 28 [8640/17010 (51%)] Loss: 0.010979
Train Epoch: 28 [8800/17010 (52%)] Loss: 0.046639
Train Epoch: 28 [8960/17010 (53%)] Loss: 0.001341
Train Epoch: 28 [9120/17010 (54%)] Loss: 0.010771
Train Epoch: 28 [9280/17010 (55%)] Loss: 0.001886
Train Epoch: 28 [9440/17010 (55%)] Loss: 0.004749
Train Epoch: 28 [9600/17010 (56%)] Loss: 0.002286
Train Epoch: 28 [9760/17010 (57%)] Loss: 0.001834
Train Epoch: 28 [9920/17010 (58%)] Loss: 0.006232
Train Epoch: 28 [10080/17010 (59%)] Loss: 0.001084
Train Epoch: 28 [10240/17010 (60%)] Loss: 0.001652
Train Epoch: 28 [10400/17010 (61%)] Loss: 0.101362
Train Epoch: 28 [10560/17010 (62%)] Loss: 0.004679
Train Epoch: 28 [10720/17010 (63%)] Loss: 0.002961
Train Epoch: 28 [10880/17010 (64%)] Loss: 0.009584
Train Epoch: 28 [11040/17010 (65%)] Loss: 0.049608
Train Epoch: 28 [11200/17010 (66%)] Loss: 0.001535
Train Epoch: 28 [11360/17010 (67%)] Loss: 0.001241
Train Epoch: 28 [11520/17010 (68%)] Loss: 0.002800
Train Epoch: 28 [11680/17010 (69%)] Loss: 0.007238
Train Epoch: 28 [11840/17010 (70%)] Loss: 0.005311
Train Epoch: 28 [12000/17010 (71%)] Loss: 0.017396
Train Epoch: 28 [12160/17010 (71%)] Loss: 0.005399
Train Epoch: 28 [12320/17010 (72%)] Loss: 0.001288
Train Epoch: 28 [12480/17010 (73%)] Loss: 0.051914
Train Epoch: 28 [12640/17010 (74%)] Loss: 0.035919
Train Epoch: 28 [12800/17010 (75%)] Loss: 0.050432
Train Epoch: 28 [12960/17010 (76%)] Loss: 0.014247
Train Epoch: 28 [13120/17010 (77%)] Loss: 0.080728
Train Epoch: 28 [13280/17010 (78%)] Loss: 0.000405
Train Epoch: 28 [13440/17010 (79%)] Loss: 0.000359
Train Epoch: 28 [13600/17010 (80%)] Loss: 0.004750
Train Epoch: 28 [13760/17010 (81%)] Loss: 0.004652
Train Epoch: 28 [13920/17010 (82%)] Loss: 0.000851
Train Epoch: 28 [14080/17010 (83%)] Loss: 0.002256
Train Epoch: 28 [14240/17010 (84%)] Loss: 0.033248
Train Epoch: 28 [14400/17010 (85%)] Loss: 0.005844
Train Epoch: 28 [14560/17010 (86%)] Loss: 0.001184
Train Epoch: 28 [14720/17010 (87%)] Loss: 0.000934
Train Epoch: 28 [14880/17010 (87%)] Loss: 0.030424
Train Epoch: 28 [15040/17010 (88%)] Loss: 0.001332
Train Epoch: 28 [15200/17010 (89%)] Loss: 0.002953
Train Epoch: 28 [15360/17010 (90%)] Loss: 0.001521
Train Epoch: 28 [15520/17010 (91%)] Loss: 0.003995
Train Epoch: 28 [15680/17010 (92%)] Loss: 0.002441
Train Epoch: 28 [15840/17010 (93%)] Loss: 0.001223
Train Epoch: 28 [16000/17010 (94%)] Loss: 0.005546
Train Epoch: 28 [16160/17010 (95%)] Loss: 0.006632
Train Epoch: 28 [16320/17010 (96%)] Loss: 0.004964
Train Epoch: 28 [16480/17010 (97%)] Loss: 0.001693
Train Epoch: 28 [16640/17010 (98%)] Loss: 0.004931
Train Epoch: 28 [16800/17010 (99%)] Loss: 0.005185
Train Epoch: 28 [16960/17010 (100%)] Loss: 0.003191
    epoch          : 28
    Train_loss     : 0.02591758037300665
    Train_accuracy : 0.9916000939849624
    Train_top_k_acc: 0.9998237781954887
    Val_loss       : 0.10549533232697286
    Val_accuracy   : 0.9671875
    Val_top_k_acc  : 0.9989583333333333
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch28.pth ...
Train Epoch: 29 [0/17010 (0%)] Loss: 0.002864
Train Epoch: 29 [160/17010 (1%)] Loss: 0.007051
Train Epoch: 29 [320/17010 (2%)] Loss: 0.001890
Train Epoch: 29 [480/17010 (3%)] Loss: 0.078400
Train Epoch: 29 [640/17010 (4%)] Loss: 0.000777
Train Epoch: 29 [800/17010 (5%)] Loss: 0.015113
Train Epoch: 29 [960/17010 (6%)] Loss: 0.007242
Train Epoch: 29 [1120/17010 (7%)] Loss: 0.047972
Train Epoch: 29 [1280/17010 (8%)] Loss: 0.002042
Train Epoch: 29 [1440/17010 (8%)] Loss: 0.024363
Train Epoch: 29 [1600/17010 (9%)] Loss: 0.000624
Train Epoch: 29 [1760/17010 (10%)] Loss: 0.020664
Train Epoch: 29 [1920/17010 (11%)] Loss: 0.010448
Train Epoch: 29 [2080/17010 (12%)] Loss: 0.001791
Train Epoch: 29 [2240/17010 (13%)] Loss: 0.009374
Train Epoch: 29 [2400/17010 (14%)] Loss: 0.001223
Train Epoch: 29 [2560/17010 (15%)] Loss: 0.000570
Train Epoch: 29 [2720/17010 (16%)] Loss: 0.037081
Train Epoch: 29 [2880/17010 (17%)] Loss: 0.002782
Train Epoch: 29 [3040/17010 (18%)] Loss: 0.003281
Train Epoch: 29 [3200/17010 (19%)] Loss: 0.039423
Train Epoch: 29 [3360/17010 (20%)] Loss: 0.009453
Train Epoch: 29 [3520/17010 (21%)] Loss: 0.000695
Train Epoch: 29 [3680/17010 (22%)] Loss: 0.001626
Train Epoch: 29 [3840/17010 (23%)] Loss: 0.004180
Train Epoch: 29 [4000/17010 (24%)] Loss: 0.000991
Train Epoch: 29 [4160/17010 (24%)] Loss: 0.001354
Train Epoch: 29 [4320/17010 (25%)] Loss: 0.001441
Train Epoch: 29 [4480/17010 (26%)] Loss: 0.003747
Train Epoch: 29 [4640/17010 (27%)] Loss: 0.003028
Train Epoch: 29 [4800/17010 (28%)] Loss: 0.042873
Train Epoch: 29 [4960/17010 (29%)] Loss: 0.001041
Train Epoch: 29 [5120/17010 (30%)] Loss: 0.005484
Train Epoch: 29 [5280/17010 (31%)] Loss: 0.016085
Train Epoch: 29 [5440/17010 (32%)] Loss: 0.009220
Train Epoch: 29 [5600/17010 (33%)] Loss: 0.011464
Train Epoch: 29 [5760/17010 (34%)] Loss: 0.016719
Train Epoch: 29 [5920/17010 (35%)] Loss: 0.012092
Train Epoch: 29 [6080/17010 (36%)] Loss: 0.013124
Train Epoch: 29 [6240/17010 (37%)] Loss: 0.027177
Train Epoch: 29 [6400/17010 (38%)] Loss: 0.000295
Train Epoch: 29 [6560/17010 (39%)] Loss: 0.001193
Train Epoch: 29 [6720/17010 (40%)] Loss: 0.005732
Train Epoch: 29 [6880/17010 (40%)] Loss: 0.000784
Train Epoch: 29 [7040/17010 (41%)] Loss: 0.001484
Train Epoch: 29 [7200/17010 (42%)] Loss: 0.004883
Train Epoch: 29 [7360/17010 (43%)] Loss: 0.001357
Train Epoch: 29 [7520/17010 (44%)] Loss: 0.001146
Train Epoch: 29 [7680/17010 (45%)] Loss: 0.021686
Train Epoch: 29 [7840/17010 (46%)] Loss: 0.012506
Train Epoch: 29 [8000/17010 (47%)] Loss: 0.024040
Train Epoch: 29 [8160/17010 (48%)] Loss: 0.171297
Train Epoch: 29 [8320/17010 (49%)] Loss: 0.001049
Train Epoch: 29 [8480/17010 (50%)] Loss: 0.000968
Train Epoch: 29 [8640/17010 (51%)] Loss: 0.020141
Train Epoch: 29 [8800/17010 (52%)] Loss: 0.003059
Train Epoch: 29 [8960/17010 (53%)] Loss: 0.122338
Train Epoch: 29 [9120/17010 (54%)] Loss: 0.031873
Train Epoch: 29 [9280/17010 (55%)] Loss: 0.013600
Train Epoch: 29 [9440/17010 (55%)] Loss: 0.001119
Train Epoch: 29 [9600/17010 (56%)] Loss: 0.006878
Train Epoch: 29 [9760/17010 (57%)] Loss: 0.001184
Train Epoch: 29 [9920/17010 (58%)] Loss: 0.001195
Train Epoch: 29 [10080/17010 (59%)] Loss: 0.003451
Train Epoch: 29 [10240/17010 (60%)] Loss: 0.012043
Train Epoch: 29 [10400/17010 (61%)] Loss: 0.003073
Train Epoch: 29 [10560/17010 (62%)] Loss: 0.029845
Train Epoch: 29 [10720/17010 (63%)] Loss: 0.059273
Train Epoch: 29 [10880/17010 (64%)] Loss: 0.002561
Train Epoch: 29 [11040/17010 (65%)] Loss: 0.018710
Train Epoch: 29 [11200/17010 (66%)] Loss: 0.093916
Train Epoch: 29 [11360/17010 (67%)] Loss: 0.002457
Train Epoch: 29 [11520/17010 (68%)] Loss: 0.001952
Train Epoch: 29 [11680/17010 (69%)] Loss: 0.000861
Train Epoch: 29 [11840/17010 (70%)] Loss: 0.008018
Train Epoch: 29 [12000/17010 (71%)] Loss: 0.048054
Train Epoch: 29 [12160/17010 (71%)] Loss: 0.003967
Train Epoch: 29 [12320/17010 (72%)] Loss: 0.069439
Train Epoch: 29 [12480/17010 (73%)] Loss: 0.010024
Train Epoch: 29 [12640/17010 (74%)] Loss: 0.003576
Train Epoch: 29 [12800/17010 (75%)] Loss: 0.050450
Train Epoch: 29 [12960/17010 (76%)] Loss: 0.007325
Train Epoch: 29 [13120/17010 (77%)] Loss: 0.000671
Train Epoch: 29 [13280/17010 (78%)] Loss: 0.002327
Train Epoch: 29 [13440/17010 (79%)] Loss: 0.006424
Train Epoch: 29 [13600/17010 (80%)] Loss: 0.011108
Train Epoch: 29 [13760/17010 (81%)] Loss: 0.002222
Train Epoch: 29 [13920/17010 (82%)] Loss: 0.004041
Train Epoch: 29 [14080/17010 (83%)] Loss: 0.006757
Train Epoch: 29 [14240/17010 (84%)] Loss: 0.001828
Train Epoch: 29 [14400/17010 (85%)] Loss: 0.147562
Train Epoch: 29 [14560/17010 (86%)] Loss: 0.004815
Train Epoch: 29 [14720/17010 (87%)] Loss: 0.001128
Train Epoch: 29 [14880/17010 (87%)] Loss: 0.000800
Train Epoch: 29 [15040/17010 (88%)] Loss: 0.142185
Train Epoch: 29 [15200/17010 (89%)] Loss: 0.105596
Train Epoch: 29 [15360/17010 (90%)] Loss: 0.034869
Train Epoch: 29 [15520/17010 (91%)] Loss: 0.032277
Train Epoch: 29 [15680/17010 (92%)] Loss: 0.189216
Train Epoch: 29 [15840/17010 (93%)] Loss: 0.004536
Train Epoch: 29 [16000/17010 (94%)] Loss: 0.002014
Train Epoch: 29 [16160/17010 (95%)] Loss: 0.021849
Train Epoch: 29 [16320/17010 (96%)] Loss: 0.122383
Train Epoch: 29 [16480/17010 (97%)] Loss: 0.011786
Train Epoch: 29 [16640/17010 (98%)] Loss: 0.026172
Train Epoch: 29 [16800/17010 (99%)] Loss: 0.005245
Train Epoch: 29 [16960/17010 (100%)] Loss: 0.002686
    epoch          : 29
    Train_loss     : 0.023634437447912944
    Train_accuracy : 0.9923637218045113
    Train_top_k_acc: 0.9998825187969925
    Val_loss       : 0.19755683189141565
    Val_accuracy   : 0.9552083333333333
    Val_top_k_acc  : 0.9963541666666667
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch29.pth ...
Train Epoch: 30 [0/17010 (0%)] Loss: 0.007826
Train Epoch: 30 [160/17010 (1%)] Loss: 0.004566
Train Epoch: 30 [320/17010 (2%)] Loss: 0.023259
Train Epoch: 30 [480/17010 (3%)] Loss: 0.053859
Train Epoch: 30 [640/17010 (4%)] Loss: 0.086579
Train Epoch: 30 [800/17010 (5%)] Loss: 0.005394
Train Epoch: 30 [960/17010 (6%)] Loss: 0.000845
Train Epoch: 30 [1120/17010 (7%)] Loss: 0.048365
Train Epoch: 30 [1280/17010 (8%)] Loss: 0.123847
Train Epoch: 30 [1440/17010 (8%)] Loss: 0.001402
Train Epoch: 30 [1600/17010 (9%)] Loss: 0.011447
Train Epoch: 30 [1760/17010 (10%)] Loss: 0.001553
Train Epoch: 30 [1920/17010 (11%)] Loss: 0.008675
Train Epoch: 30 [2080/17010 (12%)] Loss: 0.052866
Train Epoch: 30 [2240/17010 (13%)] Loss: 0.255059
Train Epoch: 30 [2400/17010 (14%)] Loss: 0.017940
Train Epoch: 30 [2560/17010 (15%)] Loss: 0.003616
Train Epoch: 30 [2720/17010 (16%)] Loss: 0.012631
Train Epoch: 30 [2880/17010 (17%)] Loss: 0.004837
Train Epoch: 30 [3040/17010 (18%)] Loss: 0.003752
Train Epoch: 30 [3200/17010 (19%)] Loss: 0.018381
Train Epoch: 30 [3360/17010 (20%)] Loss: 0.057748
Train Epoch: 30 [3520/17010 (21%)] Loss: 0.014292
Train Epoch: 30 [3680/17010 (22%)] Loss: 0.005236
Train Epoch: 30 [3840/17010 (23%)] Loss: 0.008664
Train Epoch: 30 [4000/17010 (24%)] Loss: 0.007469
Train Epoch: 30 [4160/17010 (24%)] Loss: 0.008876
Train Epoch: 30 [4320/17010 (25%)] Loss: 0.010696
Train Epoch: 30 [4480/17010 (26%)] Loss: 0.008874
Train Epoch: 30 [4640/17010 (27%)] Loss: 0.000963
Train Epoch: 30 [4800/17010 (28%)] Loss: 0.001383
Train Epoch: 30 [4960/17010 (29%)] Loss: 0.017009
Train Epoch: 30 [5120/17010 (30%)] Loss: 0.001470
Train Epoch: 30 [5280/17010 (31%)] Loss: 0.000421
Train Epoch: 30 [5440/17010 (32%)] Loss: 0.004334
Train Epoch: 30 [5600/17010 (33%)] Loss: 0.001045
Train Epoch: 30 [5760/17010 (34%)] Loss: 0.009826
Train Epoch: 30 [5920/17010 (35%)] Loss: 0.019893
Train Epoch: 30 [6080/17010 (36%)] Loss: 0.001555
Train Epoch: 30 [6240/17010 (37%)] Loss: 0.003300
Train Epoch: 30 [6400/17010 (38%)] Loss: 0.002661
Train Epoch: 30 [6560/17010 (39%)] Loss: 0.003975
Train Epoch: 30 [6720/17010 (40%)] Loss: 0.012265
Train Epoch: 30 [6880/17010 (40%)] Loss: 0.113538
Train Epoch: 30 [7040/17010 (41%)] Loss: 0.005906
Train Epoch: 30 [7200/17010 (42%)] Loss: 0.005042
Train Epoch: 30 [7360/17010 (43%)] Loss: 0.013803
Train Epoch: 30 [7520/17010 (44%)] Loss: 0.324676
Train Epoch: 30 [7680/17010 (45%)] Loss: 0.079406
Train Epoch: 30 [7840/17010 (46%)] Loss: 0.108242
Train Epoch: 30 [8000/17010 (47%)] Loss: 0.013577
Train Epoch: 30 [8160/17010 (48%)] Loss: 0.003777
Train Epoch: 30 [8320/17010 (49%)] Loss: 0.003007
Train Epoch: 30 [8480/17010 (50%)] Loss: 0.007490
Train Epoch: 30 [8640/17010 (51%)] Loss: 0.002106
Train Epoch: 30 [8800/17010 (52%)] Loss: 0.040856
Train Epoch: 30 [8960/17010 (53%)] Loss: 0.008997
Train Epoch: 30 [9120/17010 (54%)] Loss: 0.037901
Train Epoch: 30 [9280/17010 (55%)] Loss: 0.270345
Train Epoch: 30 [9440/17010 (55%)] Loss: 0.009528
Train Epoch: 30 [9600/17010 (56%)] Loss: 0.001628
Train Epoch: 30 [9760/17010 (57%)] Loss: 0.000652
Train Epoch: 30 [9920/17010 (58%)] Loss: 0.013188
Train Epoch: 30 [10080/17010 (59%)] Loss: 0.001196
Train Epoch: 30 [10240/17010 (60%)] Loss: 0.000770
Train Epoch: 30 [10400/17010 (61%)] Loss: 0.000562
Train Epoch: 30 [10560/17010 (62%)] Loss: 0.000405
Train Epoch: 30 [10720/17010 (63%)] Loss: 0.006254
Train Epoch: 30 [10880/17010 (64%)] Loss: 0.057863
Train Epoch: 30 [11040/17010 (65%)] Loss: 0.154598
Train Epoch: 30 [11200/17010 (66%)] Loss: 0.086191
Train Epoch: 30 [11360/17010 (67%)] Loss: 0.011765
Train Epoch: 30 [11520/17010 (68%)] Loss: 0.003644
Train Epoch: 30 [11680/17010 (69%)] Loss: 0.041854
Train Epoch: 30 [11840/17010 (70%)] Loss: 0.001380
Train Epoch: 30 [12000/17010 (71%)] Loss: 0.001729
Train Epoch: 30 [12160/17010 (71%)] Loss: 0.041969
Train Epoch: 30 [12320/17010 (72%)] Loss: 0.008751
Train Epoch: 30 [12480/17010 (73%)] Loss: 0.022377
Train Epoch: 30 [12640/17010 (74%)] Loss: 0.090973
Train Epoch: 30 [12800/17010 (75%)] Loss: 0.016933
Train Epoch: 30 [12960/17010 (76%)] Loss: 0.027398
Train Epoch: 30 [13120/17010 (77%)] Loss: 0.001821
Train Epoch: 30 [13280/17010 (78%)] Loss: 0.011523
Train Epoch: 30 [13440/17010 (79%)] Loss: 0.003188
Train Epoch: 30 [13600/17010 (80%)] Loss: 0.075617
Train Epoch: 30 [13760/17010 (81%)] Loss: 0.000422
Train Epoch: 30 [13920/17010 (82%)] Loss: 0.297655
Train Epoch: 30 [14080/17010 (83%)] Loss: 0.006055
Train Epoch: 30 [14240/17010 (84%)] Loss: 0.001362
Train Epoch: 30 [14400/17010 (85%)] Loss: 0.009198
Train Epoch: 30 [14560/17010 (86%)] Loss: 0.015705
Train Epoch: 30 [14720/17010 (87%)] Loss: 0.003907
Train Epoch: 30 [14880/17010 (87%)] Loss: 0.039637
Train Epoch: 30 [15040/17010 (88%)] Loss: 0.004488
Train Epoch: 30 [15200/17010 (89%)] Loss: 0.145797
Train Epoch: 30 [15360/17010 (90%)] Loss: 0.026233
Train Epoch: 30 [15520/17010 (91%)] Loss: 0.002302
Train Epoch: 30 [15680/17010 (92%)] Loss: 0.012754
Train Epoch: 30 [15840/17010 (93%)] Loss: 0.130986
Train Epoch: 30 [16000/17010 (94%)] Loss: 0.024529
Train Epoch: 30 [16160/17010 (95%)] Loss: 0.003982
Train Epoch: 30 [16320/17010 (96%)] Loss: 0.000559
Train Epoch: 30 [16480/17010 (97%)] Loss: 0.001848
Train Epoch: 30 [16640/17010 (98%)] Loss: 0.109180
Train Epoch: 30 [16800/17010 (99%)] Loss: 0.004327
Train Epoch: 30 [16960/17010 (100%)] Loss: 0.006841
    epoch          : 30
    Train_loss     : 0.03187188099274548
    Train_accuracy : 0.9894854323308271
    Train_top_k_acc: 0.9999412593984962
    Val_loss       : 0.20921928439007995
    Val_accuracy   : 0.9494791666666667
    Val_top_k_acc  : 0.9973958333333334
Saving checkpoint: saved/models/efficientnet-b7/1025_183855/checkpoint-epoch30.pth ...
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.229 MB uploaded (0.000 MB deduped)wandb: \ 0.229 MB of 0.229 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:  Train_accuracy ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      Train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: Train_top_k_acc ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    Val_accuracy ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        Val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   Val_top_k_acc ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:  Train_accuracy 0.98949
wandb:      Train_loss 0.03187
wandb: Train_top_k_acc 0.99994
wandb:    Val_accuracy 0.94948
wandb:        Val_loss 0.20922
wandb:   Val_top_k_acc 0.9974
wandb: 
wandb: Synced dauntless-bird-10: https://wandb.ai/qwer55252/Boostcamp-lv1-cv1/runs/qolcfbf8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221025_183852-qolcfbf8/logs
