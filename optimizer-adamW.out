/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Currently logged in as: qwer55252. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /opt/ml/project-T4193/wandb/run-20221103_045828-18gafmhu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-dream-113
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1
wandb: üöÄ View run at https://wandb.ai/qwer55252/Boostcamp-lv1-cv1/runs/18gafmhu
Loaded pretrained weights for efficientnet-b4
EfficientNet(
  (_conv_stem): Conv2dStaticSamePadding(
    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False
    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
  )
  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_blocks): ModuleList(
    (0): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        48, 12, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        12, 48, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_depthwise_conv): Conv2dStaticSamePadding(
        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        24, 6, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        6, 24, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False
        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        144, 6, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        6, 144, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (3): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        192, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 192, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (4): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        192, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 192, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (5): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        192, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 192, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (6): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        192, 8, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        8, 192, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (7): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        336, 14, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        14, 336, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (8): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        336, 14, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        14, 336, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (9): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        336, 14, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        14, 336, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (10): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False
        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        336, 14, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        14, 336, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (11): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        672, 28, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        28, 672, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (12): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        672, 28, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        28, 672, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (13): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        672, 28, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        28, 672, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (14): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        672, 28, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        28, 672, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (15): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        672, 28, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        28, 672, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (16): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        672, 28, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        28, 672, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (17): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (18): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (19): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (20): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (21): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (22): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False
        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        960, 40, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        40, 960, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (23): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1632, 68, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        68, 1632, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (24): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1632, 68, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        68, 1632, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (25): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1632, 68, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        68, 1632, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (26): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1632, 68, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        68, 1632, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (27): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1632, 68, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        68, 1632, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (28): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1632, 68, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        68, 1632, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (29): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False
        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)
      )
      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1632, 68, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        68, 1632, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (30): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        1632, 68, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        68, 1632, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (31): MBConvBlock(
      (_expand_conv): Conv2dStaticSamePadding(
        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2dStaticSamePadding(
        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False
        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
      )
      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_se_reduce): Conv2dStaticSamePadding(
        2688, 112, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_se_expand): Conv2dStaticSamePadding(
        112, 2688, kernel_size=(1, 1), stride=(1, 1)
        (static_padding): Identity()
      )
      (_project_conv): Conv2dStaticSamePadding(
        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False
        (static_padding): Identity()
      )
      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (_conv_head): Conv2dStaticSamePadding(
    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False
    (static_padding): Identity()
  )
  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)
  (_dropout): Dropout(p=0.4, inplace=False)
  (_fc): Linear(in_features=1792, out_features=18, bias=True)
  (_swish): MemoryEfficientSwish()
)
Train Epoch: 1 [0/19432 (0%)] Loss: 2.960813
Train Epoch: 1 [160/19432 (1%)] Loss: 2.832630
Train Epoch: 1 [320/19432 (2%)] Loss: 2.697252
Train Epoch: 1 [480/19432 (2%)] Loss: 2.755505
Train Epoch: 1 [640/19432 (3%)] Loss: 2.591217
Train Epoch: 1 [800/19432 (4%)] Loss: 2.386464
Train Epoch: 1 [960/19432 (5%)] Loss: 2.247317
Train Epoch: 1 [1120/19432 (6%)] Loss: 2.163904
Train Epoch: 1 [1280/19432 (7%)] Loss: 2.024753
Train Epoch: 1 [1440/19432 (7%)] Loss: 1.733798
Train Epoch: 1 [1600/19432 (8%)] Loss: 1.643910
Train Epoch: 1 [1760/19432 (9%)] Loss: 1.366415
Train Epoch: 1 [1920/19432 (10%)] Loss: 1.355132
Train Epoch: 1 [2080/19432 (11%)] Loss: 1.210290
Train Epoch: 1 [2240/19432 (12%)] Loss: 1.076786
Train Epoch: 1 [2400/19432 (12%)] Loss: 0.890880
Train Epoch: 1 [2560/19432 (13%)] Loss: 0.903690
Train Epoch: 1 [2720/19432 (14%)] Loss: 0.763004
Train Epoch: 1 [2880/19432 (15%)] Loss: 0.570268
Train Epoch: 1 [3040/19432 (16%)] Loss: 0.663653
Train Epoch: 1 [3200/19432 (16%)] Loss: 0.575761
Train Epoch: 1 [3360/19432 (17%)] Loss: 0.829138
Train Epoch: 1 [3520/19432 (18%)] Loss: 0.651000
Train Epoch: 1 [3680/19432 (19%)] Loss: 0.766243
Train Epoch: 1 [3840/19432 (20%)] Loss: 0.560574
Train Epoch: 1 [4000/19432 (21%)] Loss: 0.748686
Train Epoch: 1 [4160/19432 (21%)] Loss: 0.534865
Train Epoch: 1 [4320/19432 (22%)] Loss: 0.668528
Train Epoch: 1 [4480/19432 (23%)] Loss: 0.364016
Train Epoch: 1 [4640/19432 (24%)] Loss: 0.310999
Train Epoch: 1 [4800/19432 (25%)] Loss: 0.501785
Train Epoch: 1 [4960/19432 (26%)] Loss: 0.462078
Train Epoch: 1 [5120/19432 (26%)] Loss: 0.622838
Train Epoch: 1 [5280/19432 (27%)] Loss: 0.343115
Train Epoch: 1 [5440/19432 (28%)] Loss: 0.270695
Train Epoch: 1 [5600/19432 (29%)] Loss: 0.157207
Train Epoch: 1 [5760/19432 (30%)] Loss: 0.359473
Train Epoch: 1 [5920/19432 (30%)] Loss: 0.325870
Train Epoch: 1 [6080/19432 (31%)] Loss: 0.387756
Train Epoch: 1 [6240/19432 (32%)] Loss: 0.279791
Train Epoch: 1 [6400/19432 (33%)] Loss: 0.447328
Train Epoch: 1 [6560/19432 (34%)] Loss: 0.462262
Train Epoch: 1 [6720/19432 (35%)] Loss: 0.332526
Train Epoch: 1 [6880/19432 (35%)] Loss: 0.355047
Train Epoch: 1 [7040/19432 (36%)] Loss: 0.239584
Train Epoch: 1 [7200/19432 (37%)] Loss: 0.461809
Train Epoch: 1 [7360/19432 (38%)] Loss: 0.467954
Train Epoch: 1 [7520/19432 (39%)] Loss: 0.452977
Train Epoch: 1 [7680/19432 (40%)] Loss: 0.701158
Train Epoch: 1 [7840/19432 (40%)] Loss: 0.490739
Train Epoch: 1 [8000/19432 (41%)] Loss: 0.309353
Train Epoch: 1 [8160/19432 (42%)] Loss: 0.202708
Train Epoch: 1 [8320/19432 (43%)] Loss: 0.303688
Train Epoch: 1 [8480/19432 (44%)] Loss: 0.251996
Train Epoch: 1 [8640/19432 (44%)] Loss: 0.350119
Train Epoch: 1 [8800/19432 (45%)] Loss: 0.290434
Train Epoch: 1 [8960/19432 (46%)] Loss: 0.464556
Train Epoch: 1 [9120/19432 (47%)] Loss: 0.138120
Train Epoch: 1 [9280/19432 (48%)] Loss: 0.274298
Train Epoch: 1 [9440/19432 (49%)] Loss: 0.290582
Train Epoch: 1 [9600/19432 (49%)] Loss: 0.291919
Train Epoch: 1 [9760/19432 (50%)] Loss: 0.409839
Train Epoch: 1 [9920/19432 (51%)] Loss: 0.278394
Train Epoch: 1 [10080/19432 (52%)] Loss: 0.170717
Train Epoch: 1 [10240/19432 (53%)] Loss: 0.255117
Train Epoch: 1 [10400/19432 (54%)] Loss: 0.208946
Train Epoch: 1 [10560/19432 (54%)] Loss: 0.464861
Train Epoch: 1 [10720/19432 (55%)] Loss: 0.372910
Train Epoch: 1 [10880/19432 (56%)] Loss: 0.303688
Train Epoch: 1 [11040/19432 (57%)] Loss: 0.395498
Train Epoch: 1 [11200/19432 (58%)] Loss: 0.173475
Train Epoch: 1 [11360/19432 (58%)] Loss: 0.186334
Train Epoch: 1 [11520/19432 (59%)] Loss: 0.216181
Train Epoch: 1 [11680/19432 (60%)] Loss: 0.173635
Train Epoch: 1 [11840/19432 (61%)] Loss: 0.420462
Train Epoch: 1 [12000/19432 (62%)] Loss: 0.239713
Train Epoch: 1 [12160/19432 (63%)] Loss: 0.232331
Train Epoch: 1 [12320/19432 (63%)] Loss: 0.328077
Train Epoch: 1 [12480/19432 (64%)] Loss: 0.282819
Train Epoch: 1 [12640/19432 (65%)] Loss: 0.097774
Train Epoch: 1 [12800/19432 (66%)] Loss: 0.408107
Train Epoch: 1 [12960/19432 (67%)] Loss: 0.133741
Train Epoch: 1 [13120/19432 (68%)] Loss: 0.129257
Train Epoch: 1 [13280/19432 (68%)] Loss: 0.226337
Train Epoch: 1 [13440/19432 (69%)] Loss: 0.107403
Train Epoch: 1 [13600/19432 (70%)] Loss: 0.087906
Train Epoch: 1 [13760/19432 (71%)] Loss: 0.189657
Train Epoch: 1 [13920/19432 (72%)] Loss: 0.120955
Train Epoch: 1 [14080/19432 (72%)] Loss: 0.276630
Train Epoch: 1 [14240/19432 (73%)] Loss: 0.218725
Train Epoch: 1 [14400/19432 (74%)] Loss: 0.291944
Train Epoch: 1 [14560/19432 (75%)] Loss: 0.167357
Train Epoch: 1 [14720/19432 (76%)] Loss: 0.358462
Train Epoch: 1 [14880/19432 (77%)] Loss: 0.269156
Train Epoch: 1 [15040/19432 (77%)] Loss: 0.428627
Train Epoch: 1 [15200/19432 (78%)] Loss: 0.153768
Train Epoch: 1 [15360/19432 (79%)] Loss: 0.330410
Train Epoch: 1 [15520/19432 (80%)] Loss: 0.523269
Train Epoch: 1 [15680/19432 (81%)] Loss: 0.317201
Train Epoch: 1 [15840/19432 (82%)] Loss: 0.119345
Train Epoch: 1 [16000/19432 (82%)] Loss: 0.140111
Train Epoch: 1 [16160/19432 (83%)] Loss: 0.181224
Train Epoch: 1 [16320/19432 (84%)] Loss: 0.179578
Train Epoch: 1 [16480/19432 (85%)] Loss: 0.178049
Train Epoch: 1 [16640/19432 (86%)] Loss: 0.259673
Train Epoch: 1 [16800/19432 (86%)] Loss: 0.143836
Train Epoch: 1 [16960/19432 (87%)] Loss: 0.257117
Train Epoch: 1 [17120/19432 (88%)] Loss: 0.178227
Train Epoch: 1 [17280/19432 (89%)] Loss: 0.306930
Train Epoch: 1 [17440/19432 (90%)] Loss: 0.263478
Train Epoch: 1 [17600/19432 (91%)] Loss: 0.117687
Train Epoch: 1 [17760/19432 (91%)] Loss: 0.168657
Train Epoch: 1 [17920/19432 (92%)] Loss: 0.362775
Train Epoch: 1 [18080/19432 (93%)] Loss: 0.202924
Train Epoch: 1 [18240/19432 (94%)] Loss: 0.253810
Train Epoch: 1 [18400/19432 (95%)] Loss: 0.215832
Train Epoch: 1 [18560/19432 (96%)] Loss: 0.205562
Train Epoch: 1 [18720/19432 (96%)] Loss: 0.111061
Train Epoch: 1 [18880/19432 (97%)] Loss: 0.263315
Train Epoch: 1 [19040/19432 (98%)] Loss: 0.117042
Train Epoch: 1 [19200/19432 (99%)] Loss: 0.092003
Train Epoch: 1 [19360/19432 (100%)] Loss: 0.266526
    epoch          : 1
    Train_loss     : 0.5558794781467632
    Train_accuracy : 0.8418482730263158
    Train_f1_score : 0.7599368691444397
    Val_loss       : 0.19624234494917533
    Val_accuracy   : 0.9315257352941176
    Val_f1_score   : 0.8846650719642639
Warning: Metric 'val_loss' is not found. Model performance monitoring is disabled.
Train Epoch: 2 [0/19432 (0%)] Loss: 0.359413
Train Epoch: 2 [160/19432 (1%)] Loss: 0.136333
Train Epoch: 2 [320/19432 (2%)] Loss: 0.095439
Train Epoch: 2 [480/19432 (2%)] Loss: 0.202053
Train Epoch: 2 [640/19432 (3%)] Loss: 0.213871
Train Epoch: 2 [800/19432 (4%)] Loss: 0.231400
Train Epoch: 2 [960/19432 (5%)] Loss: 0.245238
Train Epoch: 2 [1120/19432 (6%)] Loss: 0.266787
Train Epoch: 2 [1280/19432 (7%)] Loss: 0.132119
Train Epoch: 2 [1440/19432 (7%)] Loss: 0.282291
Train Epoch: 2 [1600/19432 (8%)] Loss: 0.130476
Train Epoch: 2 [1760/19432 (9%)] Loss: 0.208760
Train Epoch: 2 [1920/19432 (10%)] Loss: 0.220950
Train Epoch: 2 [2080/19432 (11%)] Loss: 0.066296
Train Epoch: 2 [2240/19432 (12%)] Loss: 0.188900
Train Epoch: 2 [2400/19432 (12%)] Loss: 0.167438
Train Epoch: 2 [2560/19432 (13%)] Loss: 0.182720
Train Epoch: 2 [2720/19432 (14%)] Loss: 0.067534
Train Epoch: 2 [2880/19432 (15%)] Loss: 0.203145
Train Epoch: 2 [3040/19432 (16%)] Loss: 0.183605
Train Epoch: 2 [3200/19432 (16%)] Loss: 0.267044
Train Epoch: 2 [3360/19432 (17%)] Loss: 0.243605
Train Epoch: 2 [3520/19432 (18%)] Loss: 0.032840
Train Epoch: 2 [3680/19432 (19%)] Loss: 0.117626
Train Epoch: 2 [3840/19432 (20%)] Loss: 0.040027
Train Epoch: 2 [4000/19432 (21%)] Loss: 0.035735
Train Epoch: 2 [4160/19432 (21%)] Loss: 0.144221
Train Epoch: 2 [4320/19432 (22%)] Loss: 0.184147
Train Epoch: 2 [4480/19432 (23%)] Loss: 0.073041
Train Epoch: 2 [4640/19432 (24%)] Loss: 0.261003
Train Epoch: 2 [4800/19432 (25%)] Loss: 0.457281
Train Epoch: 2 [4960/19432 (26%)] Loss: 0.147834
Train Epoch: 2 [5120/19432 (26%)] Loss: 0.260286
Train Epoch: 2 [5280/19432 (27%)] Loss: 0.113115
Train Epoch: 2 [5440/19432 (28%)] Loss: 0.295499
Train Epoch: 2 [5600/19432 (29%)] Loss: 0.129282
Train Epoch: 2 [5760/19432 (30%)] Loss: 0.022062
Train Epoch: 2 [5920/19432 (30%)] Loss: 0.160745
Train Epoch: 2 [6080/19432 (31%)] Loss: 0.207488
Train Epoch: 2 [6240/19432 (32%)] Loss: 0.201679
Train Epoch: 2 [6400/19432 (33%)] Loss: 0.239952
Train Epoch: 2 [6560/19432 (34%)] Loss: 0.139672
Train Epoch: 2 [6720/19432 (35%)] Loss: 0.134148
Train Epoch: 2 [6880/19432 (35%)] Loss: 0.193212
Train Epoch: 2 [7040/19432 (36%)] Loss: 0.061532
Train Epoch: 2 [7200/19432 (37%)] Loss: 0.251395
Train Epoch: 2 [7360/19432 (38%)] Loss: 0.086874
Train Epoch: 2 [7520/19432 (39%)] Loss: 0.187991
Train Epoch: 2 [7680/19432 (40%)] Loss: 0.108636
Train Epoch: 2 [7840/19432 (40%)] Loss: 0.090983
Train Epoch: 2 [8000/19432 (41%)] Loss: 0.069984
Train Epoch: 2 [8160/19432 (42%)] Loss: 0.069682
Train Epoch: 2 [8320/19432 (43%)] Loss: 0.128460
Train Epoch: 2 [8480/19432 (44%)] Loss: 0.231337
Train Epoch: 2 [8640/19432 (44%)] Loss: 0.206226
Train Epoch: 2 [8800/19432 (45%)] Loss: 0.242967
Train Epoch: 2 [8960/19432 (46%)] Loss: 0.097784
Train Epoch: 2 [9120/19432 (47%)] Loss: 0.059547
Train Epoch: 2 [9280/19432 (48%)] Loss: 0.036664
Train Epoch: 2 [9440/19432 (49%)] Loss: 0.359724
Train Epoch: 2 [9600/19432 (49%)] Loss: 0.062960
Train Epoch: 2 [9760/19432 (50%)] Loss: 0.086161
Train Epoch: 2 [9920/19432 (51%)] Loss: 0.175754
Train Epoch: 2 [10080/19432 (52%)] Loss: 0.076405
Train Epoch: 2 [10240/19432 (53%)] Loss: 0.167514
Train Epoch: 2 [10400/19432 (54%)] Loss: 0.019303
Train Epoch: 2 [10560/19432 (54%)] Loss: 0.090421
Train Epoch: 2 [10720/19432 (55%)] Loss: 0.210510
Train Epoch: 2 [10880/19432 (56%)] Loss: 0.324072
Train Epoch: 2 [11040/19432 (57%)] Loss: 0.487126
Train Epoch: 2 [11200/19432 (58%)] Loss: 0.092480
Train Epoch: 2 [11360/19432 (58%)] Loss: 0.144920
Train Epoch: 2 [11520/19432 (59%)] Loss: 0.274286
Train Epoch: 2 [11680/19432 (60%)] Loss: 0.245792
Train Epoch: 2 [11840/19432 (61%)] Loss: 0.024077
Train Epoch: 2 [12000/19432 (62%)] Loss: 0.242065
Train Epoch: 2 [12160/19432 (63%)] Loss: 0.261597
Train Epoch: 2 [12320/19432 (63%)] Loss: 0.059385
Train Epoch: 2 [12480/19432 (64%)] Loss: 0.089909
Train Epoch: 2 [12640/19432 (65%)] Loss: 0.104947
Train Epoch: 2 [12800/19432 (66%)] Loss: 0.170167
Train Epoch: 2 [12960/19432 (67%)] Loss: 0.168607
Train Epoch: 2 [13120/19432 (68%)] Loss: 0.022913
Train Epoch: 2 [13280/19432 (68%)] Loss: 0.109387
Train Epoch: 2 [13440/19432 (69%)] Loss: 0.050708
Train Epoch: 2 [13600/19432 (70%)] Loss: 0.043666
Train Epoch: 2 [13760/19432 (71%)] Loss: 0.060023
Train Epoch: 2 [13920/19432 (72%)] Loss: 0.159358
Train Epoch: 2 [14080/19432 (72%)] Loss: 0.124867
Train Epoch: 2 [14240/19432 (73%)] Loss: 0.263465
Train Epoch: 2 [14400/19432 (74%)] Loss: 0.332430
Train Epoch: 2 [14560/19432 (75%)] Loss: 0.279101
Train Epoch: 2 [14720/19432 (76%)] Loss: 0.271047
Train Epoch: 2 [14880/19432 (77%)] Loss: 0.176980
Train Epoch: 2 [15040/19432 (77%)] Loss: 0.117426
Train Epoch: 2 [15200/19432 (78%)] Loss: 0.150685
Train Epoch: 2 [15360/19432 (79%)] Loss: 0.201109
Train Epoch: 2 [15520/19432 (80%)] Loss: 0.174479
Train Epoch: 2 [15680/19432 (81%)] Loss: 0.184593
Train Epoch: 2 [15840/19432 (82%)] Loss: 0.068213
Train Epoch: 2 [16000/19432 (82%)] Loss: 0.245979
Train Epoch: 2 [16160/19432 (83%)] Loss: 0.175216
Train Epoch: 2 [16320/19432 (84%)] Loss: 0.081333
Train Epoch: 2 [16480/19432 (85%)] Loss: 0.195314
Train Epoch: 2 [16640/19432 (86%)] Loss: 0.086213
Train Epoch: 2 [16800/19432 (86%)] Loss: 0.089869
Train Epoch: 2 [16960/19432 (87%)] Loss: 0.203105
Train Epoch: 2 [17120/19432 (88%)] Loss: 0.134881
Train Epoch: 2 [17280/19432 (89%)] Loss: 0.049736
Train Epoch: 2 [17440/19432 (90%)] Loss: 0.213077
Train Epoch: 2 [17600/19432 (91%)] Loss: 0.257039
Train Epoch: 2 [17760/19432 (91%)] Loss: 0.045277
Train Epoch: 2 [17920/19432 (92%)] Loss: 0.069932
Train Epoch: 2 [18080/19432 (93%)] Loss: 0.125546
Train Epoch: 2 [18240/19432 (94%)] Loss: 0.068978
Train Epoch: 2 [18400/19432 (95%)] Loss: 0.291343
Train Epoch: 2 [18560/19432 (96%)] Loss: 0.280559
Train Epoch: 2 [18720/19432 (96%)] Loss: 0.033439
Train Epoch: 2 [18880/19432 (97%)] Loss: 0.119152
Train Epoch: 2 [19040/19432 (98%)] Loss: 0.295659
Train Epoch: 2 [19200/19432 (99%)] Loss: 0.064312
Train Epoch: 2 [19360/19432 (100%)] Loss: 0.047342
    epoch          : 2
    Train_loss     : 0.15576550403707906
    Train_accuracy : 0.944592927631579
    Train_f1_score : 0.9100604057312012
    Val_loss       : 0.19385229182936362
    Val_accuracy   : 0.9443933823529411
    Val_f1_score   : 0.9206848740577698
Saving checkpoint: saved/models/split-example/1103_045831/checkpoint-epoch2.pth ...
Train Epoch: 3 [0/19432 (0%)] Loss: 0.055272
Train Epoch: 3 [160/19432 (1%)] Loss: 0.050428
Train Epoch: 3 [320/19432 (2%)] Loss: 0.064169
Train Epoch: 3 [480/19432 (2%)] Loss: 0.201525
Train Epoch: 3 [640/19432 (3%)] Loss: 0.074594
Train Epoch: 3 [800/19432 (4%)] Loss: 0.160277
Train Epoch: 3 [960/19432 (5%)] Loss: 0.018209
Train Epoch: 3 [1120/19432 (6%)] Loss: 0.121331
Train Epoch: 3 [1280/19432 (7%)] Loss: 0.014856
Train Epoch: 3 [1440/19432 (7%)] Loss: 0.117111
Train Epoch: 3 [1600/19432 (8%)] Loss: 0.063482
Train Epoch: 3 [1760/19432 (9%)] Loss: 0.070466
Train Epoch: 3 [1920/19432 (10%)] Loss: 0.052932
Train Epoch: 3 [2080/19432 (11%)] Loss: 0.133467
Train Epoch: 3 [2240/19432 (12%)] Loss: 0.109622
Train Epoch: 3 [2400/19432 (12%)] Loss: 0.122274
Train Epoch: 3 [2560/19432 (13%)] Loss: 0.151361
Train Epoch: 3 [2720/19432 (14%)] Loss: 0.089888
Train Epoch: 3 [2880/19432 (15%)] Loss: 0.021692
Train Epoch: 3 [3040/19432 (16%)] Loss: 0.029634
Train Epoch: 3 [3200/19432 (16%)] Loss: 0.089256
Train Epoch: 3 [3360/19432 (17%)] Loss: 0.011544
Train Epoch: 3 [3520/19432 (18%)] Loss: 0.058625
Train Epoch: 3 [3680/19432 (19%)] Loss: 0.015997
Train Epoch: 3 [3840/19432 (20%)] Loss: 0.109370
Train Epoch: 3 [4000/19432 (21%)] Loss: 0.143135
Train Epoch: 3 [4160/19432 (21%)] Loss: 0.017158
Train Epoch: 3 [4320/19432 (22%)] Loss: 0.044090
Train Epoch: 3 [4480/19432 (23%)] Loss: 0.045910
Train Epoch: 3 [4640/19432 (24%)] Loss: 0.108326
Train Epoch: 3 [4800/19432 (25%)] Loss: 0.209375
Train Epoch: 3 [4960/19432 (26%)] Loss: 0.094200
Train Epoch: 3 [5120/19432 (26%)] Loss: 0.194440
Train Epoch: 3 [5280/19432 (27%)] Loss: 0.027121
Train Epoch: 3 [5440/19432 (28%)] Loss: 0.170169
Train Epoch: 3 [5600/19432 (29%)] Loss: 0.324992
Train Epoch: 3 [5760/19432 (30%)] Loss: 0.044253
Train Epoch: 3 [5920/19432 (30%)] Loss: 0.034815
Train Epoch: 3 [6080/19432 (31%)] Loss: 0.025860
Train Epoch: 3 [6240/19432 (32%)] Loss: 0.075843
Train Epoch: 3 [6400/19432 (33%)] Loss: 0.235264
Train Epoch: 3 [6560/19432 (34%)] Loss: 0.017554
Train Epoch: 3 [6720/19432 (35%)] Loss: 0.106823
Train Epoch: 3 [6880/19432 (35%)] Loss: 0.059108
Train Epoch: 3 [7040/19432 (36%)] Loss: 0.118744
Train Epoch: 3 [7200/19432 (37%)] Loss: 0.031064
Train Epoch: 3 [7360/19432 (38%)] Loss: 0.276004
Train Epoch: 3 [7520/19432 (39%)] Loss: 0.015175
Train Epoch: 3 [7680/19432 (40%)] Loss: 0.102780
Train Epoch: 3 [7840/19432 (40%)] Loss: 0.218824
Train Epoch: 3 [8000/19432 (41%)] Loss: 0.100146
Train Epoch: 3 [8160/19432 (42%)] Loss: 0.009361
Train Epoch: 3 [8320/19432 (43%)] Loss: 0.147818
Train Epoch: 3 [8480/19432 (44%)] Loss: 0.034550
Train Epoch: 3 [8640/19432 (44%)] Loss: 0.047859
Train Epoch: 3 [8800/19432 (45%)] Loss: 0.076511
Train Epoch: 3 [8960/19432 (46%)] Loss: 0.012475
Train Epoch: 3 [9120/19432 (47%)] Loss: 0.031561
Train Epoch: 3 [9280/19432 (48%)] Loss: 0.044899
Train Epoch: 3 [9440/19432 (49%)] Loss: 0.075340
Train Epoch: 3 [9600/19432 (49%)] Loss: 0.008621
Train Epoch: 3 [9760/19432 (50%)] Loss: 0.102064
Train Epoch: 3 [9920/19432 (51%)] Loss: 0.036770
Train Epoch: 3 [10080/19432 (52%)] Loss: 0.017295
Train Epoch: 3 [10240/19432 (53%)] Loss: 0.122250
Train Epoch: 3 [10400/19432 (54%)] Loss: 0.056085
Train Epoch: 3 [10560/19432 (54%)] Loss: 0.159408
Train Epoch: 3 [10720/19432 (55%)] Loss: 0.067220
Train Epoch: 3 [10880/19432 (56%)] Loss: 0.021425
Train Epoch: 3 [11040/19432 (57%)] Loss: 0.127288
Train Epoch: 3 [11200/19432 (58%)] Loss: 0.031543
Train Epoch: 3 [11360/19432 (58%)] Loss: 0.103261
Train Epoch: 3 [11520/19432 (59%)] Loss: 0.061232
Train Epoch: 3 [11680/19432 (60%)] Loss: 0.147758
Train Epoch: 3 [11840/19432 (61%)] Loss: 0.055223
Train Epoch: 3 [12000/19432 (62%)] Loss: 0.118573
Train Epoch: 3 [12160/19432 (63%)] Loss: 0.078606
Train Epoch: 3 [12320/19432 (63%)] Loss: 0.051888
Train Epoch: 3 [12480/19432 (64%)] Loss: 0.059541
Train Epoch: 3 [12640/19432 (65%)] Loss: 0.206838
Train Epoch: 3 [12800/19432 (66%)] Loss: 0.188827
Train Epoch: 3 [12960/19432 (67%)] Loss: 0.086888
Train Epoch: 3 [13120/19432 (68%)] Loss: 0.024223
Train Epoch: 3 [13280/19432 (68%)] Loss: 0.034314
Train Epoch: 3 [13440/19432 (69%)] Loss: 0.058658
Train Epoch: 3 [13600/19432 (70%)] Loss: 0.056104
Train Epoch: 3 [13760/19432 (71%)] Loss: 0.023158
Train Epoch: 3 [13920/19432 (72%)] Loss: 0.019177
Train Epoch: 3 [14080/19432 (72%)] Loss: 0.182900
Train Epoch: 3 [14240/19432 (73%)] Loss: 0.012936
Train Epoch: 3 [14400/19432 (74%)] Loss: 0.050793
Train Epoch: 3 [14560/19432 (75%)] Loss: 0.033645
Train Epoch: 3 [14720/19432 (76%)] Loss: 0.029991
Train Epoch: 3 [14880/19432 (77%)] Loss: 0.050843
Train Epoch: 3 [15040/19432 (77%)] Loss: 0.022970
Train Epoch: 3 [15200/19432 (78%)] Loss: 0.097487
Train Epoch: 3 [15360/19432 (79%)] Loss: 0.005344
Train Epoch: 3 [15520/19432 (80%)] Loss: 0.153528
Train Epoch: 3 [15680/19432 (81%)] Loss: 0.047718
Train Epoch: 3 [15840/19432 (82%)] Loss: 0.021082
Train Epoch: 3 [16000/19432 (82%)] Loss: 0.049889
Train Epoch: 3 [16160/19432 (83%)] Loss: 0.040580
Train Epoch: 3 [16320/19432 (84%)] Loss: 0.034876
Train Epoch: 3 [16480/19432 (85%)] Loss: 0.018933
Train Epoch: 3 [16640/19432 (86%)] Loss: 0.215821
Train Epoch: 3 [16800/19432 (86%)] Loss: 0.125903
Train Epoch: 3 [16960/19432 (87%)] Loss: 0.028199
Train Epoch: 3 [17120/19432 (88%)] Loss: 0.038203
Train Epoch: 3 [17280/19432 (89%)] Loss: 0.047608
Train Epoch: 3 [17440/19432 (90%)] Loss: 0.017687
Train Epoch: 3 [17600/19432 (91%)] Loss: 0.128614
Train Epoch: 3 [17760/19432 (91%)] Loss: 0.025333
Train Epoch: 3 [17920/19432 (92%)] Loss: 0.014664
Train Epoch: 3 [18080/19432 (93%)] Loss: 0.171866
Train Epoch: 3 [18240/19432 (94%)] Loss: 0.049238
Train Epoch: 3 [18400/19432 (95%)] Loss: 0.039605
Train Epoch: 3 [18560/19432 (96%)] Loss: 0.030870
Train Epoch: 3 [18720/19432 (96%)] Loss: 0.108653
Train Epoch: 3 [18880/19432 (97%)] Loss: 0.042330
Train Epoch: 3 [19040/19432 (98%)] Loss: 0.064654
Train Epoch: 3 [19200/19432 (99%)] Loss: 0.030925
Train Epoch: 3 [19360/19432 (100%)] Loss: 0.031629
    epoch          : 3
    Train_loss     : 0.08364797988829914
    Train_accuracy : 0.9717824835526315
    Train_f1_score : 0.9512419700622559
    Val_loss       : 0.2682510786210461
    Val_accuracy   : 0.9241727941176471
    Val_f1_score   : 0.8914446830749512
Train Epoch: 4 [0/19432 (0%)] Loss: 0.008520
Train Epoch: 4 [160/19432 (1%)] Loss: 0.138847
Train Epoch: 4 [320/19432 (2%)] Loss: 0.137022
Train Epoch: 4 [480/19432 (2%)] Loss: 0.038397
Train Epoch: 4 [640/19432 (3%)] Loss: 0.022135
Train Epoch: 4 [800/19432 (4%)] Loss: 0.025783
Train Epoch: 4 [960/19432 (5%)] Loss: 0.018895
Train Epoch: 4 [1120/19432 (6%)] Loss: 0.013937
Train Epoch: 4 [1280/19432 (7%)] Loss: 0.065356
Train Epoch: 4 [1440/19432 (7%)] Loss: 0.017356
Train Epoch: 4 [1600/19432 (8%)] Loss: 0.207768
Train Epoch: 4 [1760/19432 (9%)] Loss: 0.027436
Train Epoch: 4 [1920/19432 (10%)] Loss: 0.122652
Train Epoch: 4 [2080/19432 (11%)] Loss: 0.020180
Train Epoch: 4 [2240/19432 (12%)] Loss: 0.061690
Train Epoch: 4 [2400/19432 (12%)] Loss: 0.004607
Train Epoch: 4 [2560/19432 (13%)] Loss: 0.066947
Train Epoch: 4 [2720/19432 (14%)] Loss: 0.019240
Train Epoch: 4 [2880/19432 (15%)] Loss: 0.048760
Train Epoch: 4 [3040/19432 (16%)] Loss: 0.023207
Train Epoch: 4 [3200/19432 (16%)] Loss: 0.032222
Train Epoch: 4 [3360/19432 (17%)] Loss: 0.066075
Train Epoch: 4 [3520/19432 (18%)] Loss: 0.045962
Train Epoch: 4 [3680/19432 (19%)] Loss: 0.013791
Train Epoch: 4 [3840/19432 (20%)] Loss: 0.079825
Train Epoch: 4 [4000/19432 (21%)] Loss: 0.011065
Train Epoch: 4 [4160/19432 (21%)] Loss: 0.024533
Train Epoch: 4 [4320/19432 (22%)] Loss: 0.007223
Train Epoch: 4 [4480/19432 (23%)] Loss: 0.040532
Train Epoch: 4 [4640/19432 (24%)] Loss: 0.025831
Train Epoch: 4 [4800/19432 (25%)] Loss: 0.033633
Train Epoch: 4 [4960/19432 (26%)] Loss: 0.055329
Train Epoch: 4 [5120/19432 (26%)] Loss: 0.036200
Train Epoch: 4 [5280/19432 (27%)] Loss: 0.102927
Train Epoch: 4 [5440/19432 (28%)] Loss: 0.040103
Train Epoch: 4 [5600/19432 (29%)] Loss: 0.250126
Train Epoch: 4 [5760/19432 (30%)] Loss: 0.030990
Train Epoch: 4 [5920/19432 (30%)] Loss: 0.031902
Train Epoch: 4 [6080/19432 (31%)] Loss: 0.015090
Train Epoch: 4 [6240/19432 (32%)] Loss: 0.030646
Train Epoch: 4 [6400/19432 (33%)] Loss: 0.014937
Train Epoch: 4 [6560/19432 (34%)] Loss: 0.037283
Train Epoch: 4 [6720/19432 (35%)] Loss: 0.068754
Train Epoch: 4 [6880/19432 (35%)] Loss: 0.070310
Train Epoch: 4 [7040/19432 (36%)] Loss: 0.004688
Train Epoch: 4 [7200/19432 (37%)] Loss: 0.013182
Train Epoch: 4 [7360/19432 (38%)] Loss: 0.022135
Train Epoch: 4 [7520/19432 (39%)] Loss: 0.019339
Train Epoch: 4 [7680/19432 (40%)] Loss: 0.009633
Train Epoch: 4 [7840/19432 (40%)] Loss: 0.132850
Train Epoch: 4 [8000/19432 (41%)] Loss: 0.009885
Train Epoch: 4 [8160/19432 (42%)] Loss: 0.017062
Train Epoch: 4 [8320/19432 (43%)] Loss: 0.032789
Train Epoch: 4 [8480/19432 (44%)] Loss: 0.017205
Train Epoch: 4 [8640/19432 (44%)] Loss: 0.032381
Train Epoch: 4 [8800/19432 (45%)] Loss: 0.093729
Train Epoch: 4 [8960/19432 (46%)] Loss: 0.052044
Train Epoch: 4 [9120/19432 (47%)] Loss: 0.011365
Train Epoch: 4 [9280/19432 (48%)] Loss: 0.020464
Train Epoch: 4 [9440/19432 (49%)] Loss: 0.094953
Train Epoch: 4 [9600/19432 (49%)] Loss: 0.043147
Train Epoch: 4 [9760/19432 (50%)] Loss: 0.027756
Train Epoch: 4 [9920/19432 (51%)] Loss: 0.130177
Train Epoch: 4 [10080/19432 (52%)] Loss: 0.074979
Train Epoch: 4 [10240/19432 (53%)] Loss: 0.008384
Train Epoch: 4 [10400/19432 (54%)] Loss: 0.064297
Train Epoch: 4 [10560/19432 (54%)] Loss: 0.030028
Train Epoch: 4 [10720/19432 (55%)] Loss: 0.009263
Train Epoch: 4 [10880/19432 (56%)] Loss: 0.049033
Train Epoch: 4 [11040/19432 (57%)] Loss: 0.029330
Train Epoch: 4 [11200/19432 (58%)] Loss: 0.069629
Train Epoch: 4 [11360/19432 (58%)] Loss: 0.011352
Train Epoch: 4 [11520/19432 (59%)] Loss: 0.011993
Train Epoch: 4 [11680/19432 (60%)] Loss: 0.050818
Train Epoch: 4 [11840/19432 (61%)] Loss: 0.012483
Train Epoch: 4 [12000/19432 (62%)] Loss: 0.120884
Train Epoch: 4 [12160/19432 (63%)] Loss: 0.023648
Train Epoch: 4 [12320/19432 (63%)] Loss: 0.022841
Train Epoch: 4 [12480/19432 (64%)] Loss: 0.037711
Train Epoch: 4 [12640/19432 (65%)] Loss: 0.027563
Train Epoch: 4 [12800/19432 (66%)] Loss: 0.020098
Train Epoch: 4 [12960/19432 (67%)] Loss: 0.047987
Train Epoch: 4 [13120/19432 (68%)] Loss: 0.018493
Train Epoch: 4 [13280/19432 (68%)] Loss: 0.046408
Train Epoch: 4 [13440/19432 (69%)] Loss: 0.022148
Train Epoch: 4 [13600/19432 (70%)] Loss: 0.129380
Train Epoch: 4 [13760/19432 (71%)] Loss: 0.035359
Train Epoch: 4 [13920/19432 (72%)] Loss: 0.013704
Train Epoch: 4 [14080/19432 (72%)] Loss: 0.013946
Train Epoch: 4 [14240/19432 (73%)] Loss: 0.016017
Train Epoch: 4 [14400/19432 (74%)] Loss: 0.052493
Train Epoch: 4 [14560/19432 (75%)] Loss: 0.016048
Train Epoch: 4 [14720/19432 (76%)] Loss: 0.080146
Train Epoch: 4 [14880/19432 (77%)] Loss: 0.164692
Train Epoch: 4 [15040/19432 (77%)] Loss: 0.006619
Train Epoch: 4 [15200/19432 (78%)] Loss: 0.007628
Train Epoch: 4 [15360/19432 (79%)] Loss: 0.010104
Train Epoch: 4 [15520/19432 (80%)] Loss: 0.015064
Train Epoch: 4 [15680/19432 (81%)] Loss: 0.034547
Train Epoch: 4 [15840/19432 (82%)] Loss: 0.023703
Train Epoch: 4 [16000/19432 (82%)] Loss: 0.011885
Train Epoch: 4 [16160/19432 (83%)] Loss: 0.003313
Train Epoch: 4 [16320/19432 (84%)] Loss: 0.008222
Train Epoch: 4 [16480/19432 (85%)] Loss: 0.019654
Train Epoch: 4 [16640/19432 (86%)] Loss: 0.058893
Train Epoch: 4 [16800/19432 (86%)] Loss: 0.062479
Train Epoch: 4 [16960/19432 (87%)] Loss: 0.005067
Train Epoch: 4 [17120/19432 (88%)] Loss: 0.041213
Train Epoch: 4 [17280/19432 (89%)] Loss: 0.027251
Train Epoch: 4 [17440/19432 (90%)] Loss: 0.074663
Train Epoch: 4 [17600/19432 (91%)] Loss: 0.005756
Train Epoch: 4 [17760/19432 (91%)] Loss: 0.007484
Train Epoch: 4 [17920/19432 (92%)] Loss: 0.037950
Train Epoch: 4 [18080/19432 (93%)] Loss: 0.079291
Train Epoch: 4 [18240/19432 (94%)] Loss: 0.022755
Train Epoch: 4 [18400/19432 (95%)] Loss: 0.028945
Train Epoch: 4 [18560/19432 (96%)] Loss: 0.024146
Train Epoch: 4 [18720/19432 (96%)] Loss: 0.219307
Train Epoch: 4 [18880/19432 (97%)] Loss: 0.052341
Train Epoch: 4 [19040/19432 (98%)] Loss: 0.030020
Train Epoch: 4 [19200/19432 (99%)] Loss: 0.016664
Train Epoch: 4 [19360/19432 (100%)] Loss: 0.006996
    epoch          : 4
    Train_loss     : 0.0449487244914053
    Train_accuracy : 0.9866365131578947
    Train_f1_score : 0.9732630848884583
    Val_loss       : 0.25041530006017315
    Val_accuracy   : 0.9352022058823529
    Val_f1_score   : 0.9013035893440247
Saving checkpoint: saved/models/split-example/1103_045831/checkpoint-epoch4.pth ...
Train Epoch: 5 [0/19432 (0%)] Loss: 0.037847
Train Epoch: 5 [160/19432 (1%)] Loss: 0.004540
Train Epoch: 5 [320/19432 (2%)] Loss: 0.012215
Train Epoch: 5 [480/19432 (2%)] Loss: 0.029372
Train Epoch: 5 [640/19432 (3%)] Loss: 0.058794
Train Epoch: 5 [800/19432 (4%)] Loss: 0.007782
Train Epoch: 5 [960/19432 (5%)] Loss: 0.007724
Train Epoch: 5 [1120/19432 (6%)] Loss: 0.011450
Train Epoch: 5 [1280/19432 (7%)] Loss: 0.025207
Train Epoch: 5 [1440/19432 (7%)] Loss: 0.018623
Train Epoch: 5 [1600/19432 (8%)] Loss: 0.073558
Train Epoch: 5 [1760/19432 (9%)] Loss: 0.158262
Train Epoch: 5 [1920/19432 (10%)] Loss: 0.033739
Train Epoch: 5 [2080/19432 (11%)] Loss: 0.014836
Train Epoch: 5 [2240/19432 (12%)] Loss: 0.041978
Train Epoch: 5 [2400/19432 (12%)] Loss: 0.002038
Train Epoch: 5 [2560/19432 (13%)] Loss: 0.031180
Train Epoch: 5 [2720/19432 (14%)] Loss: 0.017749
Train Epoch: 5 [2880/19432 (15%)] Loss: 0.021122
Train Epoch: 5 [3040/19432 (16%)] Loss: 0.012402
Train Epoch: 5 [3200/19432 (16%)] Loss: 0.040108
Train Epoch: 5 [3360/19432 (17%)] Loss: 0.036207
Train Epoch: 5 [3520/19432 (18%)] Loss: 0.043807
Train Epoch: 5 [3680/19432 (19%)] Loss: 0.019540
Train Epoch: 5 [3840/19432 (20%)] Loss: 0.015396
Train Epoch: 5 [4000/19432 (21%)] Loss: 0.018739
Train Epoch: 5 [4160/19432 (21%)] Loss: 0.002270
Train Epoch: 5 [4320/19432 (22%)] Loss: 0.006174
Train Epoch: 5 [4480/19432 (23%)] Loss: 0.025640
Train Epoch: 5 [4640/19432 (24%)] Loss: 0.078342
Train Epoch: 5 [4800/19432 (25%)] Loss: 0.010011
Train Epoch: 5 [4960/19432 (26%)] Loss: 0.007893
Train Epoch: 5 [5120/19432 (26%)] Loss: 0.021943
Train Epoch: 5 [5280/19432 (27%)] Loss: 0.012685
Train Epoch: 5 [5440/19432 (28%)] Loss: 0.018240
Train Epoch: 5 [5600/19432 (29%)] Loss: 0.015672
Train Epoch: 5 [5760/19432 (30%)] Loss: 0.076207
Train Epoch: 5 [5920/19432 (30%)] Loss: 0.004650
Train Epoch: 5 [6080/19432 (31%)] Loss: 0.034855
Train Epoch: 5 [6240/19432 (32%)] Loss: 0.018918
Train Epoch: 5 [6400/19432 (33%)] Loss: 0.002196
Train Epoch: 5 [6560/19432 (34%)] Loss: 0.032940
Train Epoch: 5 [6720/19432 (35%)] Loss: 0.014723
Train Epoch: 5 [6880/19432 (35%)] Loss: 0.010443
Train Epoch: 5 [7040/19432 (36%)] Loss: 0.084517
Train Epoch: 5 [7200/19432 (37%)] Loss: 0.031754
Train Epoch: 5 [7360/19432 (38%)] Loss: 0.009111
Train Epoch: 5 [7520/19432 (39%)] Loss: 0.006992
Train Epoch: 5 [7680/19432 (40%)] Loss: 0.025371
Train Epoch: 5 [7840/19432 (40%)] Loss: 0.015591
Train Epoch: 5 [8000/19432 (41%)] Loss: 0.022451
Train Epoch: 5 [8160/19432 (42%)] Loss: 0.006351
Train Epoch: 5 [8320/19432 (43%)] Loss: 0.006098
Train Epoch: 5 [8480/19432 (44%)] Loss: 0.002428
Train Epoch: 5 [8640/19432 (44%)] Loss: 0.050791
Train Epoch: 5 [8800/19432 (45%)] Loss: 0.002386
Train Epoch: 5 [8960/19432 (46%)] Loss: 0.095636
Train Epoch: 5 [9120/19432 (47%)] Loss: 0.005053
Train Epoch: 5 [9280/19432 (48%)] Loss: 0.032408
Train Epoch: 5 [9440/19432 (49%)] Loss: 0.053165
Train Epoch: 5 [9600/19432 (49%)] Loss: 0.022074
Train Epoch: 5 [9760/19432 (50%)] Loss: 0.013532
Train Epoch: 5 [9920/19432 (51%)] Loss: 0.141316
Train Epoch: 5 [10080/19432 (52%)] Loss: 0.009351
Train Epoch: 5 [10240/19432 (53%)] Loss: 0.106774
Train Epoch: 5 [10400/19432 (54%)] Loss: 0.007551
Train Epoch: 5 [10560/19432 (54%)] Loss: 0.116883
Train Epoch: 5 [10720/19432 (55%)] Loss: 0.051756
Train Epoch: 5 [10880/19432 (56%)] Loss: 0.003687
Train Epoch: 5 [11040/19432 (57%)] Loss: 0.074568
Train Epoch: 5 [11200/19432 (58%)] Loss: 0.044918
Train Epoch: 5 [11360/19432 (58%)] Loss: 0.043737
Train Epoch: 5 [11520/19432 (59%)] Loss: 0.008317
Train Epoch: 5 [11680/19432 (60%)] Loss: 0.017138
Train Epoch: 5 [11840/19432 (61%)] Loss: 0.056018
Train Epoch: 5 [12000/19432 (62%)] Loss: 0.102148
Train Epoch: 5 [12160/19432 (63%)] Loss: 0.034261
Train Epoch: 5 [12320/19432 (63%)] Loss: 0.006517
Train Epoch: 5 [12480/19432 (64%)] Loss: 0.025782
Train Epoch: 5 [12640/19432 (65%)] Loss: 0.039780
Train Epoch: 5 [12800/19432 (66%)] Loss: 0.021885
Train Epoch: 5 [12960/19432 (67%)] Loss: 0.146565
Train Epoch: 5 [13120/19432 (68%)] Loss: 0.021994
Train Epoch: 5 [13280/19432 (68%)] Loss: 0.086839
Train Epoch: 5 [13440/19432 (69%)] Loss: 0.041114
Train Epoch: 5 [13600/19432 (70%)] Loss: 0.025225
Train Epoch: 5 [13760/19432 (71%)] Loss: 0.017091
Train Epoch: 5 [13920/19432 (72%)] Loss: 0.056639
Train Epoch: 5 [14080/19432 (72%)] Loss: 0.008389
Train Epoch: 5 [14240/19432 (73%)] Loss: 0.006298
Train Epoch: 5 [14400/19432 (74%)] Loss: 0.004737
Train Epoch: 5 [14560/19432 (75%)] Loss: 0.099346
Train Epoch: 5 [14720/19432 (76%)] Loss: 0.039130
Train Epoch: 5 [14880/19432 (77%)] Loss: 0.035332
Train Epoch: 5 [15040/19432 (77%)] Loss: 0.119580
Train Epoch: 5 [15200/19432 (78%)] Loss: 0.022850
Train Epoch: 5 [15360/19432 (79%)] Loss: 0.004443
Train Epoch: 5 [15520/19432 (80%)] Loss: 0.012636
Train Epoch: 5 [15680/19432 (81%)] Loss: 0.005392
Train Epoch: 5 [15840/19432 (82%)] Loss: 0.055318
Train Epoch: 5 [16000/19432 (82%)] Loss: 0.007214
Train Epoch: 5 [16160/19432 (83%)] Loss: 0.011981
Train Epoch: 5 [16320/19432 (84%)] Loss: 0.062046
Train Epoch: 5 [16480/19432 (85%)] Loss: 0.012739
Train Epoch: 5 [16640/19432 (86%)] Loss: 0.011701
Train Epoch: 5 [16800/19432 (86%)] Loss: 0.014654
Train Epoch: 5 [16960/19432 (87%)] Loss: 0.025177
Train Epoch: 5 [17120/19432 (88%)] Loss: 0.003916
Train Epoch: 5 [17280/19432 (89%)] Loss: 0.020628
Train Epoch: 5 [17440/19432 (90%)] Loss: 0.149848
Train Epoch: 5 [17600/19432 (91%)] Loss: 0.070361
Train Epoch: 5 [17760/19432 (91%)] Loss: 0.018608
Train Epoch: 5 [17920/19432 (92%)] Loss: 0.026955
Train Epoch: 5 [18080/19432 (93%)] Loss: 0.017285
Train Epoch: 5 [18240/19432 (94%)] Loss: 0.004378
Train Epoch: 5 [18400/19432 (95%)] Loss: 0.013378
Train Epoch: 5 [18560/19432 (96%)] Loss: 0.141405
Train Epoch: 5 [18720/19432 (96%)] Loss: 0.025103
Train Epoch: 5 [18880/19432 (97%)] Loss: 0.008168
Train Epoch: 5 [19040/19432 (98%)] Loss: 0.006490
Train Epoch: 5 [19200/19432 (99%)] Loss: 0.002313
Train Epoch: 5 [19360/19432 (100%)] Loss: 0.002284
    epoch          : 5
    Train_loss     : 0.0351793349459752
    Train_accuracy : 0.9891550164473685
    Train_f1_score : 0.9810884594917297
    Val_loss       : 0.27553782757946893
    Val_accuracy   : 0.9347426470588235
    Val_f1_score   : 0.9012425541877747
Train Epoch: 6 [0/19432 (0%)] Loss: 0.020085
Train Epoch: 6 [160/19432 (1%)] Loss: 0.010277
Train Epoch: 6 [320/19432 (2%)] Loss: 0.005171
Train Epoch: 6 [480/19432 (2%)] Loss: 0.020774
Train Epoch: 6 [640/19432 (3%)] Loss: 0.008891
Train Epoch: 6 [800/19432 (4%)] Loss: 0.007348
Train Epoch: 6 [960/19432 (5%)] Loss: 0.022303
Train Epoch: 6 [1120/19432 (6%)] Loss: 0.009072
Train Epoch: 6 [1280/19432 (7%)] Loss: 0.014967
Train Epoch: 6 [1440/19432 (7%)] Loss: 0.005887
Train Epoch: 6 [1600/19432 (8%)] Loss: 0.025321
Train Epoch: 6 [1760/19432 (9%)] Loss: 0.023801
Train Epoch: 6 [1920/19432 (10%)] Loss: 0.019775
Train Epoch: 6 [2080/19432 (11%)] Loss: 0.004342
Train Epoch: 6 [2240/19432 (12%)] Loss: 0.003387
Train Epoch: 6 [2400/19432 (12%)] Loss: 0.019874
Train Epoch: 6 [2560/19432 (13%)] Loss: 0.013029
Train Epoch: 6 [2720/19432 (14%)] Loss: 0.013645
Train Epoch: 6 [2880/19432 (15%)] Loss: 0.008165
Train Epoch: 6 [3040/19432 (16%)] Loss: 0.011089
Train Epoch: 6 [3200/19432 (16%)] Loss: 0.028962
Train Epoch: 6 [3360/19432 (17%)] Loss: 0.006704
Train Epoch: 6 [3520/19432 (18%)] Loss: 0.047097
Train Epoch: 6 [3680/19432 (19%)] Loss: 0.008422
Train Epoch: 6 [3840/19432 (20%)] Loss: 0.004566
Train Epoch: 6 [4000/19432 (21%)] Loss: 0.006132
Train Epoch: 6 [4160/19432 (21%)] Loss: 0.013847
Train Epoch: 6 [4320/19432 (22%)] Loss: 0.063686
Train Epoch: 6 [4480/19432 (23%)] Loss: 0.015366
Train Epoch: 6 [4640/19432 (24%)] Loss: 0.046494
Train Epoch: 6 [4800/19432 (25%)] Loss: 0.042498
Train Epoch: 6 [4960/19432 (26%)] Loss: 0.006645
Train Epoch: 6 [5120/19432 (26%)] Loss: 0.013896
Train Epoch: 6 [5280/19432 (27%)] Loss: 0.064776
Train Epoch: 6 [5440/19432 (28%)] Loss: 0.027706
Train Epoch: 6 [5600/19432 (29%)] Loss: 0.018366
Train Epoch: 6 [5760/19432 (30%)] Loss: 0.023156
Train Epoch: 6 [5920/19432 (30%)] Loss: 0.003592
Train Epoch: 6 [6080/19432 (31%)] Loss: 0.015514
Train Epoch: 6 [6240/19432 (32%)] Loss: 0.178515
Train Epoch: 6 [6400/19432 (33%)] Loss: 0.002045
Train Epoch: 6 [6560/19432 (34%)] Loss: 0.005649
Train Epoch: 6 [6720/19432 (35%)] Loss: 0.016774
Train Epoch: 6 [6880/19432 (35%)] Loss: 0.002621
Train Epoch: 6 [7040/19432 (36%)] Loss: 0.004868
Train Epoch: 6 [7200/19432 (37%)] Loss: 0.009662
Train Epoch: 6 [7360/19432 (38%)] Loss: 0.057795
Train Epoch: 6 [7520/19432 (39%)] Loss: 0.005219
Train Epoch: 6 [7680/19432 (40%)] Loss: 0.041756
Train Epoch: 6 [7840/19432 (40%)] Loss: 0.004910
Train Epoch: 6 [8000/19432 (41%)] Loss: 0.141463
Train Epoch: 6 [8160/19432 (42%)] Loss: 0.005007
Train Epoch: 6 [8320/19432 (43%)] Loss: 0.006304
Train Epoch: 6 [8480/19432 (44%)] Loss: 0.003681
Train Epoch: 6 [8640/19432 (44%)] Loss: 0.027112
Train Epoch: 6 [8800/19432 (45%)] Loss: 0.021734
Train Epoch: 6 [8960/19432 (46%)] Loss: 0.052678
Train Epoch: 6 [9120/19432 (47%)] Loss: 0.019195
Train Epoch: 6 [9280/19432 (48%)] Loss: 0.009134
Train Epoch: 6 [9440/19432 (49%)] Loss: 0.051969
Train Epoch: 6 [9600/19432 (49%)] Loss: 0.013033
Train Epoch: 6 [9760/19432 (50%)] Loss: 0.006660
Train Epoch: 6 [9920/19432 (51%)] Loss: 0.005235
Train Epoch: 6 [10080/19432 (52%)] Loss: 0.005640
Train Epoch: 6 [10240/19432 (53%)] Loss: 0.062065
Train Epoch: 6 [10400/19432 (54%)] Loss: 0.117979
Train Epoch: 6 [10560/19432 (54%)] Loss: 0.032559
Train Epoch: 6 [10720/19432 (55%)] Loss: 0.009279
Train Epoch: 6 [10880/19432 (56%)] Loss: 0.002753
Train Epoch: 6 [11040/19432 (57%)] Loss: 0.065662
Train Epoch: 6 [11200/19432 (58%)] Loss: 0.073940
Train Epoch: 6 [11360/19432 (58%)] Loss: 0.107546
Train Epoch: 6 [11520/19432 (59%)] Loss: 0.079960
Train Epoch: 6 [11680/19432 (60%)] Loss: 0.009189
Train Epoch: 6 [11840/19432 (61%)] Loss: 0.008338
Train Epoch: 6 [12000/19432 (62%)] Loss: 0.052018
Train Epoch: 6 [12160/19432 (63%)] Loss: 0.012554
Train Epoch: 6 [12320/19432 (63%)] Loss: 0.015504
Train Epoch: 6 [12480/19432 (64%)] Loss: 0.004951
Train Epoch: 6 [12640/19432 (65%)] Loss: 0.002260
Train Epoch: 6 [12800/19432 (66%)] Loss: 0.040552
Train Epoch: 6 [12960/19432 (67%)] Loss: 0.012264
Train Epoch: 6 [13120/19432 (68%)] Loss: 0.008059
Train Epoch: 6 [13280/19432 (68%)] Loss: 0.008988
Train Epoch: 6 [13440/19432 (69%)] Loss: 0.147017
Train Epoch: 6 [13600/19432 (70%)] Loss: 0.039450
Train Epoch: 6 [13760/19432 (71%)] Loss: 0.001870
Train Epoch: 6 [13920/19432 (72%)] Loss: 0.005690
Train Epoch: 6 [14080/19432 (72%)] Loss: 0.013714
Train Epoch: 6 [14240/19432 (73%)] Loss: 0.020020
Train Epoch: 6 [14400/19432 (74%)] Loss: 0.016042
Train Epoch: 6 [14560/19432 (75%)] Loss: 0.008401
Train Epoch: 6 [14720/19432 (76%)] Loss: 0.032701
Train Epoch: 6 [14880/19432 (77%)] Loss: 0.032754
Train Epoch: 6 [15040/19432 (77%)] Loss: 0.003856
Train Epoch: 6 [15200/19432 (78%)] Loss: 0.057854
Train Epoch: 6 [15360/19432 (79%)] Loss: 0.010029
Train Epoch: 6 [15520/19432 (80%)] Loss: 0.015500
Train Epoch: 6 [15680/19432 (81%)] Loss: 0.003928
Train Epoch: 6 [15840/19432 (82%)] Loss: 0.005593
Train Epoch: 6 [16000/19432 (82%)] Loss: 0.008973
Train Epoch: 6 [16160/19432 (83%)] Loss: 0.003872
Train Epoch: 6 [16320/19432 (84%)] Loss: 0.002206
Train Epoch: 6 [16480/19432 (85%)] Loss: 0.007318
Train Epoch: 6 [16640/19432 (86%)] Loss: 0.013668
Train Epoch: 6 [16800/19432 (86%)] Loss: 0.019393
Train Epoch: 6 [16960/19432 (87%)] Loss: 0.026008
Train Epoch: 6 [17120/19432 (88%)] Loss: 0.008687
Train Epoch: 6 [17280/19432 (89%)] Loss: 0.007525
Train Epoch: 6 [17440/19432 (90%)] Loss: 0.019748
Train Epoch: 6 [17600/19432 (91%)] Loss: 0.072984
Train Epoch: 6 [17760/19432 (91%)] Loss: 0.100431
Train Epoch: 6 [17920/19432 (92%)] Loss: 0.013409
Train Epoch: 6 [18080/19432 (93%)] Loss: 0.019477
Train Epoch: 6 [18240/19432 (94%)] Loss: 0.046658
Train Epoch: 6 [18400/19432 (95%)] Loss: 0.047402
Train Epoch: 6 [18560/19432 (96%)] Loss: 0.003008
Train Epoch: 6 [18720/19432 (96%)] Loss: 0.001658
Train Epoch: 6 [18880/19432 (97%)] Loss: 0.163896
Train Epoch: 6 [19040/19432 (98%)] Loss: 0.029413
Train Epoch: 6 [19200/19432 (99%)] Loss: 0.006091
Train Epoch: 6 [19360/19432 (100%)] Loss: 0.008146
    epoch          : 6
    Train_loss     : 0.03249298010785807
    Train_accuracy : 0.989977384868421
    Train_f1_score : 0.982616126537323
    Val_loss       : 0.2609938630211057
    Val_accuracy   : 0.9371936274509803
    Val_f1_score   : 0.9017688632011414
Saving checkpoint: saved/models/split-example/1103_045831/checkpoint-epoch6.pth ...
Train Epoch: 7 [0/19432 (0%)] Loss: 0.011735
Train Epoch: 7 [160/19432 (1%)] Loss: 0.019285
Train Epoch: 7 [320/19432 (2%)] Loss: 0.016446
Train Epoch: 7 [480/19432 (2%)] Loss: 0.008973
Train Epoch: 7 [640/19432 (3%)] Loss: 0.078887
Train Epoch: 7 [800/19432 (4%)] Loss: 0.042782
Train Epoch: 7 [960/19432 (5%)] Loss: 0.003569
Train Epoch: 7 [1120/19432 (6%)] Loss: 0.037704
Train Epoch: 7 [1280/19432 (7%)] Loss: 0.016543
Train Epoch: 7 [1440/19432 (7%)] Loss: 0.015564
Train Epoch: 7 [1600/19432 (8%)] Loss: 0.004019
Train Epoch: 7 [1760/19432 (9%)] Loss: 0.002655
Train Epoch: 7 [1920/19432 (10%)] Loss: 0.009547
Train Epoch: 7 [2080/19432 (11%)] Loss: 0.005337
Train Epoch: 7 [2240/19432 (12%)] Loss: 0.015065
Train Epoch: 7 [2400/19432 (12%)] Loss: 0.004680
Train Epoch: 7 [2560/19432 (13%)] Loss: 0.006567
Train Epoch: 7 [2720/19432 (14%)] Loss: 0.014527
Train Epoch: 7 [2880/19432 (15%)] Loss: 0.009883
Train Epoch: 7 [3040/19432 (16%)] Loss: 0.039366
Train Epoch: 7 [3200/19432 (16%)] Loss: 0.029502
Train Epoch: 7 [3360/19432 (17%)] Loss: 0.003291
Train Epoch: 7 [3520/19432 (18%)] Loss: 0.002629
Train Epoch: 7 [3680/19432 (19%)] Loss: 0.008366
Train Epoch: 7 [3840/19432 (20%)] Loss: 0.115197
Train Epoch: 7 [4000/19432 (21%)] Loss: 0.102468
Train Epoch: 7 [4160/19432 (21%)] Loss: 0.070958
Train Epoch: 7 [4320/19432 (22%)] Loss: 0.015766
Train Epoch: 7 [4480/19432 (23%)] Loss: 0.005748
Train Epoch: 7 [4640/19432 (24%)] Loss: 0.011125
Train Epoch: 7 [4800/19432 (25%)] Loss: 0.023815
Train Epoch: 7 [4960/19432 (26%)] Loss: 0.010425
Train Epoch: 7 [5120/19432 (26%)] Loss: 0.004630
Train Epoch: 7 [5280/19432 (27%)] Loss: 0.017379
Train Epoch: 7 [5440/19432 (28%)] Loss: 0.008114
Train Epoch: 7 [5600/19432 (29%)] Loss: 0.013616
Train Epoch: 7 [5760/19432 (30%)] Loss: 0.013216
Train Epoch: 7 [5920/19432 (30%)] Loss: 0.002294
Train Epoch: 7 [6080/19432 (31%)] Loss: 0.013311
Train Epoch: 7 [6240/19432 (32%)] Loss: 0.010216
Train Epoch: 7 [6400/19432 (33%)] Loss: 0.010486
Train Epoch: 7 [6560/19432 (34%)] Loss: 0.017545
Train Epoch: 7 [6720/19432 (35%)] Loss: 0.091591
Train Epoch: 7 [6880/19432 (35%)] Loss: 0.002801
Train Epoch: 7 [7040/19432 (36%)] Loss: 0.031024
Train Epoch: 7 [7200/19432 (37%)] Loss: 0.008860
Train Epoch: 7 [7360/19432 (38%)] Loss: 0.064083
Train Epoch: 7 [7520/19432 (39%)] Loss: 0.007151
Train Epoch: 7 [7680/19432 (40%)] Loss: 0.001949
Train Epoch: 7 [7840/19432 (40%)] Loss: 0.011914
Train Epoch: 7 [8000/19432 (41%)] Loss: 0.004289
Train Epoch: 7 [8160/19432 (42%)] Loss: 0.002401
Train Epoch: 7 [8320/19432 (43%)] Loss: 0.012189
Train Epoch: 7 [8480/19432 (44%)] Loss: 0.007897
Train Epoch: 7 [8640/19432 (44%)] Loss: 0.005785
Train Epoch: 7 [8800/19432 (45%)] Loss: 0.029822
Train Epoch: 7 [8960/19432 (46%)] Loss: 0.020310
Train Epoch: 7 [9120/19432 (47%)] Loss: 0.012047
Train Epoch: 7 [9280/19432 (48%)] Loss: 0.012582
Train Epoch: 7 [9440/19432 (49%)] Loss: 0.004225
Train Epoch: 7 [9600/19432 (49%)] Loss: 0.019267
Train Epoch: 7 [9760/19432 (50%)] Loss: 0.179814
Train Epoch: 7 [9920/19432 (51%)] Loss: 0.010734
Train Epoch: 7 [10080/19432 (52%)] Loss: 0.027502
Train Epoch: 7 [10240/19432 (53%)] Loss: 0.013129
Train Epoch: 7 [10400/19432 (54%)] Loss: 0.004050
Train Epoch: 7 [10560/19432 (54%)] Loss: 0.097098
Train Epoch: 7 [10720/19432 (55%)] Loss: 0.007909
Train Epoch: 7 [10880/19432 (56%)] Loss: 0.022392
Train Epoch: 7 [11040/19432 (57%)] Loss: 0.011132
Train Epoch: 7 [11200/19432 (58%)] Loss: 0.017557
Train Epoch: 7 [11360/19432 (58%)] Loss: 0.058931
Train Epoch: 7 [11520/19432 (59%)] Loss: 0.067171
Train Epoch: 7 [11680/19432 (60%)] Loss: 0.078613
Train Epoch: 7 [11840/19432 (61%)] Loss: 0.007172
Train Epoch: 7 [12000/19432 (62%)] Loss: 0.058710
Train Epoch: 7 [12160/19432 (63%)] Loss: 0.170653
Train Epoch: 7 [12320/19432 (63%)] Loss: 0.055173
Train Epoch: 7 [12480/19432 (64%)] Loss: 0.006951
Train Epoch: 7 [12640/19432 (65%)] Loss: 0.004470
Train Epoch: 7 [12800/19432 (66%)] Loss: 0.004024
Train Epoch: 7 [12960/19432 (67%)] Loss: 0.005390
Train Epoch: 7 [13120/19432 (68%)] Loss: 0.024560
Train Epoch: 7 [13280/19432 (68%)] Loss: 0.019067
Train Epoch: 7 [13440/19432 (69%)] Loss: 0.025147
Train Epoch: 7 [13600/19432 (70%)] Loss: 0.038878
Train Epoch: 7 [13760/19432 (71%)] Loss: 0.013189
Train Epoch: 7 [13920/19432 (72%)] Loss: 0.056212
Train Epoch: 7 [14080/19432 (72%)] Loss: 0.006914
Train Epoch: 7 [14240/19432 (73%)] Loss: 0.005936
Train Epoch: 7 [14400/19432 (74%)] Loss: 0.170048
Train Epoch: 7 [14560/19432 (75%)] Loss: 0.001700
Train Epoch: 7 [14720/19432 (76%)] Loss: 0.015299
Train Epoch: 7 [14880/19432 (77%)] Loss: 0.049515
Train Epoch: 7 [15040/19432 (77%)] Loss: 0.005352
Train Epoch: 7 [15200/19432 (78%)] Loss: 0.013996
Train Epoch: 7 [15360/19432 (79%)] Loss: 0.017961
Train Epoch: 7 [15520/19432 (80%)] Loss: 0.003966
Train Epoch: 7 [15680/19432 (81%)] Loss: 0.006537
Train Epoch: 7 [15840/19432 (82%)] Loss: 0.020786
Train Epoch: 7 [16000/19432 (82%)] Loss: 0.005762
Train Epoch: 7 [16160/19432 (83%)] Loss: 0.033964
Train Epoch: 7 [16320/19432 (84%)] Loss: 0.002635
Train Epoch: 7 [16480/19432 (85%)] Loss: 0.020184
Train Epoch: 7 [16640/19432 (86%)] Loss: 0.007773
Train Epoch: 7 [16800/19432 (86%)] Loss: 0.004825
Train Epoch: 7 [16960/19432 (87%)] Loss: 0.004117
Train Epoch: 7 [17120/19432 (88%)] Loss: 0.004215
Train Epoch: 7 [17280/19432 (89%)] Loss: 0.006369
Train Epoch: 7 [17440/19432 (90%)] Loss: 0.015960
Train Epoch: 7 [17600/19432 (91%)] Loss: 0.097843
Train Epoch: 7 [17760/19432 (91%)] Loss: 0.013484
Train Epoch: 7 [17920/19432 (92%)] Loss: 0.029812
Train Epoch: 7 [18080/19432 (93%)] Loss: 0.002604
Train Epoch: 7 [18240/19432 (94%)] Loss: 0.051193
Train Epoch: 7 [18400/19432 (95%)] Loss: 0.008610
Train Epoch: 7 [18560/19432 (96%)] Loss: 0.028141
Train Epoch: 7 [18720/19432 (96%)] Loss: 0.012833
Train Epoch: 7 [18880/19432 (97%)] Loss: 0.027309
Train Epoch: 7 [19040/19432 (98%)] Loss: 0.001914
Train Epoch: 7 [19200/19432 (99%)] Loss: 0.002347
Train Epoch: 7 [19360/19432 (100%)] Loss: 0.003554
    epoch          : 7
    Train_loss     : 0.024500546961414665
    Train_accuracy : 0.9927014802631579
    Train_f1_score : 0.9859312772750854
    Val_loss       : 0.2666815560900004
    Val_accuracy   : 0.9371936274509803
    Val_f1_score   : 0.9129148721694946
Train Epoch: 8 [0/19432 (0%)] Loss: 0.003436
Train Epoch: 8 [160/19432 (1%)] Loss: 0.014518
Train Epoch: 8 [320/19432 (2%)] Loss: 0.004632
Train Epoch: 8 [480/19432 (2%)] Loss: 0.008822
Train Epoch: 8 [640/19432 (3%)] Loss: 0.030161
Train Epoch: 8 [800/19432 (4%)] Loss: 0.005611
Train Epoch: 8 [960/19432 (5%)] Loss: 0.065367
Train Epoch: 8 [1120/19432 (6%)] Loss: 0.064674
Train Epoch: 8 [1280/19432 (7%)] Loss: 0.015444
Train Epoch: 8 [1440/19432 (7%)] Loss: 0.003861
Train Epoch: 8 [1600/19432 (8%)] Loss: 0.013375
Train Epoch: 8 [1760/19432 (9%)] Loss: 0.004769
Train Epoch: 8 [1920/19432 (10%)] Loss: 0.079248
Train Epoch: 8 [2080/19432 (11%)] Loss: 0.006182
Train Epoch: 8 [2240/19432 (12%)] Loss: 0.002832
Train Epoch: 8 [2400/19432 (12%)] Loss: 0.011715
Train Epoch: 8 [2560/19432 (13%)] Loss: 0.008172
Train Epoch: 8 [2720/19432 (14%)] Loss: 0.003488
Train Epoch: 8 [2880/19432 (15%)] Loss: 0.045927
Train Epoch: 8 [3040/19432 (16%)] Loss: 0.005067
Train Epoch: 8 [3200/19432 (16%)] Loss: 0.002781
Train Epoch: 8 [3360/19432 (17%)] Loss: 0.044878
Train Epoch: 8 [3520/19432 (18%)] Loss: 0.004935
Train Epoch: 8 [3680/19432 (19%)] Loss: 0.005433
Train Epoch: 8 [3840/19432 (20%)] Loss: 0.026118
Train Epoch: 8 [4000/19432 (21%)] Loss: 0.022973
Train Epoch: 8 [4160/19432 (21%)] Loss: 0.006688
Train Epoch: 8 [4320/19432 (22%)] Loss: 0.008214
Train Epoch: 8 [4480/19432 (23%)] Loss: 0.027319
Train Epoch: 8 [4640/19432 (24%)] Loss: 0.010067
Train Epoch: 8 [4800/19432 (25%)] Loss: 0.017921
Train Epoch: 8 [4960/19432 (26%)] Loss: 0.025422
Train Epoch: 8 [5120/19432 (26%)] Loss: 0.006258
Train Epoch: 8 [5280/19432 (27%)] Loss: 0.016876
Train Epoch: 8 [5440/19432 (28%)] Loss: 0.003812
Train Epoch: 8 [5600/19432 (29%)] Loss: 0.026009
Train Epoch: 8 [5760/19432 (30%)] Loss: 0.009659
Train Epoch: 8 [5920/19432 (30%)] Loss: 0.093478
Train Epoch: 8 [6080/19432 (31%)] Loss: 0.020809
Train Epoch: 8 [6240/19432 (32%)] Loss: 0.010692
Train Epoch: 8 [6400/19432 (33%)] Loss: 0.003485
Train Epoch: 8 [6560/19432 (34%)] Loss: 0.007043
Train Epoch: 8 [6720/19432 (35%)] Loss: 0.003829
Train Epoch: 8 [6880/19432 (35%)] Loss: 0.009048
Train Epoch: 8 [7040/19432 (36%)] Loss: 0.005437
Train Epoch: 8 [7200/19432 (37%)] Loss: 0.015403
Train Epoch: 8 [7360/19432 (38%)] Loss: 0.006333
Train Epoch: 8 [7520/19432 (39%)] Loss: 0.017459
Train Epoch: 8 [7680/19432 (40%)] Loss: 0.003209
Train Epoch: 8 [7840/19432 (40%)] Loss: 0.019809
Train Epoch: 8 [8000/19432 (41%)] Loss: 0.017670
Train Epoch: 8 [8160/19432 (42%)] Loss: 0.078241
Train Epoch: 8 [8320/19432 (43%)] Loss: 0.004412
Train Epoch: 8 [8480/19432 (44%)] Loss: 0.030138
Train Epoch: 8 [8640/19432 (44%)] Loss: 0.004234
Train Epoch: 8 [8800/19432 (45%)] Loss: 0.031621
Train Epoch: 8 [8960/19432 (46%)] Loss: 0.019109
Train Epoch: 8 [9120/19432 (47%)] Loss: 0.004825
Train Epoch: 8 [9280/19432 (48%)] Loss: 0.053533
Train Epoch: 8 [9440/19432 (49%)] Loss: 0.011570
Train Epoch: 8 [9600/19432 (49%)] Loss: 0.020580
Train Epoch: 8 [9760/19432 (50%)] Loss: 0.010969
Train Epoch: 8 [9920/19432 (51%)] Loss: 0.018288
Train Epoch: 8 [10080/19432 (52%)] Loss: 0.004074
Train Epoch: 8 [10240/19432 (53%)] Loss: 0.011995
Train Epoch: 8 [10400/19432 (54%)] Loss: 0.038282
Train Epoch: 8 [10560/19432 (54%)] Loss: 0.004871
Train Epoch: 8 [10720/19432 (55%)] Loss: 0.018098
Train Epoch: 8 [10880/19432 (56%)] Loss: 0.044105
Train Epoch: 8 [11040/19432 (57%)] Loss: 0.004045
Train Epoch: 8 [11200/19432 (58%)] Loss: 0.002607
Train Epoch: 8 [11360/19432 (58%)] Loss: 0.035792
Traceback (most recent call last):
  File "train.py", line 92, in <module>
    main(config)
  File "train.py", line 66, in main
    trainer.train()
  File "/opt/ml/project-T4193/base/base_trainer.py", line 63, in train
    result = self._train_epoch(epoch)
  File "/opt/ml/project-T4193/trainer/trainer.py", line 57, in _train_epoch
    self.optimizer.step()
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 67, in wrapper
    return wrapped(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/adamw.py", line 104, in step
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 14429) is killed by signal: Killed. 
/opt/conda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb: Train_accuracy ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: Train_f1_score ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:     Train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   Val_accuracy ‚ñÑ‚ñà‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ
wandb:   Val_f1_score ‚ñÅ‚ñà‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÜ
wandb:       Val_loss ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñà‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: Train_accuracy 0.9927
wandb: Train_f1_score 0.98593
wandb:     Train_loss 0.0245
wandb:   Val_accuracy 0.93719
wandb:   Val_f1_score 0.91291
wandb:       Val_loss 0.26668
wandb: 
wandb: Synced jumping-dream-113: https://wandb.ai/qwer55252/Boostcamp-lv1-cv1/runs/18gafmhu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221103_045828-18gafmhu/logs
